{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MATH&ML-2 Линейная алгебра в констекте линейных методов. Часть2\n",
    "###  Содержание <a class=\"anchor\" id=0></a>\n",
    "\n",
    "- [2. Неоднородные СЛАУ](#2)\n",
    "- [3. Линейная регрессия МНК](#3)\n",
    "- [4. Стандартизация векторов и матрица корреляции](#4)\n",
    "- [5. Практика. Лин.регрессия МНК](#5)\n",
    "- [6. Полиноминальная регрессия](#6)\n",
    "- [7. Регуляция](#7)\n",
    "- [7.1 $L_2$ Регуляция](#7-1)\n",
    "- [7.2 $L_1$ Регуляция](#7-2)\n",
    "- [7.3 Elastic-Net](#7-3)\n",
    "- [8. Практика. Полиноминальная регрессия и регуляция](#8)\n",
    "- [9. Итоги](#9)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Неоднородные СЛАУ <a class=\"anchor\" id=2></a>\n",
    "\n",
    "[к содержанию](#0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Совокупность уравнений первой степени, в которых каждая переменная и коэффициенты в ней являются вещественными числами, называется **системой линейных алгебраических уравнений** (`СЛАУ`) и в общем случае записывается как:\n",
    ">\n",
    ">$\\left\\{ \\begin{array}{c} a_{11}x_1+a_{12}x_2+\\dots +a_{1m}x_m=b_1 \\\\ a_{21}x_1+a_{22}x_2+\\dots +a_{2m}x_m=b_2 \\\\ \\dots \\\\ a_{n1}x_1+a_{n2}x_2+\\dots +a_{nm}x_m=b_n \\end{array} \\right.\\ (1),$\n",
    ">\n",
    ">где\n",
    ">\n",
    ">* $n$— количество уравнений;\n",
    ">\n",
    ">* $m$ — количество переменных;\n",
    ">\n",
    ">* $x_i$ — неизвестные переменные системы;\n",
    ">\n",
    ">* $a_{ij}$ — коэффициенты системы;\n",
    ">\n",
    ">* $b_i$ — свободные члены системы.\n",
    ">\n",
    ">\n",
    ">\n",
    ">СЛАУ (1) называется **однородной**, если все свободные члены системы равны 0 $b_1=b_2=⋯=b_n=0$:\n",
    ">\n",
    ">$\\textrm{С}\\textrm{Л}\\textrm{А}\\textrm{У}-\\textrm{о}\\textrm{д}\\textrm{н}\\textrm{о}\\textrm{р}\\textrm{о}\\textrm{д}\\textrm{н}\\textrm{а}\\textrm{я},\\ \\textrm{е}\\textrm{с}\\textrm{л}\\textrm{и}\\ \\forall b_i=0$\n",
    ">\n",
    ">\n",
    ">\n",
    ">СЛАУ (1) называется **неоднородной**, если хотя бы один из свободных членов системы отличен от 0:\n",
    ">\n",
    ">$\\textrm{С}\\textrm{Л}\\textrm{А}\\textrm{У}- \\textrm{н}\\textrm{е}\\textrm{о}\\textrm{д}\\textrm{н}\\textrm{о}\\textrm{р}\\textrm{о}\\textrm{д}\\textrm{н}\\textrm{а}\\textrm{я},\\ \\textrm{е}\\textrm{с}\\textrm{л}\\textrm{и}\\ \\exists b_i\\neq 0$\n",
    ">\n",
    ">\n",
    ">\n",
    ">**Решением** СЛАУ (1) называется такой набор значений неизвестных переменных $x_1,x_2,…,x_n$ при котором каждое уравнение системы превращается в равенство.\n",
    ">\n",
    ">\n",
    ">\n",
    ">СЛАУ (1) называется **определённой**, если она имеет только одно решение, и **неопределённой**, если возможно больше одного решения."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вспомним, что СЛАУ можно записать в матричном виде:\n",
    "\n",
    "$A\\overrightarrow{x}=\\overrightarrow{b}$\n",
    "\n",
    "$\\left( \\begin{array}{cccc} a_{11} & a_{12} & \\dots & a_{1m} \\\\ a_{21} & a_{22} & \\dots & a_{2m} \\\\ \\dots & \\dots & \\dots & \\dots \\\\ a_{n1} & a_{n2} & \\dots & a_{nm} \\end{array} \\right) \\cdot \\left( \\begin{array}{c} x_1 \\\\ x_2 \\\\ \\dots \\\\ x_m \\end{array} \\right)=\\left( \\begin{array}{c} b_1 \\\\ b_2 \\\\ \\dots \\\\ b_n \\end{array} \\right)$\n",
    "\n",
    "где $A$ — матрица системы, $\\overrightarrow{x}$ — вектор неизвестных коэффициентов, а $b$ — вектор свободных коэффициентов. \n",
    "\n",
    "Давайте введём новое для нас определение.\n",
    "\n",
    ">**Расширенной матрицей системы $(A|b)$ неоднородных СЛАУ** называется матрица, составленная из исходной матрицы и вектора свободных коэффициентов (записывается через вертикальную черту):\n",
    ">\n",
    ">$(A \\mid \\vec{b})=\\left(\\begin{array}{cccc|c} a_{11} & a_{12} & \\ldots & a_{1 m} & b_{1} \\\\ a_{21} & a_{22} & \\ldots & a_{2 m} & b_{2} \\\\ \\ldots & \\ldots & \\ldots & \\ldots & \\ldots \\\\ a_{n 1} & a_{n 2} & \\ldots & a_{n m} & b_{n} \\end{array}\\right)$\n",
    "\n",
    "Расширенная матрица системы — это обычная матрица. Черта, отделяющая коэффициенты $a_{ij}$ от свободных членов $b_i$ — чисто символическая. \n",
    "\n",
    "Над расширенной матрицей неоднородной СЛАУ можно производить те же самые действия, что и над обычной, а именно:\n",
    "\n",
    "* складывать/вычитать между собой строки/столбцы матрицы;\n",
    "* умножать строки/столбцы на константу;\n",
    "* менять строки/столбцы местами.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Приведём пример расширенной матрицы системы. Пусть исходная система будет следующей:\n",
    "\n",
    "$\\left\\{\\begin{array}{c} w_{1}+w_{2}=1 \\\\ w_{1}+2 w_{2}=2 \\end{array}\\right.$\n",
    "\n",
    "Запишем её в матричном виде:\n",
    "\n",
    "$\\left(\\begin{array}{cc} 1 & 1 \\\\ 1 & 2  \\end{array} \\right) \\cdot \\left(\\begin{array}{c} w_1 \\\\ w_2  \\end{array} \\right) = \\left(\\begin{array}{c} 1 \\\\ 2  \\end{array} \\right)$\n",
    "\n",
    "Тогда расширенная матрица системы будет иметь вид:\n",
    "\n",
    "$(A \\mid b)=\\left(\\begin{array}{cc|c} 1 & 2 & 1 \\\\ 1 & 2 & 2 \\\\ \\end{array}\\right)$\n",
    "\n",
    "***\n",
    "\n",
    "Существует три случая при решении неоднородных СЛАУ:\n",
    "\n",
    "* **«Идеальная пара»**\n",
    "\n",
    "Это так называемые определённые системы линейных уравнений, имеющие **единственные решения**.\n",
    "\n",
    "* **«В активном поиске»**\n",
    "\n",
    "Неопределённые системы, имеющие **бесконечно много решений**.\n",
    "\n",
    "* **«Всё сложно»**\n",
    "\n",
    "Это самый интересный для нас случай — переопределённые системы, которые **не имеют точных решений**.\n",
    "\n",
    ">**Примечание**. В данной классификации неоднородных СЛАУ допущено упрощение в терминологии. На самом деле неопределённые системы — это те, в которых независимых уравнений меньше, чем неизвестных. Они могут иметь бесконечно много решений (быть совместными) или ни одного решения (быть несовместными, если уравнения противоречат друг другу).\n",
    ">\n",
    ">На практике, например в обучении регрессий, этот случай практически не встречается.\n",
    ">\n",
    ">Что касается переопределённых систем, то в них, помимо несовместности (отсутствия решений), количество независимых уравнений превышает количество неизвестных — это тот самый случай, что мы видим в регрессионном анализе."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## СЛУЧАЙ «ИДЕАЛЬНАЯ ПАРА»\n",
    "\n",
    ">Самый простой случай решения неоднородной СЛАУ — когда система **имеет единственное решение**. Такие системы называются **совместными**.\n",
    "\n",
    "На вопрос о том, когда СЛАУ является совместной, отвечает главная теорема СЛАУ — теорема **Кронекера — Капелли** (также её называют **критерием совместности системы**).\n",
    "\n",
    "***\n",
    "\n",
    "**Теорема Кронекера — Капелли:**\n",
    "\n",
    "Неоднородная система линейный алгебраических уравнений $A\\overrightarrow{w} = \\overrightarrow{b}$ является совместной тогда и только тогда, когда ранг матрицы системы $A$ равен рангу расширенной матрицы системы $(A|b)$ и равен количеству независимых переменных $m$:\n",
    "\n",
    "$rk(A) = rk(A|\\overrightarrow{b}) = m \\leftrightarrow \\exists ! \\overrightarrow{w} = (w_{1}, w_{2}, \\ldots w_m)^T$\n",
    "\n",
    "Причём решение системы будет равно:\n",
    "\n",
    "$\\overrightarrow{w} = A^{-1} \\overrightarrow{b}$\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Пример №1\n",
    "\n",
    "$\\left\\{\\begin{array}{c} w_{1}+w_{2}=1 \\\\ w_{1}+2 w_{2}=2 \\end{array}\\right.$\n",
    "\n",
    "где  $w_1$ и $w_2$ — неизвестные переменные.\n",
    "\n",
    "При решении системы «в лоб» получим:\n",
    "\n",
    "$\\left\\{\\begin{array}{c} w_{1}+w_{2}=1 \\\\ w_{1}+2 w_{2}=2 \\end{array}\\right. \\Rightarrow \\left\\{\\begin{array}{c} w_{1}+w_{2}=1 \\\\ w_{2}=2 \\end{array}\\right. \\Rightarrow \\left\\{\\begin{array}{c} w_{1}=0 \\\\ w_{2}=0 \\end{array}\\right.$\n",
    "\n",
    "Интерпретация:\n",
    "\n",
    "$\\left( \\begin{array}{c} 1 \\\\ 2 \\end{array}\\right) = 0 \\cdot \\left( \\begin{array}{c} 1 \\\\ 1 \\end{array}\\right) + 1 \\cdot \\left( \\begin{array}{c} 1 \\\\ 2 \\end{array}\\right)$\n",
    "\n",
    "На языке линейной алгебры это означает что вектор $(1, 2)^T$ линейно выражается через векторы коэффициентов системы $(1, 1)^T$ и $(1, 2)^T$.\n",
    "\n",
    "В матричном виде система запишется, как:\n",
    "\n",
    "$A\\overrightarrow{w}=\\overrightarrow{b} \\text{где}A = \\left( \\begin{array}{cc} 1 & 1 \\\\ 1 & 2 \\end{array}\\right), \\overrightarrow{w}=\\left( \\begin{array}{c} w_1 \\\\ w_2 \\end{array}\\right), \\overrightarrow{b}=\\left( \\begin{array}{c} b_1 \\\\ b_2 \\end{array}\\right) $ \n",
    "\n",
    "Преобразование уравнений будем таким же, как и при преобразовании расширенной матрицы системы $(A|b)$, вычитая сначала первую строку из второй, а затем — результат из первой, получим то же решение, что и решение «в лоб».\n",
    "\n",
    "$(A|\\overrightarrow{b})=\\left(\\begin{array}{cc|c} 1 & 1 & 1  \\\\ 1 & 2 & 2 \\end{array} \\right) \\Rightarrow \\left(\\begin{array}{cc|c} 1 & 1 & 1  \\\\ 0 & 1 & 1 \\end{array} \\right) \\Rightarrow \\left(\\begin{array}{cc|c} 1 & 0 & 0  \\\\ 0 & 1 & 1 \\end{array} \\right) \\Rightarrow \\left\\{\\begin{array}{c} w_1=0  \\\\ w_2=0 \\end{array} \\right.$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Других решений у системы нет. \n",
    "\n",
    "Посмотрим на ранги матрицы $А$ и расширенной матрицы $(A|b)$ (количество ступеней в ступенчатых матрицах):\n",
    "\n",
    "$rk(A)=\\left(\\begin{array}{cc} 1 & 0 \\\\ 0 & 1\\end{array}\\right)=2$\n",
    "\n",
    "$rk(A|b)=\\left(\\begin{array}{ccc} 1 & 0 & 0 \\\\ 0 & 1 & 1\\end{array}\\right)=2$\n",
    "\n",
    "$rk(A)=rk(A|b)$\n",
    "\n",
    "Они совпадают и равны количеству неизвестных, а это и гарантирует существование и **единственность решения**. То есть в общем случае, чтобы узнать, сколько решений существует у системы, её необязательно было бы решать — достаточно было бы найти ранги матриц $rk(A)$ и $rk(A|b)$ ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Тут возникает вопрос: «Можно ли найти решение одной формулой?»\n",
    "\n",
    "Для удобства перепишем систему без стрелок:\n",
    "\n",
    "$Aw=b$\n",
    "\n",
    "Так как матрица квадратная и невырожденная, у неё обязательно есть обратная матрица.\n",
    "\n",
    "Умножим на $A^{-1}$ слева обе части уравнения. Стоит напомнить, что произведение матриц **не перестановочно**, поэтому есть разница, с какой стороны умножать.\n",
    "\n",
    "$A^{-1} \\cdot Aw = A^{-1}\\cdot b$\n",
    "\n",
    "$w=A^{-1}\\cdot b$\n",
    "\n",
    ">**Важно**! Отсюда явно видны ограничения этого метода: его можно применять только **для квадратных невырожденных матриц** (тех, у которых определитель не равен 0).\n",
    "\n",
    "Убедимся в правильности формулы. Найдём произведение матрицы $A^{-1}$ и вектора-столбца $b$:\n",
    "\n",
    "$A^{-1}\\cdot b=\\left( \\begin{array}{cc} 1 & 1 \\\\ 1 & 2 \\end{array}\\right)^{-1} \\cdot \\left(\\begin{array}{c} 1 \\\\ 2 \\end{array}\\right)=\\left( \\begin{array}{cc} 2 & -1 \\\\ -1 & 1 \\end{array}\\right) \\cdot \\left(\\begin{array}{c} 1 \\\\ 2 \\end{array}\\right) = \\left(\\begin{array}{c} 0 \\\\ 1 \\end{array}\\right)$ \n",
    "\n",
    "***\n",
    "\n",
    "**Резюмируем ↓**\n",
    "\n",
    "У нас есть квадратная система с $m$ неизвестных. Если ранг матрицы коэффициентов $A$ **равен** рангу расширенной матрицы $(A|b)$ и **равен** количеству переменных $(rk(A)=rk(\\overrightarrow{b}))=m$, то в системе будет ровно столько независимых уравнений, сколько и неизвестных $m$, а значит будет **единственное** решение.\n",
    "\n",
    "Вектор свободных коэффициентов $b$ при этом линейно независим со столбцами матрицы $A$, его разложение по столбцам $A$ **единственно**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## СЛУЧАЙ «В АКТИВНОМ ПОИСКЕ»\n",
    "\n",
    "?А что, если система **не удовлетворяет теореме Кронекера — Капелли**? То есть ранг матрицы системы равен расширенному рангу матрицы, но не равен количеству неизвестных. Неужели тогда система нерешаема?\n",
    "\n",
    "На этот вопрос отвечает первое следствие из теоремы ↓\n",
    "\n",
    "**Следствие №1** из теоремы Кронекера — Капелли:\n",
    "\n",
    "Если ранг матрицы системы $A$ равен рангу расширенной матрицы системы $(A|b)$, **но меньше**, чем количество неизвестных $m$, то система имеет бесконечное множество решений:\n",
    "\n",
    "$rk(A) = rk(A | \\vec{b}) < m  \\leftrightarrow  \\infty \\ решений$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решим систему уравнений:\n",
    "\n",
    "$w_1+w_2+w_3=10$\n",
    "\n",
    "Да, уравнение одно, но формально оно является неоднородной СЛАУ.\n",
    "\n",
    "Итак, мы имеем одно уравнение на три неизвестных, значит две координаты из трёх вектора $w$ мы можем задать как угодно. Например, зададим вторую и третью как $\\alpha$ и $\\beta$. Тогда первая будет равна $10-\\alpha-\\beta$.\n",
    "\n",
    "$w=\\left( \\begin{array}{c} {10-\\alpha-\\beta} \\\\ \\alpha \\\\ \\beta \\end{array} \\right)$ где $\\alpha, \\beta \\in \\mathbb{R}$\n",
    "\n",
    "Вместо переменных $\\alpha$ и $\\beta$ мы можем подставлять любые числа и всегда будем получать равенство. \n",
    "\n",
    "Составим расширенную матрицу:\n",
    "\n",
    "$(A|b)=(\\begin{array}{} 1 & 1 & 1|10 \\end{array})$\n",
    "\n",
    "Её ранг, как и ранг $A$, равен 1, что меньше числа неизвестных $m=3$:\n",
    "\n",
    "$rk(A) = rk(A | \\vec{b}) = 1 < 3$\n",
    "\n",
    "Такая ситуация, по следствию из теоремы Кронекера — Капелли, говорит о существовании и не единственности решения, то есть решений **бесконечно много**.\n",
    "\n",
    "***\n",
    "\n",
    "**Резюмируем ↓\n",
    "**\n",
    "Если ранги матриц $A$ и $(A|b)$ всё ещё совпадают, но уже меньше количества неизвестных $(rk(A) = rk(A | \\vec{b}) < m)$, значит, уравнений не хватает для того, чтобы определить систему полностью, и решений будет бесконечно много.\n",
    "\n",
    "На языке линейной алгебры это значит, что вектор $\\overrightarrow{b}$ линейно зависим со столбцами матрицы $A$, но также и сами столбцы зависимы между собой, поэтому равнозначного разложения не получится, т. е. таких разложений может быть сколько угодно."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## СЛУЧАЙ «ВСЁ СЛОЖНО»\n",
    "\n",
    "А теперь посмотрим на самый интересный для нас случай. Его формально регламентирует второе следствие из теоремы Кронекера — Капелли.\n",
    "\n",
    "**Следствие №2 из теоремы Кронекера — Капелли**:\n",
    "\n",
    "Если ранг матрицы системы $A$ меньше, чем ранг расширенной матрицы системы $(A|b)$, то система несовместна, то есть не имеет точных решений:\n",
    "\n",
    "$rk(A)  < rk(A | \\vec{b})  \\leftrightarrow  \\nexists \\ решений$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решим систему уравнений:\n",
    "\n",
    "$\\left\\{\\begin{array}{c} w_1+w_2=1 \\\\ w_1+2 w_2=2 \\\\ w_1+w_2=12 \\end{array}\\right.$\n",
    "\n",
    "Посмотрим на первое и третье уравнение — очевидно, что такая система не имеет решений, так как данные уравнения противоречат друг другу.\n",
    "\n",
    "Но давайте обоснуем это математически. Для этого запишем расширенную матрицу системы:\n",
    "\n",
    "$(A|b)=\\left(\\begin{array}{cc|c} 1 & 1 & 1 \\\\ 1 & 2 & 2 \\\\ 1 & 1 & 12 \\end{array}\\right)$\n",
    "\n",
    "Посчитаем ранги матриц $A$ и $(A|b)$:\n",
    "\n",
    "$rk(A)=rk\\left(\\begin{array}{cc} 1 & 1  \\\\ 1 & 2 \\\\ 1 & 1 \\end{array}\\right) \\Rightarrow I-III \\Rightarrow rk\\left(\\begin{array}{cc} 1 & 1  \\\\ 1 & 2 \\end{array}\\right) \\Rightarrow II-I \\Rightarrow rk\\left(\\begin{array}{cc} 1 & 1  \\\\ 0 & 1 \\end{array}\\right)=2$\n",
    "\n",
    "$rk(A|b)=rk\\left(\\begin{array}{ccc} 1 & 1 & 1 \\\\ 1 & 2 & 2 \\\\ 1 & 1 & 12 \\end{array}\\right) \\Rightarrow III-I, II-I \\Rightarrow rk\\left(\\begin{array}{cc} 1 & 1 & 1 \\\\ 0 & 1 & 1 \\\\ 0 & 0 & 12\\end{array}\\right)=3$\n",
    "\n",
    "Итак, $rk(A)=2$, в то время как $rk(A|b)=3$. Это и есть критерий **переопределённости системы уравнений**: ранг матрицы системы меньше ранга расширенной матрицы системы.\n",
    "\n",
    "Получается, что идеальное решение найти нельзя, но чуть позже мы увидим, что такие системы возникают в задачах регрессии практически всегда, а значит нам всё-таки хотелось бы каким-то образом её решать. Можно попробовать найти приблизительное решение — вопрос лишь в том, какое из всех этих решений лучшее."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдем наилучшее приближение для $w_1$, $w_2$, если:\n",
    "\n",
    "$\\left\\{\\begin{array}{c} w_1+w_2=1 \\\\ w_1+2 w_2=2 \\\\ w_1+w_2=12 \\end{array}\\right. \\Rightarrow \\left(\\begin{array}{cc} 1 & 1  \\\\ 1 & 2 \\\\ 1 & 1 \\end{array}\\right)\\cdot \\left(\\begin{array}{c} w  \\\\ w \\end{array}\\right)=\\left(\\begin{array}{c} 1 \\\\ 2 \\\\ 12 \\end{array}\\right)$\n",
    "\n",
    "Обозначим приближённое решение как $\\hat{w}$. Приближением для вектора $b$ будет $\\hat{b}=A \\hat{w}$. Также введём некоторый вектор ошибок $e=b-\\hat{b}=b-A \\hat{w}$.\n",
    "\n",
    ">**Примечание**. Здесь мы снова опустили стрелки у векторов $b$, $\\hat{b}$ и $\\hat{w}$ для наглядности.\n",
    "\n",
    "Например, если мы возьмём в качестве вектора $\\hat{w}$ вектор $\\hat{w_1}=(1,1)^T$, то получим:\n",
    "\n",
    "$\\hat{b}=A\\hat{w_1}=\\left(\\begin{array}{cc} 1 & 1 \\\\ 1 & 2 \\\\ 1 & 1\\end{array}\\right)\\cdot\\left(\\begin{array}{c} 1 \\\\ 1  \\end{array}\\right)=\\left(\\begin{array}{c} 2 \\\\ 3 \\\\ 2 \\end{array}\\right)$\n",
    "\n",
    "$e_1=b-A\\hat{w_1}=\\left(\\begin{array}{c} 1 \\\\ 2 \\\\ 12\\end{array}\\right) - \\left(\\begin{array}{c} 2 \\\\ 3 \\\\ 2\\end{array}\\right)=\\left(\\begin{array}{c} -1 \\\\ -1 \\\\ 10 \\end{array}\\right)$\n",
    "\n",
    "Теперь возьмём в качестве вектора $\\hat{w_2}=(4, -1)^T$, получим:\n",
    "\n",
    "$\\hat{b}=A\\hat{w_2}=\\left(\\begin{array}{cc} 1 & 1 \\\\ 1 & 2 \\\\ 1 & 1\\end{array}\\right)\\cdot\\left(\\begin{array}{c} 4 \\\\ -1  \\end{array}\\right)=\\left(\\begin{array}{c} 3 \\\\ 2 \\\\ 3 \\end{array}\\right)$\n",
    "\n",
    "$e_2=b-A\\hat{w_2}=\\left(\\begin{array}{c} 1 \\\\ 2 \\\\ 12\\end{array}\\right) - \\left(\\begin{array}{c} 3 \\\\ 2 \\\\ 3\\end{array}\\right)=\\left(\\begin{array}{c} -2 \\\\ -1 \\\\ 9 \\end{array}\\right)$\n",
    "\n",
    ">Конечно, нам хотелось бы, чтобы ошибка была поменьше. Но какая из них поменьше? Векторы сами по себе сравнить нельзя, **но зато можно сравнить их длины**.\n",
    "\n",
    "$\\left\\|e_1 \\right\\| = \\sqrt{(-1)^2 + (-1)^2 + (10)^2} = \\sqrt{102}$\n",
    "\n",
    "$\\left\\|e_2 \\right\\| = \\sqrt{(-2)^2 + 0^2 + 9^2} = \\sqrt{85}$\n",
    "\n",
    ">Видно, что вторая ошибка всё-таки меньше, соответственно, приближение лучше. Но в таком случае из всех приближений нам нужно выбрать то, у которого длина вектора ошибок минимальна, если, конечно, это возможно.\n",
    ">\n",
    ">$||e||\\rightarrow min$\n",
    ">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    ">**Примечание**. Проблема поиска оптимальных приближённых решений неоднородных переопределённых СЛАУ стояла у математиков вплоть до XIX века. До этого времени математики использовали частные решения, зависящие от вида уравнений и размерности. Впервые данную задачу для общего случая решил Гаусс, опубликовав метод решения этой задачи, который впоследствии будет назван методом наименьших квадратов (МНК). В дальнейшем Лаплас прибавил к данному методу теорию вероятности и доказал оптимальность МНК-оценок с точки зрения статистики.\n",
    "\n",
    "Сейчас мы почувствуем себя настоящими математиками и попробуем решить эту задачу самостоятельно с помощью простой геометрии и знакомых нам операций над матрицами.\n",
    "\n",
    "Вспомним, что на языке линейной алгебры неразрешимость системы\n",
    "\n",
    "$\\left(\\begin{array}{cc} 1 & 1  \\\\ 1 & 2 \\\\ 1 & 1 \\end{array}\\right)\\cdot \\left(\\begin{array}{c} w  \\\\ w \\end{array}\\right)=\\left(\\begin{array}{c} 1 \\\\ 2 \\\\ 12 \\end{array}\\right)$\n",
    "\n",
    "означает, что попытка выразить вектор $(1,2,12)^T$ через $(1,1,1)^T$ и $(1,2,1)^T$ не будет успешной, так как они **линейно независимы**.\n",
    "\n",
    "Геометрически это означает, что вектор свободных коэффициентов $\\textcolor{brown}{b}$ (коричневый) не лежит в одной плоскости со столбцами матрицы $\\textcolor{blue}{A}$ (синие векторы).\n",
    "\n",
    "<img src=m2_img1.png>\n",
    "\n",
    "Идея состояла в том, что наилучшим приближением для коричневого вектора будет ортогональная проекция на синюю плоскость — $\\textcolor{cyan}{голубой}$ вектор. Так происходит потому, что наименьший по длине вектор ошибок — $\\textcolor{red}{красный}$ — должен быть перпендикулярен к синей плоскости:\n",
    "\n",
    "$e=b-\\hat{b}$\n",
    "\n",
    "В прошлом модуле мы производили расчёты интуитивно, а теперь настала пора вывести формулу.\n",
    "\n",
    "Давайте умножим наши уравнения слева на $A^T$:\n",
    "\n",
    "$A^T\\cdot\\textcolor{cyan}{A\\hat{w}}=A^T\\cdot\\textcolor{brown}{b}$\n",
    "\n",
    "Идея заключается в следующем: справа мы найдём скалярное произведение столбцов матрицы $A$ на вектор $b$, а слева — произведение столбцов $A$ на приближённый вектор $\\hat{b}$ (по сути, на голубую проекцию).\n",
    "\n",
    "Упростим уравнение, перемножив всё, что содержит только числа. В левой части умножим $A^T$ на $A$, в правой — умножим $A^T$ на $b$. Тогда слева получим матрицу 2×2 — это не что иное, как матрица Грама столбцов $A$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Столбцы $A$ линейно независимы, а это значит, что, по свойству матрицы Грама, $A^T\\cdot A$  — невырожденная квадратная матрица (её определитель не равен нулю, и для неё существует обратная матрица). Получившаяся система — один в один случай «идеальная пара» (ранг матрицы, как и ранг расширенной матрицы, равен 2, в чём несложно убедиться), а это значит, что теперь мы можем её решить.\n",
    "\n",
    "$\\left(\\begin{array}{} 3 & 4 \\\\ 4 & 6 \\end{array}\\right)\\cdot\\left(\\begin{array}{} \\hat{w} \\\\ \\hat{w} \\end{array}\\right) = \\left(\\begin{array}{} 15 \\\\ 17 \\end{array}\\right)$\n",
    "\n",
    "$\\left(\\begin{array}{} 3 & 4 \\\\ 4 & 6 \\end{array}\\right)$ - матрица Грамма столбцов $A$\n",
    "\n",
    "$A^T\\cdot A=Gram(\\left(\\begin{array}{} 1 \\\\ 1 \\\\ 1 \\end{array}\\right),\\left(\\begin{array}{} 1 \\\\ 2 \\\\ 1 \\end{array}\\right))$\n",
    "\n",
    "Но ведь мы не могли решить изначальную задачу, так как она была переопределена, а эту — можем. **Как так получилось**?\n",
    "\n",
    "Мы потребовали, чтобы у приближения $\\hat{b}$ были с векторами $(1,1,1)^T$ и $(1,2,1)^T$ такие же скалярные произведения, как у $b$. Это и означает что $\\hat{b}$ — ортогональная проекция на нашу синюю плоскость, в которой лежат столбцы матрицы $A$, и в этой плоскости мы можем найти коэффициенты.\n",
    "\n",
    "Мы с вами отлично умеем решать системы типа «Идеальная пара». Для этого нам нужно найти обратную матрицу $(A^T\\cdot A)^{-1}$ и умножить на неё слева всё уравнение. Так мы и получим наше приближение:\n",
    "\n",
    "$(A^TA)\\cdot\\hat{w}=A^Tb$\n",
    "\n",
    "Находим определитель матрицы $(A^TA)$: \n",
    "\n",
    "$\\mathbb{det}(A^TA) = 3\\cdot6-4\\cdot4=2$\n",
    "\n",
    "Находим обратную матрицу $(A^TA)^{-1}$:\n",
    "\n",
    "$(A^TA)^{-1}=\\left(\\begin{array}{cc} 3 & 4 \\\\ 4 & 6\\end{array}\\right)^{-1}=\\frac{1}{2}\\left(\\begin{array}{cc} 6 & -4 \\\\ -4 & 3\\end{array}\\right)=\\left(\\begin{array}{cc} 3 & -2 \\\\ -2 & 1.5\\end{array}\\right)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Умножаем всё уравнение на обратную матрицу слева:\n",
    "\n",
    "$(A^TA)^{-1}\\cdot(A^TA)\\cdot\\hat{w}=(A^TA)^{-1}\\cdot A^T\\cdot b$\n",
    "\n",
    "$\\hat{w}=(A^TA)^{-1}\\cdot A^T\\cdot b$\n",
    "\n",
    "И, наконец, вот он — долгожданный приближённый вектор $\\hat{w}$:\n",
    "\n",
    "$\\hat{w}=\\left(\\begin{array}{cc} 3 & -2 \\\\ -2 & 1.5\\end{array}\\right)\\cdot\\left(\\begin{array}{} 15 \\\\ 17\\end{array}\\right)=\\left(\\begin{array}{} 11 \\\\ -4.5\\end{array}\\right)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "⭐ **Пришло время открытий!**\n",
    "\n",
    "Только что мы геометрическим образом вывели формулу оценки решения методом наименьших квадратов (`МНК` или `OLS`, Ordinary Least Squares).\n",
    "\n",
    ">**Примечание**. Стоит отметить, что полученная матричная формула не зависит от размерностей и конкретных значений, а значит применима не только в нашем локальном случае, но и в общем.\n",
    "\n",
    "Нам осталось выполнить проверку полученных результатов, чтобы убедиться в верности решения.\n",
    "\n",
    "Вычислим голубой вектор $\\hat{b}$. Для этого возьмём линейную комбинацию столбцов матрицы $А$ с найденными нами коэффициентами $\\hat{w_1}$ и $\\hat{w_2}$\n",
    "\n",
    "$\\hat{b}=A\\hat{w}=\\hat{w_1}\\cdot\\left(\\begin{array}{} 1\\\\1\\\\1\\end{array}\\right)+\\hat{w_2}\\cdot\\left(\\begin{array}{} 1\\\\2\\\\1\\end{array}\\right)=11\\cdot\\left(\\begin{array}{} 1\\\\1\\\\1\\end{array}\\right)-4.5\\cdot\\left(\\begin{array}{} 1\\\\2\\\\1\\end{array}\\right)=\\left(\\begin{array}{} 6.5\\\\2\\\\6.5\\end{array}\\right)$\n",
    "\n",
    "Вычислим вектор ошибок $e$:\n",
    "\n",
    "$e=b-\\hat{b}=b-A\\hat{w}=\\left(\\begin{array}{} 1\\\\2\\\\12\\end{array}\\right)-\\left(\\begin{array}{} 6.5\\\\2\\\\6.5\\end{array}\\right)=\\left(\\begin{array}{} -5.5\\\\0\\\\5.5\\end{array}\\right)$\n",
    "\n",
    "Убедимся, что данный вектор действительно ортогонален столбцам матрицы $А$. Для этого найдём их скалярные произведения:\n",
    "\n",
    "$(e,A_1)=e^T\\cdot A_1=(-5.5, 0, 5.5) \\cdot \\left(\\begin{array}{} 1\\\\1\\\\1\\end{array}\\right) = 0$\n",
    "\n",
    "$(e,A_2)=e^T\\cdot A_2=(-5.5, 0, 5.5) \\cdot \\left(\\begin{array}{} 1\\\\2\\\\1\\end{array}\\right) = 0$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скалярные произведения равны 0, а это означает, что вектор ошибок $\\textcolor{red}e$ действительно ортогонален всей синей плоскости, а голубой вектор $\\textcolor{cyan}{\\hat{b}}$ приближённого значения является ортогональной проекцией коричневого вектора $\\textcolor{brown}b$.\n",
    "\n",
    ">**Примечание**. Прежде чем перейти к выводам, стоит отметить, что обычно `OLS`-оценку выводят немного иначе, а именно минимизируя в явном виде длину вектора ошибок по коэффициентам $\\hat{w}$, вернее, даже квадрат длины для удобства вычисления производных.\n",
    ">\n",
    ">$||\\overrightarrow{e}||\\rightarrow min$\n",
    ">\n",
    ">$||\\overrightarrow{e}||^2\\rightarrow min$\n",
    ">\n",
    ">$||\\overrightarrow{b}-A\\overrightarrow{w}||^2\\rightarrow min$\n",
    ">\n",
    ">Формула получится точно такой же, какая есть у нас, просто способ вычислений будет не геометрический, а аналитический. Мы вернёмся к этому способу, когда будем обсуждать оптимизацию функции многих переменных в разделе по математическому анализу.\n",
    "\n",
    "Наконец, мы может подвести итоги для случая «Всё сложно»."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**Резюмируем ↓**\n",
    "\n",
    "Если ранг матрицы $A$ меньше ранга расширенной системы $(A|b)$, то независимых уравнений больше, чем переменных $(rkA<(A|b)<m)$, а значит некоторые из них будут противоречить друг другу, то есть решений у системы нет.\n",
    "\n",
    "Говоря на языке линейной алгебры, вектор $b$ линейно независим со столбцами матрицы $A$, а значит его нельзя выразить в качестве их линейной комбинации.\n",
    "\n",
    "Однако можно получить приближённые решения по методу наименьших квадратов (`OLS` - оценка - $\\hat{b}=(A^TA)^{-1}\\cdot A^Tb$), идеей которого является ортогональная проекция вектора $b$ на столбцы матрицы $A$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдите `OLS`-оценку для коэффициентов $w_1$, $w_2$ СЛАУ:\n",
    "\n",
    "$\\left\\{\\begin{array}{c} w_1+2w_2=1 \\\\ -3w_1+w_2=4 \\\\ w_1+2w_2=5 \\\\ w_1-w_2=0\\end{array}\\right.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "A = np.array([[1,2],[-3,1],[1,2],[1,-1]])\n",
    "b = np.array([1,4,5,0])\n",
    "b = np.reshape(b,(4,1))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Вычислите матрицу Грама столбцов $A:A^{T}A=$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12,  0],\n",
       "       [ 0, 10]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.T@A"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Вычислите матрицу $(A^{T}A)^{-1}$. Она имеет вид:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.08333333, 0.        ],\n",
       "       [0.        , 0.1       ]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_a = np.linalg.inv(A.T@A)\n",
    "a_a"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Вычислите $A^{T} \\overrightarrow{b}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-6],\n",
       "       [16]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_b = A.T@b\n",
    "a_b"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Вычислите вектор оценок коэффициентов $\\overrightarrow{w}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.5],\n",
       "       [ 1.6]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_a@a_b"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Линейная регрессия МНК <a class=\"anchor\" id=3></a>\n",
    "\n",
    "[к содержанию](#0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В задаче регрессии обычно есть **целевая переменная**, которую мы хотим предсказать. Её, как правило, обозначают буквой $y$. Помимо целевой переменной, есть **признаки** (их также называют **факторами** или **регрессорами**). Пусть их будет $k$ штук:\n",
    "\n",
    "$y$ — целевая переменная\n",
    "\n",
    "$x_1, x_2, \\dots, x_k$ — признаки/факторы/регрессоры\n",
    "\n",
    "Поставить задачу — значит ответить на два вопроса:\n",
    "\n",
    "1. Что у нас есть?\n",
    "\n",
    "2. Что мы хотим получить?\n",
    "\n",
    "В задаче регрессии есть $N$ (как правило, их действительно много) наблюдений. Это наша обучающая выборка или датасет, представленный в виде таблицы. В столбцах таблицы располагаются векторы признаков $\\overrightarrow{x_i}$.\n",
    "\n",
    "$\\overrightarrow{y} \\in \\mathbb{R}^N$\n",
    "\n",
    "$x_1, x_2, \\dots, x_k\\in \\mathbb{R}^N$\n",
    "\n",
    "$\\left(\\begin{array}{} y_1 \\\\ y_2 \\\\ \\dots \\\\ y_N\\end{array}\\right), \\left(\\begin{array}{} x_1 \\\\ x_2 \\\\ \\dots \\\\ x_N\\end{array}\\right), \\dots, \\left(\\begin{array}{} x_k1 \\\\ x_k2 \\\\ \\dots \\\\ x_kN\\end{array}\\right)$\n",
    "\n",
    "То есть и целевая переменная, и признаки представлены векторами из векторного пространства $\\mathbb{R}^N$ — каждого вектора $N$ координат.\n",
    "\n",
    "В качестве регрессионной модели мы будем использовать **модель линейной регрессии**. Мы предполагаем, что связь между целевой переменной и признаками линейная. Это означает, что:\n",
    "\n",
    "$y=w_0+w_1x_1+w_2x_2+…+w_kx_k,$\n",
    "\n",
    "или \n",
    "\n",
    "$y=(\\overrightarrow{w}, \\overrightarrow{x})$\n",
    "\n",
    "Здесь $\\overrightarrow{w}=(w_0,w_1,…,w_k)^T$ обозначают веса (коэффициенты уравнения линейной регрессии), а $\\overrightarrow{x}=(1,x_1, x_2,…, x_k)^T$.\n",
    "\n",
    ">Наличие коэффициента $w_0$ говорит о том, что мы строим регрессию с константой, или, как ещё иногда говорят, с **интерсептом**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пока что коэффициенты $w$ нам неизвестны. Как же их найти?\n",
    "\n",
    "Для этого у нас есть $N$ наблюдений — обучающий набор данных.\n",
    "\n",
    "Давайте попробуем подобрать такие веса $w$, чтобы для каждого наблюдения наше равенство было выполнено. Таким образом, получается $N$ уравнений на $k+1$ неизвестную.\n",
    "\n",
    "$\\left(\\begin{array}{} y_1 \\\\ y_2 \\\\ \\dots \\\\ y_N\\end{array}\\right)=w_0\\cdot \\left(\\begin{array}{} 1 \\\\ 2 \\\\ \\dots \\\\ 1 \\end{array}\\right)+w_1 \\cdot\\left(\\begin{array}{} x_11 \\\\ x_12 \\\\ \\dots \\\\ x_1N\\end{array}\\right)+ \\dots +w_k\\cdot \\left(\\begin{array}{} x_k1 \\\\ x_k2 \\\\ \\dots \\\\ x_kN\\end{array}\\right)$\n",
    "\n",
    "Или в привычном виде систем уравнений:\n",
    "\n",
    "$\\left\\{\\begin{array}{} w_0 1+w_1 x_{11} + \\dots +w_k x_{k1}=y_1 \\\\ w_0 1+w_1 x_{12} + \\dots +w_k x_{k2}=y_2 \\\\ \\dots \\\\ w_0 1+w_1 x_{1N} + \\dots +w_k x_{kN}=y_N \\end{array}\\right.$\n",
    "\n",
    ">Говоря на языке машинного обучения, мы хотим обучить такую модель, которая описывала бы зависимость целевой переменной от факторов на обучающей выборке.\n",
    "\n",
    "Как правило, $N$ гораздо больше $k$ (количество строк с данными в таблице намного больше количества столбцов) и система переопределена, значит точного решения нет. Поэтому можно найти только приближённое."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Примечание**. Полученной СЛАУ можно дать геометрическую интерпретацию. Если представить каждое наблюдение в виде точки на графике (см. ниже), то уравнение линейной регрессии будет задавать прямую (если фактор один) или гиперплоскость (если факторов $k$ штук). Приравняв уравнение прямой к целевому признаку, мы потребовали, чтобы эта прямая проходила через все точки в нашем наборе данных. Конечно же, это условие не может быть выполнено полностью, так как в данных всегда присутствует какой-то шум, и идеальной прямой (гиперплоскости) не получится, но зато можно построить приближённое решение.\n",
    ">\n",
    "><img src=m2_img2.png width=400>\n",
    ">\n",
    ">**Обратите внимание**, что у нас появился новый вектор из единиц. Он здесь из-за того, что мы взяли модель с интерсептом. Можно считать, что это новый регрессор-константа. Данная константа тянется из уравнения прямой, которое мы разбирали в модуле «ML-2. Обучение с учителем: регрессия».\n",
    "\n",
    "Мы уже умеем решать переопределённые системы, для этого мы должны составить матрицу системы $A$, записав в столбцы все наши регрессоры, включая регрессор константу:\n",
    "\n",
    "$A=\\left(\\begin{array}{} 1 & x_{11} & \\dots & x_{k1} \\\\ 1 & x_{12} & \\dots & x_{k2} \\\\ \\dots & \\dots & \\dots & \\dots \\\\ 1 & x_{1N} & \\cdot & x_{kN} \\end{array}\\right) \\Rightarrow N - строк, k+1 - столбец$\n",
    "\n",
    ">**Примечание**. В контексте задач машинного обучения матрица $A$ называется **матрицей наблюдений**: по строкам отложены наблюдения (объекты), а по столбцам — характеризующие их признаки. В модулях по машинному обучению мы в основном обозначали её за $X$. Здесь же мы будем придерживаться традиций линейной алгебры и обозначать матрицу за $A$.\n",
    ">\n",
    ">**Примечание**. Обратите внимание, что индексация матрицы $A$ отличается от привычной нам индексации матрицы. Например, здесь $x_{12}$ — второе наблюдение первого регрессора. Это чистая формальность. Если обозначать за первый индекс номер наблюдения, а за второй индекс — номер регрессора, мы получим привычную нам нумерацию элементов матрицы (строка-столбец)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осталось записать финальную формулу `OLS`-оценки для коэффициентов:\n",
    "\n",
    "$\\hat{\\overrightarrow{w}} = (A^T A)^{-1} A^T \\overrightarrow{y}$\n",
    "\n",
    "Казалось бы, задача решена, однако это совсем не так, ведь мы искали коэффициенты не просто так, а чтобы сделать прогноз — предсказание на новых данных.\n",
    "\n",
    "Допустим, у нас есть новое наблюдение по регрессорам, которое характеризуется признаками $\\overrightarrow{x}_{NEW} = (x_{1, NEW}, x_{2, NEW}, ..., x_{k, NEW})^T$. Тогда, предсказание будет строиться следующим образом:\n",
    "\n",
    "$\\vec{y}_{NEW} = \\vec{w}_0 + \\vec{w}_1 x_{1, NEW} + ... + \\vec{w}_k x_{k, NEW}$\n",
    "\n",
    "или\n",
    "\n",
    "$\\vec{y}_{NEW} = (\\hat{\\vec{w}}, \\vec{x}_{NEW})$\n",
    "\n",
    "Теперь перейдём от формул к практике и решим задачу в контексте."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "Рассмотрим классический датасет для обучения линейной регрессии — `Boston Housing`. В нём собраны усреднённые данные по стоимости недвижимости в 506 районах Бостона. Ниже вы видите фрагмент датасета.\n",
    "\n",
    "Целевой переменной будет $\\textcolor{red}{PRICE}$ — это, в некотором смысле, типичная (медианная) стоимость дома в районе.\n",
    "\n",
    "Для примера возьмём в качестве регрессоров уровень преступности $\\textcolor{blue}{CRIM}$ и среднее количество комнат в доме $\\textcolor{blue}{RM}$.\n",
    "\n",
    "<img src=m2_img3.png width=600>\n",
    "\n",
    "Запишем нашу модель:\n",
    "\n",
    "$y=w_0+w_1 \\cdot x_1+w_2 \\cdot x_2$\n",
    "\n",
    "Для наглядности обозначим:\n",
    "\n",
    "$y=w_0+w_1 \\cdot CRIM+w_2 \\cdot RM$\n",
    "\n",
    "Составим матрицу регрессоров:\n",
    "\n",
    "$A=\\left(\\begin{array}{} 1 & CRIM_1 & RM_1 \\\\ 1 & CRIM_2 & RM_2 \\\\ \\dots & \\dots & \\dots \\\\ 1 & CRIM_N & RM_N \\end{array}\\right)$\n",
    "\n",
    "В нашем случае $N=506$, а $k=2$. Размерность матрицы $A$ будет равна $\\mathbb{dim}A=(506,3)$. Далее мы применяем формулу для вычисления оценок коэффициентов:\n",
    "\n",
    "$\\hat{\\vec{w}} = (A^T A)^{-1} A^T \\vec{y}$\n",
    "\n",
    "Вычисления к этой задаче мы сделаем в `Python` ниже, а пока приведём конечный результат. Если сократить запись до двух знаков после точки, получим следующие коэффициенты:\n",
    "\n",
    "$\\hat{\\vec{w}} = (-29.3, \\ -0.26, \\ 8.4)^T$\n",
    "\n",
    "То есть:\n",
    "\n",
    "$\\hat{w}_0 = -29.3$\n",
    "\n",
    "$\\hat{w}_1 = -0.26$\n",
    "\n",
    "$\\hat{w}_2 = 8.4$\n",
    "\n",
    "Мы можем переписать нашу модель для прогноза:\n",
    "\n",
    "$\\hat{y} = -29.3 - 0.26 \\cdot CRIM + 8.4 \\cdot RM$\n",
    "\n",
    "Теперь, если у нас появятся новые наблюдения, то есть ещё один небольшой район с уровнем преступности 0.1 на душу населения и средним количеством комнат в доме, равным 8, мы сможем сделать прогноз на типичную стоимость дома в этом районе — 37 тысяч долларов:\n",
    "\n",
    "$CRIM_{NEW} = 0.1$\n",
    "\n",
    "$RM_{NEW} = 8$\n",
    "\n",
    "$\\hat{y}_{NEW} = -29.3 -0.26 \\cdot 0.1 + 8.4 \\cdot 8 \\approx 37$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "## → Решение на Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296.0   \n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242.0   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242.0   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222.0   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  PRICE  \n",
       "0     15.3  396.90   4.98   24.0  \n",
       "1     17.8  396.90   9.14   21.6  \n",
       "2     17.8  392.83   4.03   34.7  \n",
       "3     18.7  394.63   2.94   33.4  \n",
       "4     18.7  396.90   5.33   36.2  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузка библиотек\n",
    "import numpy as np # для работы с массивами\n",
    "import pandas as pd # для работы с DataFrame \n",
    "#from sklearn import datasets # для импорта данных\n",
    "import seaborn as sns # для визуализации статистических данных\n",
    "import matplotlib.pyplot as plt # для построения графиков\n",
    "\n",
    "# загружаем датасет\n",
    "\"\"\"boston = datasets.load_boston()\n",
    "boston_data = pd.DataFrame(\n",
    "    data=boston.data, #данные\n",
    "    columns=boston.feature_names #наименования столбцов\n",
    ")\n",
    "boston_data['PRICE'] = boston.target\"\"\"\n",
    "column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'PRICE']\n",
    "boston_data = pd.read_csv('housing.csv', header=None, delimiter=r\"\\s+\", names=column_names)\n",
    "boston_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Формируем матрицу $A$ из столбца единиц и факторов $CRIM$ и $RM$, а также вектор целевой переменной $y$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0000e+00 6.3200e-03 6.5750e+00]\n",
      " [1.0000e+00 2.7310e-02 6.4210e+00]\n",
      " [1.0000e+00 2.7290e-02 7.1850e+00]\n",
      " ...\n",
      " [1.0000e+00 6.0760e-02 6.9760e+00]\n",
      " [1.0000e+00 1.0959e-01 6.7940e+00]\n",
      " [1.0000e+00 4.7410e-02 6.0300e+00]]\n"
     ]
    }
   ],
   "source": [
    "# составляем матрицу А и вектор целевой переменной\n",
    "CRIM = boston_data['CRIM']\n",
    "RM = boston_data['RM']\n",
    "A = np.column_stack((np.ones(506), CRIM, RM))\n",
    "y = boston_data[['PRICE']]\n",
    "print(A)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на размерность матрицы $A$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 3)\n"
     ]
    }
   ],
   "source": [
    "# проверим размерность\n",
    "print(A.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нам ничего не мешает вычислить оценку вектора коэффициентов $w$ по выведенной нами формуле МНК:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-29.24471945]\n",
      " [ -0.26491325]\n",
      " [  8.39106825]]\n"
     ]
    }
   ],
   "source": [
    "# вычислим OLS-оценку для коэффициентов\n",
    "w_hat = np.linalg.inv(A.T@A)@A.T@y\n",
    "print(w_hat.values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь составим прогноз нашей модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37.85733519]\n"
     ]
    }
   ],
   "source": [
    "# добавились новые данные:\n",
    "CRIM_new = 0.1\n",
    "RM_new = 8\n",
    "# делаем прогноз типичной стоимости дома\n",
    "PRICE_new = w_hat.iloc[0]+w_hat.iloc[1]*CRIM_new+w_hat.iloc[2]*RM_new\n",
    "print(PRICE_new.values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Согласитесь, такая запись вычисления оценки стоимости слишком длинная и неудобная, особенно если факторов не два, как у нас, а 200. Более короткий способ сделать прогноз — вычислить скалярное произведение вектора признаков и коэффициентов регрессии.\n",
    "\n",
    "Для удобства дальнейшего использования оформим характеристики нового наблюдения в виде матрицы размером $(1, 3)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: [[37.85733519]]\n"
     ]
    }
   ],
   "source": [
    "new=np.array([[1,CRIM_new,RM_new]])\n",
    "print('prediction:', (new@w_hat).values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Примечание**. Обратите внимание, что, решая задачу с помощью `Python`, мы получили немного другой результат прогноза стоимости. Это связано с тем, что при выполнении ручного расчёта мы округлили значения коэффициентов и получили менее точный результат.\n",
    "\n",
    "Мы уже знаем, что алгоритм построения модели линейной регрессии по МНК реализован в классе `LinearRegression`, находящемся в модуле `sklearn.linear_model`. Для вычисления коэффициентов (обучения модели) нам достаточно передать в метод `fit()` нашу матрицу с наблюдениями и вектор целевой переменной, а для построения прогноза — вызвать метод `predict()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_hat: [[-29.24471945  -0.26491325   8.39106825]]\n",
      "prediction: [[37.85733519]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "# создаём модель линейной регрессии\n",
    "model = LinearRegression(fit_intercept=False)\n",
    "# вычисляем коэффициенты регрессии\n",
    "model.fit(A, y)\n",
    "print('w_hat:', model.coef_)\n",
    "new_prediction = model.predict(new)\n",
    "print('prediction:', new_prediction)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Примечание. Здесь при создании объекта класса `LinearRegression` мы указали `fit_itercept=False`, так как в нашей матрице наблюдений $A$ уже присутствует столбец с единицами для умножения на свободный член $w_0$. Его повторное добавление не имеет смысла."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделайте прогноз типичной стоимости (в тыс. долларов) дома в городе с уровнем преступности $CRIM=0.2$ и средним количеством комнат в доме $RM=6$. В качестве модели используйте линейную регрессию, оценка вектора коэффициентов которой равна: $\\hat{\\vec{w}} = (-29.3, \\ -0.26, \\ 8.4)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21.04870738]\n"
     ]
    }
   ],
   "source": [
    "# добавились новые данные:\n",
    "CRIM_new = 0.2\n",
    "RM_new = 6\n",
    "# делаем прогноз типичной стоимости дома\n",
    "PRICE_new = w_hat.iloc[0]+w_hat.iloc[1]*CRIM_new+w_hat.iloc[2]*RM_new\n",
    "print(PRICE_new.values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ПРОБЛЕМЫ В КЛАССИЧЕСКОЙ МНК-МОДЕЛИ\n",
    "\n",
    "Заметим, что в уравнении классической `OLS`-регрессии присутствует очень важный множитель $A^TA$:\n",
    "\n",
    "$\\vec{w}=\\left(A^{T} A\\right)^{-1} A^{T}\\vec{y}$\n",
    "\n",
    "Вы могли заметить, что это матрица Грама значений наших признаков, включая признак-константу.\n",
    "\n",
    "Вспомним свойства этой матрицы: \n",
    "\n",
    "* квадратная (размерности $k+1$ на $K+1$, где $k$ — количество факторов);\n",
    "\n",
    "* симметричная.\n",
    "\n",
    ">Как и у любого метода, у классической `OLS`-регрессии есть свои **ограничения**. Если матрица $A^TA$ вырождена или близка к вырожденной, то хорошего решения у классической модели не получится. Такие данные называют **плохо обусловленными**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Корректна ли модель классической `OLS`-регрессии, если\n",
    "\n",
    "$\\vec{y}=\\left(\\begin{array}{} 1\\\\2\\\\5\\\\1\\end{array}\\right), \\vec{x_1}=\\left(\\begin{array}{} 2\\\\1\\\\1\\\\2\\end{array}\\right), \\vec{x_2}=\\left(\\begin{array}{} -2\\\\-1\\\\-1\\\\-2\\end{array}\\right)$\n",
    "\n",
    "Запишем матрицу $A$ и вычислим $A^TA$:\n",
    "\n",
    "$A=(\\vec{1}, \\vec{x_1}, \\vec{x_2})=\\left(\\begin{array}{} 1&2&-2\\\\1&1&-1\\\\1&1&-1\\\\1&2&-2\\end{array}\\right)$\n",
    "\n",
    "$A^TA=\\left(\\begin{array}{} 1&1&1&1\\\\2&1&1&2\\\\-2&-1&-1&-2\\end{array}\\right)\\cdot\\left(\\begin{array}{} 1&2&-2\\\\1&1&-1\\\\1&1&-1\\\\1&2&-2\\end{array}\\right)=\\left(\\begin{array}{} 4&6&-6\\\\6&10&-10\\\\-6&-10&10\\end{array}\\right)$\n",
    "\n",
    "Как видите, две последние строки матрицы $A^TA$ являются пропорциональными. Это говорит о том, что матрица вырождена $(det A^T A =0)$ или её ранг $(rkA)$ меньше количества неизвестных $(3)$, а значит обратной матрицы $(A^TA)^{-1}$ к ней не существует. Отсюда следует, что классическая `OLS`-модель **неприменима для этих данных**.\n",
    "\n",
    ">Борьба с вырожденностью матрицы $A^TA$ часто сводится к устранению «плохих» (зависимых) признаков. Для этого анализируют корреляционную матрицу признаков или матрицу их значений. Но иногда проблема может заключаться, например, в том, что один признак измерен в тысячных долях, а другой — в тысячах единиц. Тогда коэффициенты при них могут отличаться в миллион раз, что потенциально может привести к вырожденности матрицы $A^TA$.\n",
    "\n",
    "В устранении этой проблемы может помочь знакомая нам **нормализация/стандартизация данных**.\n",
    "\n",
    "## ОСОБЕННОСТИ КЛАССА LINEAR REGRESSION БИБЛИОТЕКИ SKLEARN\n",
    "\n",
    "Давайте посмотрим, что «скажет» `Python`, если мы попробуем построить модель линейной регрессии на вырожденной матрице наблюдений, используя классическую формулу линейной регрессии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Личные документы\\Python\\Py\\math&ml-2\\math&ml-2_lecture.ipynb Cell 56\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/%D0%9B%D0%B8%D1%87%D0%BD%D1%8B%D0%B5%20%D0%B4%D0%BE%D0%BA%D1%83%D0%BC%D0%B5%D0%BD%D1%82%D1%8B/Python/Py/math%26ml-2/math%26ml-2_lecture.ipynb#Y130sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m5\u001b[39m, \u001b[39m1\u001b[39m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/%D0%9B%D0%B8%D1%87%D0%BD%D1%8B%D0%B5%20%D0%B4%D0%BE%D0%BA%D1%83%D0%BC%D0%B5%D0%BD%D1%82%D1%8B/Python/Py/math%26ml-2/math%26ml-2_lecture.ipynb#Y130sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# вычислим OLS-оценку для коэффициентов\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/%D0%9B%D0%B8%D1%87%D0%BD%D1%8B%D0%B5%20%D0%B4%D0%BE%D0%BA%D1%83%D0%BC%D0%B5%D0%BD%D1%82%D1%8B/Python/Py/math%26ml-2/math%26ml-2_lecture.ipynb#Y130sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m w_hat\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39;49mlinalg\u001b[39m.\u001b[39;49minv(A\u001b[39m.\u001b[39;49mT\u001b[39m@A\u001b[39;49m)\u001b[39m@A\u001b[39m\u001b[39m.\u001b[39mT\u001b[39m@y\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/%D0%9B%D0%B8%D1%87%D0%BD%D1%8B%D0%B5%20%D0%B4%D0%BE%D0%BA%D1%83%D0%BC%D0%B5%D0%BD%D1%82%D1%8B/Python/Py/math%26ml-2/math%26ml-2_lecture.ipynb#Y130sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(w_hat)\n",
      "File \u001b[1;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36minv\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\AubakirovMA\\Anaconda3\\lib\\site-packages\\numpy\\linalg\\linalg.py:545\u001b[0m, in \u001b[0;36minv\u001b[1;34m(a)\u001b[0m\n\u001b[0;32m    543\u001b[0m signature \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mD->D\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m isComplexType(t) \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39md->d\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    544\u001b[0m extobj \u001b[39m=\u001b[39m get_linalg_error_extobj(_raise_linalgerror_singular)\n\u001b[1;32m--> 545\u001b[0m ainv \u001b[39m=\u001b[39m _umath_linalg\u001b[39m.\u001b[39;49minv(a, signature\u001b[39m=\u001b[39;49msignature, extobj\u001b[39m=\u001b[39;49mextobj)\n\u001b[0;32m    546\u001b[0m \u001b[39mreturn\u001b[39;00m wrap(ainv\u001b[39m.\u001b[39mastype(result_t, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m))\n",
      "File \u001b[1;32mc:\\Users\\AubakirovMA\\Anaconda3\\lib\\site-packages\\numpy\\linalg\\linalg.py:88\u001b[0m, in \u001b[0;36m_raise_linalgerror_singular\u001b[1;34m(err, flag)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_raise_linalgerror_singular\u001b[39m(err, flag):\n\u001b[1;32m---> 88\u001b[0m     \u001b[39mraise\u001b[39;00m LinAlgError(\u001b[39m\"\u001b[39m\u001b[39mSingular matrix\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "# создадим вырожденную матрицу А\n",
    "A = np.array([\n",
    "    [1, 1, 1, 1], \n",
    "    [2, 1, 1, 2], \n",
    "    [-2, -1, -1, -2]]\n",
    ").T\n",
    "y = np.array([1, 2, 5, 1])\n",
    "# вычислим OLS-оценку для коэффициентов\n",
    "w_hat=np.linalg.inv(A.T@A)@A.T@y\n",
    "print(w_hat) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как и ожидалось, мы получили ошибку, говорящую о том, что матрица $A^TA$ — сингулярная (вырожденная), а значит обратить её не получится. Что и требовалось доказать — с математикой всё сходится.\n",
    "\n",
    "⭐ Настало время фокусов!\n",
    "\n",
    "Попробуем обучить модель линейной регрессии `LinearRegression` из модуля `sklearn`, используя нашу вырожденную матрицу :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_hat: [ 6.   -1.25  1.25]\n"
     ]
    }
   ],
   "source": [
    "# создаём модель линейной регрессии\n",
    "model = LinearRegression(fit_intercept=False)\n",
    "# вычисляем коэффициенты регрессии\n",
    "model.fit(A, y)\n",
    "print('w_hat:', model.coef_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Никакой ошибки не возникло! Более того, у нас даже получились вполне адекватные оценки коэффициентов линейной регрессии $\\hat{\\vec{w}}$.\n",
    "\n",
    "?Но ведь мы только что использовали формулу для вычисления коэффициентов при расчётах вручную и получали ошибку. Как мы могли получить результат, если матрица $A^TA$ вырожденная? Существование обратной матрицы для неё противоречит законам линейной алгебры. Неужели это очередной случай, когда «мнения» математики и Python расходятся?\n",
    "\n",
    "На самом деле, не совсем. Здесь нет никакой магии, ошибки округления или бага. Просто в реализации линейной регрессии в `sklearn` предусмотрена **борьба с плохо определёнными (близкими к вырожденным и вырожденными) матрицами**.\n",
    "\n",
    ">Для этого используется метод под названием **сингулярное разложение** (`SVD`). О нём мы будем говорить отдельно, однако уже сейчас отметим тот факт, что данный метод позволяет всегда получать корректные значения при обращении матриц.\n",
    ">\n",
    ">Если вы хотите понять, почему так происходит, ознакомьтесь с этой [статьёй](https://towardsdatascience.com/understanding-linear-regression-using-the-singular-value-decomposition-1f37fb10dd33).\n",
    ">\n",
    ">**Суть метода** заключается в том, что в `OLS`-формуле мы на самом деле используем не саму матрицу $A$, а её диагональное представление из сингулярного разложения, которое гарантированно является невырожденным. Вот и весь секрет.\n",
    "\n",
    "?Правда, открытым остаётся вопрос: **можно ли доверять коэффициентам**, полученным таким способом, и интерпретировать их? \n",
    "\n",
    "В дальнейшем мы увидим, что делать этого лучше не стоит: возможна такая ситуация, при которой коэффициенты при линейно зависимых факторах, которые получаются в результате применения линейной регрессии через сингулярное разложение, могут получиться слишком большими по модулю. Они могут измеряться миллионами, миллиардами и более высокими порядками, что не будет иметь отношения к действительности. Такие коэффициенты не подлежат интерпретации.\n",
    "\n",
    "Заметим, что в случае использования решения через сингулярное разложение для линейно зависимых столбцов коэффициенты будут всегда получаться одинаковыми по модулю, но различными по знаку: $w_1=-1.25$ и $w_2=1.25$. Неудивительно, ведь второй и третий столбцы матрицы $A$ линейно зависимы с коэффициентом $-1$.\n",
    "\n",
    "Запишем итоговое уравнение линейной регрессии:\n",
    "\n",
    "$y=w_{0} \\overrightarrow{1}+w_{1} \\vec{x}_{1}+w_{2} \\vec{x}_{2}=6-1.25 \\cdot \\vec{x}_{1}+1.25 \\cdot \\vec{x}_{2},$\n",
    "\n",
    "поставим столбцы матрицы $A$ в данное уравнение, чтобы получить прогноз:\n",
    "\n",
    "$\\vec{y}=6\\left(\\begin{array}{} 1\\\\1\\\\1\\\\1\\end{array}\\right)-1.25\\left(\\begin{array}{} 2\\\\1\\\\1\\\\2\\end{array}\\right)+1.25\\left(\\begin{array}{} -2\\\\-1\\\\-1\\\\-2\\end{array}\\right)=\\left(\\begin{array}{} 1\\\\3.5\\\\3.5\\\\1\\end{array}\\right)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Примечание**. На самом деле сингулярное разложение зашито в функцию `np.linalg.lstsq()`, которая позволяет в одну строку построить модель линейной регрессии по МНК:\n",
    ">\n",
    ">классическая `OLS`-регрессия в numpy с возможностью получения решения даже для вырожденных матриц\n",
    ">\n",
    ">`np.linalg.lstsq(A, y, rcond=None)`\n",
    ">\n",
    ">Функция возвращает четыре значения:\n",
    ">\n",
    ">* вектор рассчитанных коэффициентов линейной регрессии;\n",
    ">\n",
    ">* сумму квадратов ошибок, `MSE` (она не считается, если ранг матрицы $A$ меньше числа неизвестных, как в нашем случае);\n",
    ">\n",
    ">* ранг матрицы $A$;\n",
    ">\n",
    ">* вектор из сингулярных значений, которые как раз и оберегают нас от ошибки (о них мы поговорим позже).\n",
    ">\n",
    ">Обратите внимание, что мы получили те же коэффициенты, что и с помощью `sklearn`. При этом ранг матрицы $A$ равен 2, что меньше количества неизвестных коэффициентов. Это ожидаемо говорит о вырожденности матрицы $A$ и, как следствие, матрицы $A^TA$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "## Резюмируем ↓\n",
    "\n",
    "* Для поиска коэффициентов модели линейной регрессии используется МНК-оценка: \n",
    "\n",
    "$\\vec{w}=\\left(A^{T} A\\right)^{-1} A^{T} \\vec{y}$\n",
    "\n",
    "* Полученная матричная формула не зависит от размерности матрицы наблюдений $A$ и работает при любом количестве объектов/признаков в данных.\n",
    "\n",
    "* Для реализации обучения модели линейной регрессии по `МНК` в `sklearn` используется класс `LinearRegression`.\n",
    "\n",
    "* Для предотвращения обращения вырожденной матрицы $A$ в `LinearRegression` вместо самой матрицы используется её сингулярное разложение. Поэтому на практике при построении модели линейной регрессии вместо ручного вычисления обратной матрицы с помощью `np.inv()` приоритетнее пользоваться именно `LinearRegression` из `sklearn` (или `np.linalg.lstsq()`).\n",
    "\n",
    "Данный метод оберегает от ошибки только при обращении плохо обусловленных и вырожденных матриц и не гарантирует получение корректных коэффициентов линейной регрессии."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Стандартизация векторов и матрица корреляции <a class=\"anchor\" id=4></a>\n",
    "\n",
    "[к содержанию](#0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## СТАНДАРТИЗАЦИЯ ВЕКТОРОВ\n",
    "\n",
    "В модулях по разведывательному анализу данных и машинному обучению мы не раз говорили о преобразованиях признаков путём нормализации и стандартизации. Вспомним, что это такое ↓\n",
    "\n",
    ">**Нормализация** — это процесс приведения признаков к единому масштабу, например от 0 до 1. Пример — min-max-нормализация:\n",
    ">\n",
    ">$x_{scaled} =  \\frac{x - x_{min}}{x_{max} - x_{min}}$\n",
    ">\n",
    ">**Стандартизация** — это процесс приведения признаков к единому масштабу характеристик распределения — нулевому среднему и единичному стандартному отклонению:\n",
    ">\n",
    ">$x_{scaled} =  \\frac{x - x_{mean}}{x_{std}}$\n",
    "\n",
    "В линейной алгебре под стандартизацией вектора $\\vec{x} \\in \\mathbb{R}^n$ понимается несколько другая операция, которая проходит в два этапа:\n",
    "\n",
    "1. Центрирование вектора — это операция приведения среднего к 0:\n",
    "\n",
    "$\\vec{x}_{cent} = \\vec{x} - \\vec{x}_{mean}$\n",
    "\n",
    "2. Нормирование вектора — это операция приведения диапазона вектора к масштабу от -1 до 1 путём деления центрированного вектора на его длину:\n",
    "\n",
    "$\\vec{x}_{st} =  \\frac{\\vec{x}_{cent}}{ \\| \\vec{x}_{cent} \\| },$\n",
    "\n",
    "где $\\vec{x}_{mean}$ — вектор, составленный из среднего значения вектора $\\vec{x}$, а $\\| \\vec{x}_{cent} \\|$ — длина вектора  $\\vec{x}_{cent}$.\n",
    "\n",
    "В результате стандартизации вектора всегда получается новый вектор, длина которого равна 1:\n",
    "\n",
    "$\\| \\vec{x}_{st} \\|  = 1$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Пример № 1**\n",
    "\n",
    "Необходимо стандартизировать векторы:\n",
    "\n",
    "$\\vec{x_1}=\\left(\\begin{array}{} 1\\\\2\\\\3 \\end{array}\\right)$ и $\\vec{x_2}=\\left(\\begin{array}{} 3000\\\\1000\\\\2000 \\end{array}\\right)$\n",
    "\n",
    "Центрируем:\n",
    "\n",
    "$\\vec{x_1}=\\frac{1+2+3}{3}=3$\n",
    "\n",
    "$\\vec{x}_{1cent}=\\left(\\begin{array}{} 1\\\\2\\\\3 \\end{array}\\right)-\\left(\\begin{array}{} 3\\\\3\\\\3 \\end{array}\\right)=\\left(\\begin{array}{} -2\\\\-1\\\\3 \\end{array}\\right)$\n",
    "\n",
    "$\\vec{x_2}=\\frac{3000+1000+2000}{3}=2000$\n",
    "\n",
    "$\\vec{x}_{2cent}=\\left(\\begin{array}{} 3000\\\\1000\\\\2000 \\end{array}\\right)-\\left(\\begin{array}{} 2000\\\\2000\\\\2000 \\end{array}\\right)=\\left(\\begin{array}{} 1000\\\\-1000\\\\0 \\end{array}\\right)$\n",
    "\n",
    "Нормируем:\n",
    "\n",
    "$\\| \\vec{x}_{1cent} \\| =\\sqrt{(-2)^2 + (-1)^2 + 3^2}=\\sqrt{14}$\n",
    "\n",
    "$\\| \\vec{x}_{1st} \\| = \\frac{1}{\\| \\vec{x}_{1cent} \\|}\\cdot\\vec{x}_{1cent}=\\frac{1}{\\sqrt{14}}\\cdot\\left(\\begin{array}{} -2\\\\-1\\\\3 \\end{array}\\right)\\approx\\left(\\begin{array}{} -0.535\\\\-0.267\\\\0.802 \\end{array}\\right)$\n",
    "\n",
    "$. $\n",
    "\n",
    "$\\| \\vec{x}_{2cent} \\| =\\sqrt{(1000)^2 + (-1000)^2 + 0^2}=1000\\sqrt{2}$\n",
    "\n",
    "$\\| \\vec{x}_{2st} \\| = \\frac{1}{\\| \\vec{x}_{2cent} \\|}\\cdot\\vec{x}_{2cent}=\\frac{1}{1000\\sqrt{2}}\\cdot\\left(\\begin{array}{} 1000\\\\-1000\\\\0 \\end{array}\\right)\\approx\\left(\\begin{array}{} 0.707\\\\-0.707\\\\0 \\end{array}\\right)$\n",
    "\n",
    "Как видите, теперь оба признака имеют значения от -1 до 1 и равный порядок, в отличие от исходных признаков.\n",
    "\n",
    "Давайте посмотрим, что произойдёт с матрицей Грама после стандартизации векторов $\\vec{x}_1$ и $\\vec{x}_2$:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Пример № 2**\n",
    "\n",
    "Найти матрицу для стандартизированных признаков для\n",
    "\n",
    "$\\vec{x_1}=\\left(\\begin{array}{} 1\\\\2\\\\3 \\end{array}\\right)$ и $\\vec{x_2}=\\left(\\begin{array}{} 3000\\\\1000\\\\2000 \\end{array}\\right)$\n",
    "\n",
    "Вычислим попарные скалярные произведения новых признаков:\n",
    "\n",
    "$G(\\vec{x}_{1,st},\\vec{x}_{2,st})=G(\\left(\\begin{array}{} -0.535\\\\-0.267\\\\0.802 \\end{array}\\right),\\left(\\begin{array}{} 0.707\\\\-0.707\\\\0 \\end{array}\\right))=\\left(\\begin{array}{} (\\vec{x}_{1,st},\\vec{x}_{1,st}) & (\\vec{x}_{1,st},\\vec{x}_{2,st}) \\\\ (\\vec{x}_{2,st},\\vec{x}_{1,st}) & (\\vec{x}_{2,st},\\vec{x}_{2,st})  \\end{array}\\right)=\\left(\\begin{array}{} 1 & -0.189 \\\\ -0.189 & 1 \\end{array}\\right)$\n",
    "\n",
    "Как видите, все числа — в диапазоне от -1 до 1. \n",
    "\n",
    ">Забегая вперёд, скажем, что это так называемые **выборочные корреляции признаков**, а сама матрица является **матрицей корреляций** или **корреляционной матрицей**. Пока просто запомните, как выглядит эта матрица.\n",
    "\n",
    "Вот **ещё одна особенность стандартизации** ↓\n",
    "\n",
    "До стандартизации мы прогоняли регрессию $y$ на регрессоры $x_1, x_2, …, x_k$ и константу. Всего получалось $k+1$ коэффициентов:\n",
    "\n",
    "$\\vec{y}=w_{0}+w_{1} \\vec{x}_{1}+w_{2} \\vec{x}_{2}+\\ldots+w_{k} \\vec{x}_{k}$\n",
    "\n",
    "После стандартизации мы прогоняем регрессию стандартизованного $y$ на стандартизованные регрессоры **без константы**:\n",
    "\n",
    "$\\vec{y}=w_{1_{st}} \\vec{x}_{1_{st}}+w_{2_{st}} \\vec{x}_{2_{st}}+\\ldots+w_{k_{st}} \\vec{x}_{k_{st}}$\n",
    "\n",
    "Математически мы получим одну и ту же регрессию в том смысле, что если пересчитать стандартизированные коэффициенты, мы получим исходные. То же и с прогнозом (пересчёт здесь опустим).\n",
    "\n",
    "***\n",
    "\n",
    "**В ЧЁМ БОНУСЫ?**\n",
    "\n",
    "Математика говорит, что регрессия исходного $y$ на исходные («сырые») признаки c константой точно такая же, как регрессия стандартизированного на стандартизированные признаки без константы. В чём же разница? Математически — ни в чём.\n",
    "\n",
    "На прогноз модели линейной регрессии, построенной по МНК, и её качество стандартизация практически не влияет. Масштабы признаков будут иметь значение только в том случае, если для поиска коэффициентов вы используете численные методы, такие как градиентный спуск (`SGDRegressor` из `sklearn`). О нём мы поговорим, когда будем знакомиться с алгоритмом градиентного спуска в модуле по оптимизации.\n",
    "\n",
    "Однако с точки зрения интерпретации важности коэффициентов разница есть. Если вы занимаетесь отбором наиболее важных признаков по значению коэффициентов линейной регрессии на нестандартизированных данных, это будет не совсем корректно: один признак может изменяться от 0 до 1, а второй — от -1000 до 1000. Коэффициенты при них также будут различного масштаба. Если же вы посмотрите оценки коэффициентов регрессии после стандартизации, то они будут в едином масштабе, что даст более цельную и объективную картину.\n",
    "\n",
    "Более важный бонус заключается в том, что после стандартизации матрица Грама признаков как по волшебству превращается в корреляционную матрицу, о которой пойдёт речь далее. Почему это хорошо? На свойства корреляционной матрицы опираются такие алгоритмы, как метод главных компонент и сингулярное разложение, а так как «сырая» и стандартизированная регрессия математически эквивалентны, то имеет смысл исследовать стандартизированную, а результаты обобщить на «сырую»."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Пример № 3**\n",
    "\n",
    "Вновь рассмотрим данные о стоимости жилья в районах Бостона.\n",
    "\n",
    "На этот раз возьмём четыре признака: `CHAS`, `LSTAT`, `CRIM` и `RM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CHAS</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>CRIM</th>\n",
       "      <th>RM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.069170</td>\n",
       "      <td>12.653063</td>\n",
       "      <td>3.613524</td>\n",
       "      <td>6.284634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.253994</td>\n",
       "      <td>7.141062</td>\n",
       "      <td>8.601545</td>\n",
       "      <td>0.702617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.730000</td>\n",
       "      <td>0.006320</td>\n",
       "      <td>3.561000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.950000</td>\n",
       "      <td>0.082045</td>\n",
       "      <td>5.885500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.360000</td>\n",
       "      <td>0.256510</td>\n",
       "      <td>6.208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.955000</td>\n",
       "      <td>3.677083</td>\n",
       "      <td>6.623500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>37.970000</td>\n",
       "      <td>88.976200</td>\n",
       "      <td>8.780000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             CHAS       LSTAT        CRIM          RM\n",
       "count  506.000000  506.000000  506.000000  506.000000\n",
       "mean     0.069170   12.653063    3.613524    6.284634\n",
       "std      0.253994    7.141062    8.601545    0.702617\n",
       "min      0.000000    1.730000    0.006320    3.561000\n",
       "25%      0.000000    6.950000    0.082045    5.885500\n",
       "50%      0.000000   11.360000    0.256510    6.208500\n",
       "75%      0.000000   16.955000    3.677083    6.623500\n",
       "max      1.000000   37.970000   88.976200    8.780000"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_data[['CHAS', 'LSTAT', 'CRIM','RM']].describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что каждый из признаков измеряется в различных единицах и изменяется в различных диапазонах: например, `CHAS` лежит в диапазоне от 0 до 1, а вот `CRIM` — в диапазоне от 0.006 до 88.976.\n",
    "\n",
    "Рассмотрим модель линейной регрессии по МНК без стандартизации. Помним, что необходимо добавить столбец из единиц:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.92052548]\n",
      " [ 3.9975594 ]\n",
      " [-0.58240212]\n",
      " [-0.09739445]\n",
      " [ 5.07554248]]\n"
     ]
    }
   ],
   "source": [
    "# составляем матрицу наблюдений и вектор целевой переменной\n",
    "A = np.column_stack((np.ones(506), boston_data[['CHAS', 'LSTAT', 'CRIM','RM']]))\n",
    "y = boston_data[['PRICE']]\n",
    "# вычисляем OLS-оценку для коэффициентов без стандартизации\n",
    "w_hat=np.linalg.inv(A.T@A)@A.T@y\n",
    "print(w_hat.values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот наши коэффициенты. Округлим их для наглядности:\n",
    "\n",
    "$\\hat{w}_0 = -1.92$\n",
    "\n",
    "$\\hat{w}_{CHAS} = 4$\n",
    "\n",
    "$\\hat{w}_{LSTAT} = -0.6$\n",
    "\n",
    "$\\hat{w}_{CRIM} = -0.1$\n",
    "\n",
    "$\\hat{w}_{RM} = 5$\n",
    "\n",
    "Давайте вспомним интерпретацию коэффициентов построенной модели линейной регрессии, которую мы изучали в модуле «ML-2. Обучение с учителем: регрессия». Значение коэффициента $\\hat{w}_i$ означает, на сколько в среднем изменится медианная цена (в тысячах долларов) при увеличении $x_i$ на 1.\n",
    "\n",
    "Например, если количество низкостатусного населения (`LSTAT`) увеличится на 1 %, то медианная цена домов в районе (в среднем) упадёт на 0.1 тысяч долларов. А если среднее количество комнат (`RM`) в районе станет больше на 1, то медианная стоимость домов в районе (в среднем) увеличится на 5 тысяч долларов. \n",
    "\n",
    ">Тут в голову может прийти мысль: судя по значению коэффициентов, количество комнат (`RM`) оказывает на стоимость жилья большее влияние, чем процент низкостатусного населения (`LSTAT`). Однако **такой вывод будет ошибочным**. Мы не учитываем, что признаки, а значит и коэффициенты линейной регрессии, лежат в разных масштабах. Чтобы говорить о важности влияния признаков на модель, нужно строить её на стандартизированных данных.\n",
    "\n",
    "Помним, что для построения стандартизированной линейной регрессии нам не нужен вектор свободных коэффициентов, а значит и столбец из единиц тоже не понадобится.\n",
    "\n",
    "Сначала центрируем векторы, которые находятся в столбцах матрицы $A$. Для этого вычтем среднее, вычисленное по строкам матрицы $A$ в каждом столбце, с помощью метода `mean()`. Затем разделим результат на длины центрированных векторов, вычисленных с помощью функции `linalg.norm()`.\n",
    "\n",
    ">**Примечание**. Обратите внимание, что для функции `linalg.norm()` обязательно необходимо указать параметр `axis=0`, так как по умолчанию норма считается для всей матрицы, а не для каждого столбца в отдельности. С определением нормы матрицы и тем, как она считается, вы можете ознакомиться в [документации к функции norm()](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CHAS</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>CRIM</th>\n",
       "      <th>RM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>506.00</td>\n",
       "      <td>506.00</td>\n",
       "      <td>506.00</td>\n",
       "      <td>506.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.16</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         CHAS   LSTAT    CRIM      RM\n",
       "count  506.00  506.00  506.00  506.00\n",
       "mean    -0.00   -0.00    0.00   -0.00\n",
       "std      0.04    0.04    0.04    0.04\n",
       "min     -0.01   -0.07   -0.02   -0.17\n",
       "25%     -0.01   -0.04   -0.02   -0.03\n",
       "50%     -0.01   -0.01   -0.02   -0.00\n",
       "75%     -0.01    0.03    0.00    0.02\n",
       "max      0.16    0.16    0.44    0.16"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# составляем матрицу наблюдений без дополнительного столбца из единиц\n",
    "A = boston_data[['CHAS', 'LSTAT', 'CRIM','RM']]\n",
    "y = boston_data[['PRICE']]\n",
    "# стандартизируем векторы в столбцах матрицы A\n",
    "A_cent = A - A.mean()\n",
    "A_st = A_cent/np.linalg.norm(A_cent, axis=0)\n",
    "A_st.describe().round(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь векторы имеют одинаковые средние значения и стандартные отклонения. Если вычислить длину каждого из векторов, мы увидим, что они будут равны 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(np.linalg.norm(A_st, axis=0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для получения стандартизированных коэффициентов нам также понадобится стандартизация целевой переменной $y$ по тому же принципу:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# стандартизируем вектор целевой переменной\n",
    "y_cent = y - y.mean()\n",
    "y_st = y_cent/np.linalg.norm(y_cent)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Формула для вычисления коэффициента та же, что и раньше, только матрица $A$ теперь заменяется на $A_{st}$, а $y$ — на $y_{st}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.11039956]\n",
      " [-0.45220423]\n",
      " [-0.09108766]\n",
      " [ 0.38774848]]\n"
     ]
    }
   ],
   "source": [
    "# вычислим OLS-оценку для стандартизированных коэффициентов\n",
    "w_hat_st=np.linalg.inv(A_st.T@A_st)@A_st.T@y_st\n",
    "print(w_hat_st.values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вновь смотрим на коэффициенты. Помним, что коэффициента $\\hat{w}_0$ у нас больше нет:\n",
    "\n",
    "$\\hat{w}_{CHAS} = 0.11$\n",
    "\n",
    "$\\hat{w}_{LSTAT} = -0.45$\n",
    "\n",
    "$\\hat{w}_{CRIM} = -0.09$\n",
    "\n",
    "$\\hat{w}_{RM} = 0.38$\n",
    "\n",
    "Итак, мы видим картину, прямо противоположную той, что видели ранее. Теперь модуль коэффициента $\\left|\\hat{w}_{LSTAT, \\ st} \\right| = 0.45$ будет выше, чем модуль коэффициента $\\left|\\hat{w}_{RM, \\ st} \\right| = 0.38$. Значит, процент низкостатусного населения оказывает большее влияние на значение стоимости жилья, чем количество комнат.\n",
    "\n",
    "Однако теперь интерпретировать сами коэффициенты в тех же измерениях у нас не получится.\n",
    "\n",
    "***\n",
    "\n",
    "**Сделаем важный вывод** ↓\n",
    "\n",
    "Для того чтобы проинтерпретировать оценки коэффициентов линейной регрессии (понять, каков будет прирост целевой переменной при изменении фактора на 1 условную единицу), нам достаточно построить линейную регрессию в обычном виде без стандартизации и получить обычный вектор $\\hat{\\vec{w}}$.\n",
    "\n",
    "Однако, чтобы корректно говорить о том, какой фактор оказывает на прогноз большее влияние, необходимо рассматривать стандартизированную оценку вектора коэффициентов $\\hat{\\vec{w}}_{st}$.\n",
    "\n",
    "Давайте поближе взглянем на матрицу Грама для стандартизированных факторов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CHAS</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>CRIM</th>\n",
       "      <th>RM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CHAS</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.053929</td>\n",
       "      <td>-0.055892</td>\n",
       "      <td>0.091251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LSTAT</th>\n",
       "      <td>-0.053929</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.455621</td>\n",
       "      <td>-0.613808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRIM</th>\n",
       "      <td>-0.055892</td>\n",
       "      <td>0.455621</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.219247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RM</th>\n",
       "      <td>0.091251</td>\n",
       "      <td>-0.613808</td>\n",
       "      <td>-0.219247</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           CHAS     LSTAT      CRIM        RM\n",
       "CHAS   1.000000 -0.053929 -0.055892  0.091251\n",
       "LSTAT -0.053929  1.000000  0.455621 -0.613808\n",
       "CRIM  -0.055892  0.455621  1.000000 -0.219247\n",
       "RM     0.091251 -0.613808 -0.219247  1.000000"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# матрица Грама\n",
    "A_st.T @ A_st"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На самом деле мы с вами только что вычислили **матрицу выборочных корреляций** наших исходных факторов. Мы уже сталкивались с ней много раз в разделах по разведывательному анализу данных и машинному обучению, правда, вычисляли её мы с помощью функции `Pandas`, а теперь научились делать это вручную.\n",
    "\n",
    ">**Примечание**. Матрицу корреляций можно получить только в том случае, если производить стандартизацию признаков как векторы (делить на длину центрированного вектора $\\vec{x}_{st}$). Другие способы стандартизации/нормализации признаков не превращают матрицу Грама в матрицу корреляций."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## КОРРЕЛЯЦИОННАЯ МАТРИЦА\n",
    "\n",
    ">Напомним, что **корреляционная матрица** $C$ — это матрица выборочных корреляций между факторами регрессий.\n",
    "\n",
    "$C=\\mathbb{corr}(X)$\n",
    "\n",
    "Корреляция является одной из важнейших статистических характеристик выборки. Как мы уже знаем из модуля «EDA-2. Математическая статистика в контексте EDA», корреляцию можно измерять различным способами:\n",
    "\n",
    "* корреляцией Пирсона;\n",
    "\n",
    "* корреляцией Спирмена;\n",
    "\n",
    "* корреляцией Кендалла.\n",
    "\n",
    "В этом модуле мы будем говорить именно о **корреляции Пирсона**. Она измеряет тесноту линейных связей между непрерывными числовыми факторами и может принимать значения от -1 до +1.\n",
    "\n",
    "$c_{ij} = corr(\\vec{x}_{i}, \\vec{x}_{j})$\n",
    "\n",
    "Как и любая статистическая величина, корреляция бывает **генеральной** и **выборочной**. Разница очень тонкая, и мы подробнее разберём её в модуле по теории вероятностей.\n",
    "\n",
    ">**Генеральная (истинная) корреляция** — это теоретическая величина, которая отражает общую линейную зависимость между случайными величинами $X_i$ и $X_j$. Забегая вперёд скажем, что данная характеристика является абстрактной и вычисляется для **генеральных совокупностей** — всех возможных реализаций $X_i$ и $X_j$. В природе такой величины не существует, она есть только в теории вероятностей.\n",
    "\n",
    ">>**Выборочная корреляция** — это корреляция, вычисленная на ограниченной выборке. Это уже ближе к нашей теме. Выборочная корреляция отражает линейную взаимосвязь между факторами $\\vec{x}_i$ и $\\vec{x}_j$, реализации которых представлены в выборке.\n",
    "\n",
    "Выборочная корреляция между факторами высчитывается по громоздкой (на первый взгляд) формуле:\n",
    "\n",
    "$c_{ij} = corr(\\vec{x}_{i}, \\vec{x}_{j})=\\frac{\\sum^n_{l=1}(\\vec{x}_{il}-x_{imean})(\\vec{x}_{jl}-x_{jmean})}{\\sqrt{\\sum^n_{l=1}(\\vec{x}_{il}-x_{imean})^2\\cdot(\\vec{x}_{jl}-x_{jmean})^2}}$\n",
    "\n",
    "Из вычисленных $c_{ij}$ как раз и составляется матрица корреляций $C$. Если факторов $k$ штук, то матрица $C$ будет квадратной размера $dim C =(k,k)$:\n",
    "\n",
    "$C=\\left(\\begin{array}{} c_{11}&c_{12}&\\dots&c_{1k}\\\\ c_{21}&c_{22}&\\dots&c_{2k}\\\\ \\dots&\\dots&\\dots&\\dots\\\\ c_{k1}&c_{k2}&\\dots&c_{kk}\\end{array}\\right)$\n",
    "\n",
    "Давайте разберём представленную выше формулу на простом примере. Но сначала нас вновь будут ждать довольно сложные формулы — не пугайтесь.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Пример № 1**\n",
    "\n",
    "Найти выборочную корреляцию факторов:\n",
    "\n",
    "$\\vec{x}_1=\\left(\\begin{array}{} 1\\\\ 2\\\\ 6\\end{array}\\right)$ и $\\vec{x}_1=\\left(\\begin{array}{} 3000\\\\ 1000\\\\ 2000\\end{array}\\right)$\n",
    "\n",
    "Смотрим на формулу для выборочной корреляции. Чтобы вычислить коэффициент корреляции $c_{12}$, необходимо предварительно вычислить $\\vec{x}_{1mean}$ и $\\vec{x}_{2mean}$ — средние значения координат векторов.\n",
    "\n",
    "Мы уже вычисляли их ранее:\n",
    "\n",
    "$\\vec{x}_{1mean}=\\frac{1+2+6}{3}=3$\n",
    "\n",
    "$\\vec{x}_{2mean}=\\frac{3000+1000+2000}{3}=2000$\n",
    "\n",
    "Далее нужно вычислить числитель:\n",
    "\n",
    "$\\sum^{n}_{l=1}(\\vec{x}_{1l}-{x}_{1mean})(\\vec{x}_{2l}-{x}_{2mean})$\n",
    "\n",
    "Если присмотреться, можно заметить не что иное, как скалярное произведение векторов $(\\vec{x}_{1l}-{x}_{1mean})$ и $(\\vec{x}_{2l}-{x}_{2mean})$. Считаем:\n",
    "\n",
    "$(\\vec{x}_{1l}-{x}_{1mean})=\\left(\\begin{array}{} 1\\\\ 2\\\\ 6\\end{array}\\right)-\\left(\\begin{array}{} 3\\\\ 3\\\\ 3\\end{array}\\right)=\\left(\\begin{array}{} -2\\\\ -1\\\\ 3\\end{array}\\right)$\n",
    "\n",
    "$(\\vec{x}_{2l}-{x}_{2mean})=\\left(\\begin{array}{} 3000\\\\ 1000\\\\ 2000\\end{array}\\right)-\\left(\\begin{array}{} 2000\\\\ 2000\\\\ 2000\\end{array}\\right)=\\left(\\begin{array}{} 1000\\\\ -1000\\\\ 0\\end{array}\\right)$\n",
    "\n",
    "Кажется, мы это уже где-то видели. Да — это векторы $\\vec{x}_{1cent}$ и $\\vec{x}_{2cent}$, которые мы получили, когда стандартизировали векторы. Посчитаем их скалярное произведение:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(\\vec{x}_{1cent},\\vec{x}_{2cent})=\\left(\\left(\\begin{array}{} -2\\\\ -1\\\\ 3\\end{array}\\right),\\left(\\begin{array}{} 1000\\\\ -1000\\\\ 0\\end{array}\\right)\\right)=-2\\cdot 1000 + (-1)\\cdot (-1000)+3\\cdot 0=-1000$\n",
    "\n",
    "А что в знаменателе?\n",
    "\n",
    "$\\sqrt{\\sum^n_{l=1}(\\vec{x}_{il}-x_{imean})^2\\cdot\\sum^n_{l=1}(\\vec{x}_{jl}-x_{jmean})^2}$\n",
    "\n",
    "Это произведение длин векторов $\\vec{x}_{1cent}$ и $\\vec{x}_{2cent}$.\n",
    "\n",
    "$\\sqrt{\\sum^n_{l=1}(\\vec{x}_{il}-x_{imean})^2\\cdot\\sum^n_{l=1}(\\vec{x}_{jl}-x_{jmean})^2}=\\|\\vec{x}_{1cent}\\|\\cdot\\|\\vec{x}_{2cent}\\|$\n",
    "\n",
    "Мы также уже считали их в примере по стандартизации:\n",
    "\n",
    "$\\|\\vec{x}_{1cent}\\|=\\sqrt{(-2)^2+(-1)^2+3^2}=\\sqrt{14}$\n",
    "\n",
    "$\\|\\vec{x}_{2cent}\\|=\\sqrt{(1000)^2+(-1000)^2+0^2}=1000\\sqrt{2}$\n",
    "\n",
    "Считаем коэффициент корреляции:\n",
    "\n",
    "$c_{12}=\\frac{(\\vec{x}_{1cent},\\vec{x}_{2cent})}{\\|\\vec{x}_{1cent}\\|\\cdot\\|\\vec{x}_{2cent}\\|}=\\frac{-1000}{\\sqrt{14}\\cdot 1000 \\cdot \\sqrt{2}}=-\\frac{1}{\\sqrt{28}}\\approx-0.189$\n",
    "\n",
    "Снова знакомые числа. Да — это элемент на побочной диагонали матрицы Грама, вычисленной для стандартизированных векторов $\\vec{x}_{1cent}$ и $\\vec{x}_{2cent}$, а значит:\n",
    "\n",
    "$c_{12}=(\\vec{x}_{1cent},\\vec{x}_{2cent})$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Если посчитать корреляцию в обратном порядке между факторами $c_{21}=(\\vec{x}_{2cent},\\vec{x}_{1cent})$, получим то же самое число, ведь скалярное произведение перестановочно: $c_{12}=c_{21}$.\n",
    "\n",
    ">**Ещё один очевидный факт** → Корреляция фактора с самим собой всегда равна 1: $c_{ii}=1$, то есть $c_{11}=c_{22}=1$. Так происходит потому, что скалярное произведение вектора с самим собой всегда даёт 1 по свойствам скалярного произведения.\n",
    "\n",
    "Вот мы и нашли нашу матрицу корреляций:\n",
    "\n",
    "$C=\\left(\\begin{array}{} c_{11}&c_{12}\\\\ c_{21}&c_{22} \\end{array}\\right)=\\left(\\begin{array}{} 1&-0.189\\\\ -0.189&1 \\end{array}\\right)$\n",
    "\n",
    "Она в точности совпадает с матрицей Грама, вычисленной для стандартизированных векторов $\\vec{x}_{1st}$ и $\\vec{x}_{2st}$:\n",
    "\n",
    "$C=G(\\vec{x}_{1st},\\vec{x}_{2st})$\n",
    "\n",
    "Но «магия» ещё не закончилась. Давайте подумаем: какова геометрическая интерпретация корреляции?\n",
    "\n",
    "Присмотритесь к формуле, вспомните свойства скалярного произведения, а затем загляните в ответ:\n",
    "\n",
    "$c_{ij}=\\frac{(\\vec{x}_{icent},\\vec{x}_{jcent})}{\\| \\vec{x}_{icent}\\|\\cdot \\|\\vec{x}_{jcent} \\|}$\n",
    "\n",
    "Это косинус угла между центрированными векторами $\\vec{x}_{icent}$ и $\\vec{x}_{jcent}$. По свойству скалярного произведения:\n",
    "\n",
    "$c_{ij}=corr(\\vec{x}_{i},\\vec{x}_{j})=cos(\\widehat{\\vec{x}_{icent},\\vec{x}_{jcent}})=\\frac{(\\vec{x}_{icent},\\vec{x}_{jcent})}{\\| \\vec{x}_{icent}\\|\\cdot \\|\\vec{x}_{jcent} \\|}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Примечание**. В `NumPy` матрица корреляций вычисляется функцией `np.corrcoef()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.18898224],\n",
       "       [-0.18898224,  1.        ]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_1 = np.array([1, 2, 6])\n",
    "x_2 = np.array([3000, 1000, 2000])\n",
    "np.corrcoef(x_1, x_2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Получили тот же результат, что и раньше.\n",
    ">\n",
    ">В `Pandas` матрица корреляций вычисляется методом `corr()`, вызванным от имени `DataFrame`.\n",
    "\n",
    "***\n",
    "\n",
    "На практике корреляция с точки зрения линейной алгебры означает следующее:\n",
    "\n",
    "* Если корреляция $c_{ij}=1$, значит векторы $\\vec{x}_i$ и $\\vec{x}_j$ пропорциональны и сонаправлены.\n",
    "\n",
    "* Если корреляция $c_{ij}=-1$, значит векторы $\\vec{x}_i$ и $\\vec{x}_j$ пропорциональны и противонаправлены.\n",
    "\n",
    "* Если корреляция $c_{ij}=0$, значит векторы $\\vec{x}_i$ и $\\vec{x}_j$ ортогональны друг другу и, таким образом, являются линейно независимыми.\n",
    "\n",
    "Во всех остальных случаях между факторами $\\vec{x}_i$ и $\\vec{x}_j$ существует какая-то линейная взаимосвязь, причём чем ближе модуль коэффициента корреляции к 1, тем сильнее эта взаимосвязь. Вспомним классификацию связей факторов, которую мы рассматривали в модуле «EDA-2. Математическая статистика в контексте EDA»:\n",
    "\n",
    "<img src=m2_img4.png width=800>\n",
    "\n",
    "**Промежуточный вывод ↓**\n",
    "\n",
    "Таким образом, матрица корреляций — это матрица Грама, составленная для стандартизированных столбцов исходной матрицы наблюдений $A$. Она всегда (в теории) симметричная. На главной диагонали этой матрицы стоят 1, а на местах всех остальных элементов — коэффициенты корреляции между факторами $\\vec{x}_i$ и $\\vec{x}_j$.\n",
    "\n",
    "Если коэффициент корреляции больше 0, то взаимосвязь между факторами прямая (растёт один — растёт второй), в противном случае — обратная (растёт один — падает второй)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Пример № 2**\n",
    "\n",
    "Проинтерпретировать выборочные коэффициенты корреляции:\n",
    "\n",
    "$corr(\\vec{x}, \\vec{u}) = 1,corr(\\vec{x}, \\vec{v}) = -1,corr(\\vec{x}, \\vec{w}) = 0$\n",
    "\n",
    "Даны коэффициенты корреляции трёх пар факторов, причём это краевые значения. Что они означают?\n",
    "\n",
    "* $corr(\\vec{x}, \\vec{u}) = 1$ означает что $\\vec{x}$ и $\\vec{u}$ линейно выражаются друг через друга и имеют прямую зависимость (когда растёт один фактор, растёт и другой).\n",
    "\n",
    "* $corr(\\vec{x}, \\vec{v}) = -1$ говорит о точно такой же линейной, но обратной взаимосвязи.\n",
    "\n",
    "* $corr(\\vec{x}, \\vec{w}) = 0$ означает, что факторы не связаны, то есть один фактор не чувствителен к изменениям другого.\n",
    "\n",
    "Теперь коэффициенты принимают уже не экстремальные значения:\n",
    "\n",
    "$corr(\\vec{x}, \\vec{u}) = 0.73,corr(\\vec{x}, \\vec{v}) = -0.72,corr(\\vec{x}, \\vec{w}) = 0.12$\n",
    "\n",
    "* $corr(\\vec{x}, \\vec{u}) = 0.73$ говорит о сильной прямой взаимосвязи. Угол между векторами — острый.\n",
    "\n",
    "* $corr(\\vec{x}, \\vec{v}) = -0.72$ говорит о сильной обратной взаимосвязи. Угол между векторами — тупой.\n",
    "\n",
    "* $corr(\\vec{x}, \\vec{w}) = 0.12$ говорит о слабой прямой взаимосвязи. Угол между векторами острый, но близок к 90 градусам."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Пример № 3**\n",
    "\n",
    "Давайте посмотрим на корреляционную матрицу в задаче прогнозирования количества показов квартир агентства недвижимости «Рай в шалаше» в зависимости от разных параметров.\n",
    "\n",
    "Здесь:\n",
    "\n",
    "* `Demo 2 w` — количество показов квартир за две недели;\n",
    "* `Rub` — стоимость аренды в рублях;\n",
    "* `Area` — площадь квартиры;\n",
    "* `Liv.Area` — жилая площадь квартиры;\n",
    "* `Floor` — этаж;\n",
    "* `Euro` — стоимость аренды в евро;\n",
    "* `NLiv.Area` — нежилая площадь квартиры.\n",
    "\n",
    "<img src=m2_img5.png width=800>\n",
    "\n",
    "Матрица получилась размером `7x7`, однако её `ранг равен 5`, а **определитель — и вовсе 0**. Что это значит? Для начала заметим, что по **главной диагонали** матрицы стоят единицы — это корреляция каждого фактора с самим собой. Разумеется, матрица симметрична: в первой строке и первом столбце расположены корреляции целевого параметра, то есть количества показов со всеми остальными факторами. Чем эти корреляции больше, тем сильнее взаимосвязь факторов.\n",
    "\n",
    "Подозрительно одинаковыми выглядят **корреляции со стоимостью** аренды в рублях и евро. Корреляция между ними равна 1. Это логично так как факторы пропорциональны с каким-то коэффициентом. Кроме того, также странно **велика корреляция между жилой и общей площадью**. Чистой пропорциональности здесь нет, но из предыдущего модуля мы помним, что жилая, нежилая и общая площади линейно зависимы.\n",
    "\n",
    "Обратите внимание, что **корреляции с нежилой площадью** не так велики. Итого мы нашли два избыточных набора факторов: один набор пропорционален, другой просто линейно зависим. Это случай чистой коллинеарности. Уберём по одному фактору из каждого, и ранг станет максимальным. \n",
    "\n",
    "***\n",
    "\n",
    "Нежилая площадь имеет самую маленькую корреляцию с целевым параметром, поэтому мы избавимся от неё.\n",
    "\n",
    "Между рублями и евро нет разницы — оставим рубли, так как они нам привычнее.\n",
    "\n",
    "<img src=m2_img6.png width=600>\n",
    "\n",
    "Итак, мы избавились от нежилой площади и аренды в евро. Ранг стал максимальным (то есть равным 5), чистой коллинеарности больше нет, но определитель всё равно маловат. В чём же дело?\n",
    "\n",
    "Стоимость аренды жилой площади и общей площади сильно коррелируют между собой. Обратите внимание на значения коэффициентов корреляции — они практически равны 1, хотя формально эти факторы линейно независимы. Такие корреляции ощутимо портят картину, что и отражается на определителе.\n",
    "\n",
    "Давайте оставим только жилую площадь, её корреляция с показами максимальна.\n",
    "\n",
    "Корреляции между жилой площадью и этажом уже не такие сильные.\n",
    "\n",
    "<img src=m2_img7.png width=400>\n",
    "\n",
    "Ранг матрицы теперь равен 3 (как ему и положено), а определитель не так близок к нулю.\n",
    "\n",
    "***\n",
    "\n",
    ">**Резюмируем ↓**\n",
    ">\n",
    ">* Корреляция — это мера линейной зависимости между признаками.\n",
    ">\n",
    ">* Чем больше по модулю корреляция между каким-нибудь фактором и целевым признаком, тем лучше:\n",
    ">\n",
    ">$\\left|corr(\\vec{x}_{i}, \\vec{y}) \\right| \\rightarrow 1$ - хорошо\n",
    ">\n",
    ">* Чем больше по модулю корреляция между факторами, тем хуже:\n",
    ">\n",
    ">$\\left|corr(\\vec{x}_{i}, \\vec{x}_{j}) \\right| \\rightarrow 1$ - плохо\n",
    ">\n",
    ">* Чем больше линейно зависимых факторов, тем меньше ранг."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно выделить два неприятных случая:\n",
    "\n",
    "1. **Чистая коллинеарность**\n",
    "\n",
    "Некоторые факторы являются линейно зависимыми между собой. Это влечёт к уменьшению ранга матрицы факторов. Корреляции между зависимыми факторами близки к +1 или -1. Матрица корреляции вырождена.\n",
    "\n",
    "Такие случаи очень редко встречаются на практике, но если вы таковые заметите, можете смело избавиться от одного из факторов.\n",
    "\n",
    "2. **Мультиколлинеарность**\n",
    "\n",
    "Формально линейной зависимости между факторами нет, и матрица факторов имеет максимальный ранг. Однако корреляции между мультиколлинеарными факторами по-прежнему близки к +1 или -1, и матрица корреляции практически вырождена, несмотря на то что имеет максимальный ранг.\n",
    "\n",
    "Таким образом, чистая коллинеарность провоцирует больше проблем, но её легче заметить. Мультиколлинеарность же может быть скрытой, и заметить её не так просто.\n",
    "\n",
    "## КАК ОБНАРУЖИТЬ МУЛЬТИКОЛЛИНЕАРНОСТЬ?\n",
    "\n",
    "* Иногда видно сразу или заметно по контексту, что некоторые факторы будут коррелировать между собой.\n",
    "\n",
    "* Также можно посмотреть на определитель матрицы корреляции: если он близок к нулю, значит дела обстоят не очень хорошо.\n",
    "\n",
    "* Важным маркером будут странные результаты стандартной регрессионной формулы, например слишком большие по модулю коэффициенты (вспомните модуль «ML-2. Обучение с учителем: регрессия», где у нас получились запредельные коэффициенты при решении задач) или взаимно обратные коэффициенты (как мы видели в примере в предыдущем юните). \n",
    "\n",
    "* И, наконец, исследование спектра матрицы корреляций и числа обусловленности не только позволяет обнаружить мультиколлинераность, но и помогает избавиться от неё.\n",
    "\n",
    ">Есть много способов борьбы с мультиколлинеарностью. Мы с вами применили самый наивный — **удаление взаимных факторов «на глаз»**. Увы, это получается не всегда.\n",
    ">\n",
    ">Два других метода называются по-разному, но по сути делают одно и тоже: это **метод главных компонент** для корреляционной матрицы и **сингулярное разложение** матрицы факторов. О них мы поговорим в следующем модуле. \n",
    ">\n",
    ">Кроме того, можно воспользоваться знакомыми нам **методами регуляризации**, о которых поговорим уже в этом модуле."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## В ЧЁМ ПРОБЛЕМА МУЛЬТИКОЛЛИНЕАРНОСТИ ДЛЯ `LINEAR REGRESSION`?\n",
    "\n",
    "Несмотря на то что мультиколлинеарность делает матрицу корреляций более вырожденной, она не оказывает прямого влияния на точность модели сама по себе. Проблема полной вырожденности матрицы $(A^TA)$, как мы уже обсуждали ранее, в `sklearn` вполне решается с помощью сингулярного разложения. То есть решение можно получить всегда даже при полной коллинеарности и сильной мультиколлинеарности, несмотря на противоречие с теорией линейной алгебры.\n",
    "\n",
    "?Однако сможем ли мы доверять такому решению?\n",
    "\n",
    "*Бывают задачи, где важно не просто построить модель, но и проинтерпретировать её результат — коэффициенты линейной регрессии. Типичный пример — задача кредитного скоринга: в ней важно понять, что влияет на вероятность дефолта заёмщика.*\n",
    "\n",
    "*Проблема заключается в том, что в случае мультиколлинеарности коэффициенты линейной регрессии становятся неустойчивыми. Например, признак «остаток долга/сумма выдачи» вроде бы должен приводить к уменьшению вероятности дефолта, так как клиенту остаётся выплачивать всё меньшую сумму. Однако мультиколлинеарность приводит к тому, что подобранный в ходе обучения модели коэффициент может сменить знак на противоположный, а признак, с точки зрения модели, может начать говорить об обратном: чем меньше остаётся платить, тем больше вероятность дефолта. Подобный кейс хорошо описан в этой статье — рекомендуем с ней ознакомиться.*\n",
    "\n",
    "*К тому же, чем больше в данных мультиколлинеарных факторов, тем сильнее увеличивается разброс коэффициентов регрессии. Полная коллинеарность означает, что существует бесконечное количество способов выразить один фактор через линейную комбинацию других. В свою очередь это значит, что есть бесконечное число возможных коэффициентов регрессии , таких, которые дают одни и те же результаты.* \n",
    "\n",
    "*Чем больше высококоррелированных факторов в данных, тем больше таких линейных комбинаций и тем больше коэффициенты становятся по модулю, что приводит к проблеме под названием «взрывной рост весов», когда коэффициенты регрессии начинают стремиться к бесконечности, что приводит к «поломке» даже устойчивой к вырожденным матрицам модели.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank corr A = 2\n",
      "det corr A = 0.0\n"
     ]
    }
   ],
   "source": [
    "x_1 = np.array([5.1, 1.8, 2.1, 10.3, 12.1, 12.6])\n",
    "x_2 = np.array([10.2, 3.7, 4.1, 20.5, 24.2, 24.1])\n",
    "x_3 = np.array([2.5, 0.9, 1.1, 5.1, 6.1, 6.3])\n",
    "A = np.array([x_1,x_2,x_3]).T\n",
    "corr_a = np.corrcoef(A)\n",
    "print('rank corr A =',np.linalg.matrix_rank(corr_A))\n",
    "print('det corr A =',round(np.linalg.det(corr_A),7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0     1    2\n",
      "0   5.1  10.2  2.5\n",
      "1   1.8   3.7  0.9\n",
      "2   2.1   4.1  1.1\n",
      "3  10.3  20.5  5.1\n",
      "4  12.1  24.2  6.1\n",
      "5  12.6  24.1  6.3\n",
      "rank corr A = 3\n",
      "det corr A = 5e-07\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame([x_1,x_2,x_3]).T\n",
    "print(df)\n",
    "corr = df.corr()\n",
    "print('rank corr A =',np.linalg.matrix_rank(corr))\n",
    "print('det corr A =',round(np.linalg.det(corr),7))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Практика. Лин.регрессия МНК <a class=\"anchor\" id=5></a>\n",
    "\n",
    "[к содержанию](#0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У Василия, основателя компании «Газ-Таз-Ваз-Нефть», дела идут в гору: в этом году он открывает 100 новых скважин по добыче газа. Однако в целях оптимизации расходов и для потенциального повышения дохода Василию необходимо оценить, сколько денег будет приносить ему каждая из скважин, а также понять, какие факторы потенциально сильнейшим образом влияют на объём добычи газа. Для этого Василий решил нанять вас как специалиста по построению моделей машинного обучения.\n",
    "\n",
    "Василий представляет вам набор данных о добыче газа на своих скважинах. Файл с данными вы можете скачать здесь.\n",
    "\n",
    "**Признаки**:\n",
    "\n",
    "* `Well` — идентификатор скважины;\n",
    "* `Por` — пористость скважины (%);\n",
    "* `Perm` — проницаемость скважины;\n",
    "* `AI` — акустический импеданс ($кг/м^2\\cdot 10^6$);\n",
    "* `Brittle` — коэффициент хрупкости скважины (%);\n",
    "* `TOC` — общий органический углерод (%);\n",
    "* `VR` — коэффициент отражения витринита (%);\n",
    "* `Prod` — добыча газа в сутки (млн. кубических футов).\n",
    "\n",
    ">Ваша задача — построить регрессионную модель, которая прогнозирует выработку газа на скважине (**целевой признак** — `Prod`) на основе остальных характеристик скважины, и проинтерпретировать результаты вашей модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Well</th>\n",
       "      <th>Por</th>\n",
       "      <th>Perm</th>\n",
       "      <th>AI</th>\n",
       "      <th>Brittle</th>\n",
       "      <th>TOC</th>\n",
       "      <th>VR</th>\n",
       "      <th>Prod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>12.08</td>\n",
       "      <td>2.92</td>\n",
       "      <td>2.80</td>\n",
       "      <td>81.40</td>\n",
       "      <td>1.16</td>\n",
       "      <td>2.31</td>\n",
       "      <td>4165.196191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>12.38</td>\n",
       "      <td>3.53</td>\n",
       "      <td>3.22</td>\n",
       "      <td>46.17</td>\n",
       "      <td>0.89</td>\n",
       "      <td>1.88</td>\n",
       "      <td>3561.146205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>14.02</td>\n",
       "      <td>2.59</td>\n",
       "      <td>4.01</td>\n",
       "      <td>72.80</td>\n",
       "      <td>0.89</td>\n",
       "      <td>2.72</td>\n",
       "      <td>4284.348574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>17.67</td>\n",
       "      <td>6.75</td>\n",
       "      <td>2.63</td>\n",
       "      <td>39.81</td>\n",
       "      <td>1.08</td>\n",
       "      <td>1.88</td>\n",
       "      <td>5098.680869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>17.52</td>\n",
       "      <td>4.57</td>\n",
       "      <td>3.18</td>\n",
       "      <td>10.94</td>\n",
       "      <td>1.51</td>\n",
       "      <td>1.90</td>\n",
       "      <td>3406.132832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Well    Por  Perm    AI  Brittle   TOC    VR         Prod\n",
       "0     1  12.08  2.92  2.80    81.40  1.16  2.31  4165.196191\n",
       "1     2  12.38  3.53  3.22    46.17  0.89  1.88  3561.146205\n",
       "2     3  14.02  2.59  4.01    72.80  0.89  2.72  4284.348574\n",
       "3     4  17.67  6.75  2.63    39.81  1.08  1.88  5098.680869\n",
       "4     5  17.52  4.57  3.18    10.94  1.51  1.90  3406.132832"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('unconv.zip')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Well</th>\n",
       "      <th>Por</th>\n",
       "      <th>Perm</th>\n",
       "      <th>AI</th>\n",
       "      <th>Brittle</th>\n",
       "      <th>TOC</th>\n",
       "      <th>VR</th>\n",
       "      <th>Prod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>100.500000</td>\n",
       "      <td>14.991150</td>\n",
       "      <td>4.330750</td>\n",
       "      <td>2.968850</td>\n",
       "      <td>48.161950</td>\n",
       "      <td>0.990450</td>\n",
       "      <td>1.964300</td>\n",
       "      <td>4311.219852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>57.879185</td>\n",
       "      <td>2.971176</td>\n",
       "      <td>1.731014</td>\n",
       "      <td>0.566885</td>\n",
       "      <td>14.129455</td>\n",
       "      <td>0.481588</td>\n",
       "      <td>0.300827</td>\n",
       "      <td>992.038414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.550000</td>\n",
       "      <td>1.130000</td>\n",
       "      <td>1.280000</td>\n",
       "      <td>10.940000</td>\n",
       "      <td>-0.190000</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>2107.139414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>50.750000</td>\n",
       "      <td>12.912500</td>\n",
       "      <td>3.122500</td>\n",
       "      <td>2.547500</td>\n",
       "      <td>37.755000</td>\n",
       "      <td>0.617500</td>\n",
       "      <td>1.770000</td>\n",
       "      <td>3618.064513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>100.500000</td>\n",
       "      <td>15.070000</td>\n",
       "      <td>4.035000</td>\n",
       "      <td>2.955000</td>\n",
       "      <td>49.510000</td>\n",
       "      <td>1.030000</td>\n",
       "      <td>1.960000</td>\n",
       "      <td>4284.687348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>150.250000</td>\n",
       "      <td>17.402500</td>\n",
       "      <td>5.287500</td>\n",
       "      <td>3.345000</td>\n",
       "      <td>58.262500</td>\n",
       "      <td>1.350000</td>\n",
       "      <td>2.142500</td>\n",
       "      <td>5086.089761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>200.000000</td>\n",
       "      <td>23.550000</td>\n",
       "      <td>9.870000</td>\n",
       "      <td>4.630000</td>\n",
       "      <td>84.330000</td>\n",
       "      <td>2.180000</td>\n",
       "      <td>2.870000</td>\n",
       "      <td>6662.622385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Well         Por        Perm          AI     Brittle         TOC  \\\n",
       "count  200.000000  200.000000  200.000000  200.000000  200.000000  200.000000   \n",
       "mean   100.500000   14.991150    4.330750    2.968850   48.161950    0.990450   \n",
       "std     57.879185    2.971176    1.731014    0.566885   14.129455    0.481588   \n",
       "min      1.000000    6.550000    1.130000    1.280000   10.940000   -0.190000   \n",
       "25%     50.750000   12.912500    3.122500    2.547500   37.755000    0.617500   \n",
       "50%    100.500000   15.070000    4.035000    2.955000   49.510000    1.030000   \n",
       "75%    150.250000   17.402500    5.287500    3.345000   58.262500    1.350000   \n",
       "max    200.000000   23.550000    9.870000    4.630000   84.330000    2.180000   \n",
       "\n",
       "               VR         Prod  \n",
       "count  200.000000   200.000000  \n",
       "mean     1.964300  4311.219852  \n",
       "std      0.300827   992.038414  \n",
       "min      0.930000  2107.139414  \n",
       "25%      1.770000  3618.064513  \n",
       "50%      1.960000  4284.687348  \n",
       "75%      2.142500  5086.089761  \n",
       "max      2.870000  6662.622385  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала в качестве модели будем использовать простую линейную регрессию.\n",
    "\n",
    "**Задание 5.1**\n",
    "\n",
    "Постройте корреляционную матрицу факторов, включив в неё целевой признак. Ответьте на следующие вопросы:\n",
    "\n",
    "1. Выберите топ-3 факторов, наиболее коррелированных с целевой переменной:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Well</th>\n",
       "      <th>Por</th>\n",
       "      <th>Perm</th>\n",
       "      <th>AI</th>\n",
       "      <th>Brittle</th>\n",
       "      <th>TOC</th>\n",
       "      <th>VR</th>\n",
       "      <th>Prod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Well</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.068927</td>\n",
       "      <td>0.077928</td>\n",
       "      <td>0.041483</td>\n",
       "      <td>-0.079252</td>\n",
       "      <td>0.022624</td>\n",
       "      <td>-0.007279</td>\n",
       "      <td>0.026817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Por</th>\n",
       "      <td>0.068927</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.760546</td>\n",
       "      <td>-0.461549</td>\n",
       "      <td>-0.218570</td>\n",
       "      <td>0.711831</td>\n",
       "      <td>0.111860</td>\n",
       "      <td>0.861910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perm</th>\n",
       "      <td>0.077928</td>\n",
       "      <td>0.760546</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.239636</td>\n",
       "      <td>-0.124017</td>\n",
       "      <td>0.471746</td>\n",
       "      <td>0.051023</td>\n",
       "      <td>0.727426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AI</th>\n",
       "      <td>0.041483</td>\n",
       "      <td>-0.461549</td>\n",
       "      <td>-0.239636</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.127599</td>\n",
       "      <td>-0.531864</td>\n",
       "      <td>0.499143</td>\n",
       "      <td>-0.390835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Brittle</th>\n",
       "      <td>-0.079252</td>\n",
       "      <td>-0.218570</td>\n",
       "      <td>-0.124017</td>\n",
       "      <td>0.127599</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.214282</td>\n",
       "      <td>0.317929</td>\n",
       "      <td>0.237155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOC</th>\n",
       "      <td>0.022624</td>\n",
       "      <td>0.711831</td>\n",
       "      <td>0.471746</td>\n",
       "      <td>-0.531864</td>\n",
       "      <td>-0.214282</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.299483</td>\n",
       "      <td>0.654445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VR</th>\n",
       "      <td>-0.007279</td>\n",
       "      <td>0.111860</td>\n",
       "      <td>0.051023</td>\n",
       "      <td>0.499143</td>\n",
       "      <td>0.317929</td>\n",
       "      <td>0.299483</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.323182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prod</th>\n",
       "      <td>0.026817</td>\n",
       "      <td>0.861910</td>\n",
       "      <td>0.727426</td>\n",
       "      <td>-0.390835</td>\n",
       "      <td>0.237155</td>\n",
       "      <td>0.654445</td>\n",
       "      <td>0.323182</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Well       Por      Perm        AI   Brittle       TOC        VR  \\\n",
       "Well     1.000000  0.068927  0.077928  0.041483 -0.079252  0.022624 -0.007279   \n",
       "Por      0.068927  1.000000  0.760546 -0.461549 -0.218570  0.711831  0.111860   \n",
       "Perm     0.077928  0.760546  1.000000 -0.239636 -0.124017  0.471746  0.051023   \n",
       "AI       0.041483 -0.461549 -0.239636  1.000000  0.127599 -0.531864  0.499143   \n",
       "Brittle -0.079252 -0.218570 -0.124017  0.127599  1.000000 -0.214282  0.317929   \n",
       "TOC      0.022624  0.711831  0.471746 -0.531864 -0.214282  1.000000  0.299483   \n",
       "VR      -0.007279  0.111860  0.051023  0.499143  0.317929  0.299483  1.000000   \n",
       "Prod     0.026817  0.861910  0.727426 -0.390835  0.237155  0.654445  0.323182   \n",
       "\n",
       "             Prod  \n",
       "Well     0.026817  \n",
       "Por      0.861910  \n",
       "Perm     0.727426  \n",
       "AI      -0.390835  \n",
       "Brittle  0.237155  \n",
       "TOC      0.654445  \n",
       "VR       0.323182  \n",
       "Prod     1.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.corr()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Вычислите ранг полученной матрицы корреляций:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.matrix_rank(data.corr())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Вычислите определитель матрицы корреляций. Ответ округлите до четвёртого знака после точки-разделителя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0007"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(np.linalg.det(data.corr()),4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создайте матрицу наблюдений. Обозначьте её за $X$, а вектор правильных ответов — за $y$.\n",
    "\n",
    "1. Постройте модель линейной регрессии по методу наименьших квадратов. Для этого используйте матричную формулу NumPy. В качестве ответа укажите полученные оценки коэффициентов модели. **Ответ округлите до целого числа**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_0 [-1232.30802956]\n",
      "w_well [0.05070036]\n",
      "w_por [230.17914038]\n",
      "w_per [116.23900607]\n",
      "w_pvr [785.25981457]\n"
     ]
    }
   ],
   "source": [
    "X = np.column_stack((np.ones(200), data.drop('Prod', axis=1)))\n",
    "y = data[['Prod']]\n",
    "w_hat = np.linalg.inv(X.T@X)@X.T@y\n",
    "print('w_0',w_hat.values[0])\n",
    "print('w_well',w_hat.values[1])\n",
    "print('w_por',w_hat.values[2])\n",
    "print('w_per',w_hat.values[3])\n",
    "print('w_pvr',w_hat.values[7])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 5.3**\n",
    "\n",
    "1. Постройте прогноз выработки газа для скважины с параметрами, указанными ниже. Чему равна абсолютная ошибка построенного вами прогноза для предложенной скважины (в миллионах кубических футов в день). Ответ округлите до целого числа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prod   -1232.30803\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_hat.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prod    4723.064054\n",
       "dtype: float64"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Prod_new = w_hat.iloc[0]+w_hat.iloc[1]*106+w_hat.iloc[2]*15.32+w_hat.iloc[3]*3.71+w_hat.iloc[4]*3.29+w_hat.iloc[5]*55.99+w_hat.iloc[6]*1.35+w_hat.iloc[7]*2.42\n",
    "Prod_new"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Постройте прогноз выработки газа для всех скважин из обучающего набора данных. Чему равно значение метрики `MAPE` вашей модели? Ответ приведите в процентах (не указывайте знак процента), округлив его до первого знака после точки-разделителя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# будем умножать столбцы векторы скалярно на соответствующие коэффициенты w_hat и суммировать результаты\n",
    "y_pred = w_hat.iloc[0][0]+X[:,1]*w_hat.iloc[1][0]+X[:,2]*w_hat.iloc[2][0]+X[:,3]*w_hat.iloc[3][0]+X[:,4]*w_hat.iloc[4][0]+X[:,5]*w_hat.iloc[5][0]+X[:,6]*w_hat.iloc[6][0]+X[:,7]*w_hat.iloc[7][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# то же самое чуть изящнее\n",
    "y_pred_1 = []\n",
    "for i in range(X.shape[0]):\n",
    "    y_pred_1.append(np.sum(X[i]*w_hat.T, axis=1))\n",
    "y_pred_1 = np.array(y_pred_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE y_1 score: 3.6 %\n",
      "MAPE y_2 score: 3.6 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print('MAPE y_1 score: {:.1f} %'.format(metrics.mean_absolute_percentage_error(y, y_pred) * 100))\n",
    "print('MAPE y_2 score: {:.1f} %'.format(metrics.mean_absolute_percentage_error(y, y_pred_1) * 100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание 5.4\n",
    "\n",
    "Настало время анализа построенной модели. Посмотрите на коэффициенты и сравните их знаки со значениями выборочных корреляций между целевым признаком и факторами, которые вы нашли ранее.\n",
    "\n",
    "1. Есть ли в вашей модели фактор, при котором коэффициент в модели линейной регрессии противоречит соответствующему коэффициенту корреляции? Например, корреляция говорит, что зависимость между фактором и целью прямая, а модель говорит обратное."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Well</th>\n",
       "      <th>Por</th>\n",
       "      <th>Perm</th>\n",
       "      <th>AI</th>\n",
       "      <th>Brittle</th>\n",
       "      <th>TOC</th>\n",
       "      <th>VR</th>\n",
       "      <th>Prod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Well</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.068927</td>\n",
       "      <td>0.077928</td>\n",
       "      <td>0.041483</td>\n",
       "      <td>-0.079252</td>\n",
       "      <td>0.022624</td>\n",
       "      <td>-0.007279</td>\n",
       "      <td>0.026817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Por</th>\n",
       "      <td>0.068927</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.760546</td>\n",
       "      <td>-0.461549</td>\n",
       "      <td>-0.218570</td>\n",
       "      <td>0.711831</td>\n",
       "      <td>0.111860</td>\n",
       "      <td>0.861910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perm</th>\n",
       "      <td>0.077928</td>\n",
       "      <td>0.760546</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.239636</td>\n",
       "      <td>-0.124017</td>\n",
       "      <td>0.471746</td>\n",
       "      <td>0.051023</td>\n",
       "      <td>0.727426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AI</th>\n",
       "      <td>0.041483</td>\n",
       "      <td>-0.461549</td>\n",
       "      <td>-0.239636</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.127599</td>\n",
       "      <td>-0.531864</td>\n",
       "      <td>0.499143</td>\n",
       "      <td>-0.390835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Brittle</th>\n",
       "      <td>-0.079252</td>\n",
       "      <td>-0.218570</td>\n",
       "      <td>-0.124017</td>\n",
       "      <td>0.127599</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.214282</td>\n",
       "      <td>0.317929</td>\n",
       "      <td>0.237155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOC</th>\n",
       "      <td>0.022624</td>\n",
       "      <td>0.711831</td>\n",
       "      <td>0.471746</td>\n",
       "      <td>-0.531864</td>\n",
       "      <td>-0.214282</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.299483</td>\n",
       "      <td>0.654445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VR</th>\n",
       "      <td>-0.007279</td>\n",
       "      <td>0.111860</td>\n",
       "      <td>0.051023</td>\n",
       "      <td>0.499143</td>\n",
       "      <td>0.317929</td>\n",
       "      <td>0.299483</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.323182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prod</th>\n",
       "      <td>0.026817</td>\n",
       "      <td>0.861910</td>\n",
       "      <td>0.727426</td>\n",
       "      <td>-0.390835</td>\n",
       "      <td>0.237155</td>\n",
       "      <td>0.654445</td>\n",
       "      <td>0.323182</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Well       Por      Perm        AI   Brittle       TOC        VR  \\\n",
       "Well     1.000000  0.068927  0.077928  0.041483 -0.079252  0.022624 -0.007279   \n",
       "Por      0.068927  1.000000  0.760546 -0.461549 -0.218570  0.711831  0.111860   \n",
       "Perm     0.077928  0.760546  1.000000 -0.239636 -0.124017  0.471746  0.051023   \n",
       "AI       0.041483 -0.461549 -0.239636  1.000000  0.127599 -0.531864  0.499143   \n",
       "Brittle -0.079252 -0.218570 -0.124017  0.127599  1.000000 -0.214282  0.317929   \n",
       "TOC      0.022624  0.711831  0.471746 -0.531864 -0.214282  1.000000  0.299483   \n",
       "VR      -0.007279  0.111860  0.051023  0.499143  0.317929  0.299483  1.000000   \n",
       "Prod     0.026817  0.861910  0.727426 -0.390835  0.237155  0.654445  0.323182   \n",
       "\n",
       "             Prod  \n",
       "Well     0.026817  \n",
       "Por      0.861910  \n",
       "Perm     0.727426  \n",
       "AI      -0.390835  \n",
       "Brittle  0.237155  \n",
       "TOC      0.654445  \n",
       "VR       0.323182  \n",
       "Prod     1.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Prod</th>\n",
       "      <td>-1232.30803</td>\n",
       "      <td>0.0507</td>\n",
       "      <td>230.17914</td>\n",
       "      <td>116.239006</td>\n",
       "      <td>-365.202301</td>\n",
       "      <td>24.99437</td>\n",
       "      <td>-78.400929</td>\n",
       "      <td>785.259815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0       1          2           3           4         5  \\\n",
       "Prod -1232.30803  0.0507  230.17914  116.239006 -365.202301  24.99437   \n",
       "\n",
       "              6           7  \n",
       "Prod -78.400929  785.259815  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(data.corr())\n",
    "w_hat.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_0 [-1835.44646069]\n",
      "w_1 [293.03624565]\n",
      "w_2 [-200.03091206]\n",
      "w_3 [27.64098209]\n",
      "MAPE y_2 score: 4.0 %\n"
     ]
    }
   ],
   "source": [
    "X_1 = np.column_stack((np.ones(200), data.drop(['Prod','Well','Perm','TOC'], axis=1)))\n",
    "y_1 = data[['Prod']]\n",
    "w_hat_1 = np.linalg.inv(X_1.T@X_1)@X_1.T@y_1\n",
    "print('w_0',w_hat_1.values[0])\n",
    "print('w_1',w_hat_1.values[1])\n",
    "print('w_2',w_hat_1.values[2])\n",
    "print('w_3',w_hat_1.values[3])\n",
    "y_pred_1 = []\n",
    "for i in range(X.shape[0]):\n",
    "    y_pred_1.append(np.sum(X_1[i]*w_hat_1.T, axis=1))\n",
    "y_pred_1 = np.array(y_pred_1)\n",
    "print('MAPE y_2 score: {:.1f} %'.format(metrics.mean_absolute_percentage_error(y, y_pred_1) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Por</th>\n",
       "      <th>AI</th>\n",
       "      <th>Brittle</th>\n",
       "      <th>VR</th>\n",
       "      <th>Prod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Por</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.461549</td>\n",
       "      <td>-0.218570</td>\n",
       "      <td>0.111860</td>\n",
       "      <td>0.861910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AI</th>\n",
       "      <td>-0.461549</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.127599</td>\n",
       "      <td>0.499143</td>\n",
       "      <td>-0.390835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Brittle</th>\n",
       "      <td>-0.218570</td>\n",
       "      <td>0.127599</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.317929</td>\n",
       "      <td>0.237155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VR</th>\n",
       "      <td>0.111860</td>\n",
       "      <td>0.499143</td>\n",
       "      <td>0.317929</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.323182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prod</th>\n",
       "      <td>0.861910</td>\n",
       "      <td>-0.390835</td>\n",
       "      <td>0.237155</td>\n",
       "      <td>0.323182</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Por        AI   Brittle        VR      Prod\n",
       "Por      1.000000 -0.461549 -0.218570  0.111860  0.861910\n",
       "AI      -0.461549  1.000000  0.127599  0.499143 -0.390835\n",
       "Brittle -0.218570  0.127599  1.000000  0.317929  0.237155\n",
       "VR       0.111860  0.499143  0.317929  1.000000  0.323182\n",
       "Prod     0.861910 -0.390835  0.237155  0.323182  1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Prod</th>\n",
       "      <td>-1835.446461</td>\n",
       "      <td>293.036246</td>\n",
       "      <td>-200.030912</td>\n",
       "      <td>27.640982</td>\n",
       "      <td>517.402726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0           1           2          3           4\n",
       "Prod -1835.446461  293.036246 -200.030912  27.640982  517.402726"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1 = data.drop(['Well','Perm','TOC'], axis=1)\n",
    "display(data_1.corr())\n",
    "w_hat_1.T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Полиноминальная регрессия <a class=\"anchor\" id=6></a>\n",
    "\n",
    "[к содержанию](#0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полином (многочлен) от $k$ переменных $x_1, \\ x_2, \\ ..., \\ x_k$ — это выражение (функция) вида:\n",
    "\n",
    "$P\\left(x_{1}, x_{2}, \\ldots, x_{k}\\right)=\\sum_{I} w_{i} x_{1}^{i_{1}} x_{2}{ }^{i_{2}} \\ldots x_{k}^{i_{k}},$\n",
    "\n",
    "где\n",
    "\n",
    "* $I=(i_1, i_2, \\ ...., \\ i_k)$ — набор из $k$ целых неотрицательных чисел — степеней полинома;\n",
    "* $w_I$  — числа, называемые коэффициентами полинома.\n",
    "\n",
    "Пока эта форма записи нам ничего не даёт — она слишком сложная. Давайте рассмотрим пример попроще. Когда переменная всего одна, полином будет записываться как:\n",
    "\n",
    "$P(x) = \\sum_{I} w_i x^i = w_0 + w_1 x^1 + w_2 x^2 + ... + w_k x^k$\n",
    "\n",
    "Выражение для полинома первой степени уже можно прочитать без особого труда. Видно, что на самом деле полином — это линейная комбинация из различных степеней переменной $x$, взятой с какими-то коэффициентами, причём некоторые из коэффициентов могут быть нулевыми.\n",
    "\n",
    ">Максимальная степень при переменной $x$ называется степенью полинома.\n",
    "\n",
    "Самый простой пример полинома от одной переменной — парабола. Это полином второй степени. Вспомним её уравнение:\n",
    "\n",
    "$y = ax^2 + bx + c,$\n",
    "\n",
    "где $x$ — это некоторая неизвестная, а коэффициенты $a$, $b$ и $c$ определяют различные параметры этой параболы (направление её ветвей, начало параболы, её растяжение и т. д.).\n",
    "\n",
    "Ниже представлены возможные варианты расположения параболы в зависимости от коэффициентов $a$, $b$ и $c$ \n",
    "\n",
    "<img src=m2_img8.png>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вспомним, что уравнение $y = ax^2 + bx + c = 0$ определяет точки пересечения параболы с осью абсцисс — осью $X$. Чтобы найти точки пересечения, необходимо решить это квадратное уравнение. Вы наверняка делали это в школе: найти дискриминант, затем квадратные корни и т. д. Сейчас мы не будем этим заниматься, но понимание сути процедуры полезно для общего осознания принципа работы полиномиальной регрессии.\n",
    "\n",
    ">Несколько простых примеров различных полиномов с числовыми коэффициентами для наглядности:\n",
    ">\n",
    ">$y=8+5x+2x^2$ — парабола\n",
    ">\n",
    ">$y=5x+x^3$ — кубическая парабола\n",
    ">\n",
    ">$y=8x^5+3x^4+x^3+x^2+5x$ — полином пятой степени\n",
    "\n",
    "Кстати, отметим **важный факт**: уравнение прямой также является частным случае полинома первой степени:\n",
    "\n",
    "$y=w_0+w_1 x$\n",
    "\n",
    "## Чем нам так интересны полиномы (особенно степени > 1)?\n",
    "\n",
    "На самом деле всё очень просто: полином степени  способен описать абсолютно любую зависимость. Для этого ему достаточно задать набор наблюдений — точек, через которые он должен пройти (или пройти приблизительно). Вопрос стоит только в степени этого полинома — . Например, ниже представлено три полинома: первой степени — линейная регрессия, второй степени — квадратичная регрессия и третьей степени — кубическая регрессия.\n",
    "\n",
    "<img src=m2_img9.png>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что для полинома первой степени (линейной регрессии) представленная в данных нелинейная зависимость целевой переменной $y$ от фактора $x$, даётся тяжело: ошибка прогноза довольно велика. Два других полинома хорошо описывают поведение точек в пространстве.\n",
    "\n",
    ">Цель обучения модели полиномиальной регрессии степени та же, что и для линейной регрессии: найти такие коэффициенты $w_i$, при которых ошибка между построенной функцией и обучающей выборкой была бы наименьшей из возможных.\n",
    "\n",
    "На самом деле для поиска этих коэффициентов мы можем использовать те же самые методы, что и для линейной регрессии, а именно **метод наименьших квадратов**. Мы можем взять уравнение полинома и потребовать, чтобы кривая проходила через точки в обучающей выборке (на графике выше они обозначены синим). Значения точек можно обозначить за $y_1, \\ y_2, \\ ..., \\ y_N$. Тогда мы хотим, чтобы для полинома степени $k$ (от одной переменной) выполнялась система уравнений:\n",
    "\n",
    "$\\left\\{ \\begin{array}{} w_0+w_1x+w_2x^2+\\dots+w_kx^k=y_1 \\\\ w_0+w_1x+w_2x^2+\\dots\n",
    "+w_kx^k=y_2 \\\\ \\dots \\\\ w_0+w_1x+w_2x^2+\\dots+w_kx^k=y_N \\end{array}\\right.$\n",
    "\n",
    "Обычно количество точек в обучающей выборке $N$ значительно больше, чем степень полинома $k$, а значит перед нами переопределённая СЛАУ относительно с $k+1$ неизвестной — $w_i$. Точных решений у системы практически никогда не будет, но мы умеем решать её приближённо. Мы даже вывели формулу для приближённого решения:\n",
    "\n",
    "$\\vec{w}=\\left(A^{T} A\\right)^{-1} A^{T} \\vec{y}$\n",
    "\n",
    ">Итак, мы вкратце обсудили, как должно выглядеть решение для полинома от одной переменной (от одного фактора). Теперь давайте более подробно остановимся на нюансах решения этой задачи и плавно перейдём к полиномам от нескольких переменных."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнём с **квадратичной регрессии** от одной переменной.\n",
    "\n",
    "Пусть у нас некоторый вектор-фактор $\\vec{x}=(x_1, x_2, ..., x_N)^T$, от которого зависит целевая переменная $\\vec{y}=(y_1, y_2, ..., y_N)^T$. Будем предполагать, что зависимость нелинейная — допустим, квадратичная, то есть в качестве модели используется уравнение параболы. Тогда мы хотим выразить вектор $\\vec{y}$ как линейную комбинацию из векторов $\\vec{x}$ и $\\vec{x}^2$:\n",
    "\n",
    "$\\vec{y}=w_0 +w_1 \\vec{x}+w_2 \\vec{x}^2$\n",
    "\n",
    "или\n",
    "\n",
    "$\\left(\\begin{array}{} y_1\\\\y_2\\\\ \\dots \\\\y_N \\end{array}\\right)=w_0+w_1\\left(\\begin{array}{} x_1\\\\x_2\\\\ \\dots \\\\x_N \\end{array}\\right)+w_2\\left(\\begin{array}{} x_1\\\\x_2\\\\ \\dots \\\\x_N \\end{array}\\right)^2$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Важное лирическое отступление ↓\n",
    "\n",
    "У внимательного студента должен был возникнуть вопрос: а что значит возвести вектор в квадрат? На первый взгляд, может показаться, что мы ищем скалярное произведение вектора с самим собой, ведь:\n",
    "\n",
    "$\\vec{x}^2=\\vec{x} \\cdot \\vec{x}=(\\vec{x} \\cdot \\vec{x})$\n",
    "\n",
    "Однако такой вариант нам не подходит, так как скалярное произведение — это число, а нам нужен именно вектор, иначе у нас не получится составить линейную комбинацию. Поэтому здесь под $\\vec{x}^2$ понимается вектор из квадратов координат вектора $\\vec{x}$. Тогда:\n",
    "\n",
    "$\\left(\\begin{array}{} x_1\\\\x_2\\\\ \\dots \\\\x_N \\end{array}\\right)^2=\\left(\\begin{array}{} x_{1}^2\\\\x_{2}^2\\\\ \\dots \\\\x_{N}^2 \\end{array}\\right)$\n",
    "\n",
    "Ещё одно важное замечание: обратите внимание, что, несмотря на то что в нашем уравнении появились квадраты, оно всё равно продолжает быть линейным, так как неизвестным является не вектор $\\vec{x}$, а коэффициенты разложения $w_0$, $w_1$ и $w_2$. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Итак, у нас получилась СЛАУ, состоящая из $N$ уравнений на трёх неизвестных:\n",
    "\n",
    "$\\left\\{ \\begin{array}{} y_1+w_01+w_1x_1+w_2x_{1}^2 \\\\ y_1+w_01+w_1x_2+w_2x_{2}^2  \\\\ \\dots \\\\ y_1+w_01+w_1x_N+w_2x_{N}^2  \\end{array}\\right.$\n",
    "\n",
    "Давайте для удобства и привычной нумерации переменных обозначим вектор $\\vec{z}_1=\\vec{x}$, а $\\vec{z}_2=\\vec{x}^2$. Тогда:\n",
    "\n",
    "$\\vec{z}_1=\\left(\\begin{array}{} z_{11}\\\\z_{12}\\\\ \\dots \\\\z_{1N} \\end{array}\\right)=\\left(\\begin{array}{} x_1\\\\x_2\\\\ \\dots \\\\x_N \\end{array}\\right)$, a $\\vec{z}_2=\\left(\\begin{array}{} z_{21}\\\\z_{22}\\\\ \\dots \\\\z_{2N} \\end{array}\\right)=\\left(\\begin{array}{} x_1^2\\\\x_2^2\\\\ \\dots \\\\x_N^2 \\end{array}\\right)$\n",
    "\n",
    "Тогда получим уже знакомую нам неоднородную переопределённую СЛАУ:\n",
    "\n",
    "$\\left\\{ \\begin{array}{} w_01+w_1z_{11}+w_2z_{21} =y_1 \\\\ w_01+w_1z_{12}+w_2z_{22} =y_2   \\\\ \\dots \\\\ w_01+w_1z_{1N}+w_2z_{2N} =y_N  \\end{array}\\right.$\n",
    "\n",
    "Или в матричном виде:\n",
    "\n",
    "$A\\vec{w}=\\vec{y}$\n",
    "\n",
    "$A=\\left(\\begin{array}{} 1 & z_{11} & z_{21} \\\\ 1 & z_{12} & z_{22} \\\\ \\dots \\\\ 1 & z_{1N} & z_{2N} \\end{array}\\right)$\n",
    "\n",
    "$\\vec{w}=(w_0, w_1, w_2)^T$\n",
    "\n",
    "Что мы делаем с такими СЛАУ? Верно — решаем. Правда, только приближённо. Раз система линейная и переопределённая, то МНК — наш лучший выбор. Тогда решение такой системы будет полностью аналогичным формуле для поиска коэффициентов простой линейной регрессии, разве что коэффициентов будет немного побольше:\n",
    "\n",
    "$\\vec{w}=\\left(A^{T} A\\right)^{-1} A^{T} \\vec{y}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример № 1\n",
    "\n",
    "Построить квадратичную регрессию на целевую переменную $\\vec{y}$ из одного фактора $\\vec{x}$ , если:\n",
    "\n",
    "$\\vec{x}=\\left(\\begin{array}{} 1\\\\3\\\\-2\\\\1 \\end{array}\\right)$ и $\\vec{y}=\\left(\\begin{array}{} 4\\\\5\\\\2\\\\2 \\end{array}\\right)$ \n",
    "\n",
    "Итак, вот наша полиномиальная модель второй степени (квадратичная регрессионная модель):\n",
    "\n",
    "$\\vec{y}=w_0 +w_1 \\vec{x}+w_2 \\vec{x}^2$\n",
    "\n",
    "$\\left(\\begin{array}{} 4\\\\5\\\\2\\\\2 \\end{array}\\right)=w_0+w_1\\left(\\begin{array}{} 1\\\\3\\\\-2\\\\1 \\end{array}\\right)+w_2\\left(\\begin{array}{} 1^2\\\\3^2\\\\(-2)^2\\\\1^2 \\end{array}\\right)$\n",
    "\n",
    "Нам нужно найти такую линейную комбинацию из векторов $\\vec{1}$, $\\vec{x}$ и $\\vec{x}^2$, которая в сумме давала бы наилучшее приближение для $y%. Записываем систему в матричном виде:\n",
    "\n",
    "$A\\vec{w}=\\vec{y}$\n",
    "\n",
    "$A=\\left(\\begin{array}{} 1 & z_{11} & z_{21} \\\\ 1 & z_{12} & z_{22} \\\\ \\dots \\\\ 1 & z_{1N} & z_{2N} \\end{array}\\right) = \\left(\\begin{array}{} 1&1&1^2\\\\1&3&3^2\\\\1&-2&(-2)^2\\\\1&1&1^2 \\end{array}\\right) = \\left(\\begin{array}{} 1&1&1\\\\1&3&9\\\\1&-2&4\\\\1&1&1 \\end{array}\\right)$\n",
    "\n",
    "$\\vec{w}=(w_0,w_1,w_2)^T$\n",
    "\n",
    "$\\vec{y}=(4,5,2,2)^T$\n",
    "\n",
    "Посчитаем ранг матрицы системы и ранг расширенной матрицы системы на случай, если система определённая и имеет конкретное решение или вовсе имеет бесконечное количество решений:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$rk(A|y)=rk\\left(\\begin{array}{} 1&1&1|4\\\\1&3&9|5\\\\1&-2&4|2\\\\1&1&1|2\\end{array}\\right)=rk\\left(\\begin{array}{} 1&1&1|4\\\\0&2&5|4\\\\0&-3&3|1\\\\0&0&0|-2\\end{array}\\right)\n",
    "=rk\\left(\\begin{array}{} 1&1&1|4\\\\0&6&24|12\\\\0&-6&6|3\\\\0&0&0|-2\\end{array}\\right)\n",
    "=rk\\left(\\begin{array}{} 1&1&1|4\\\\0&6&24|12\\\\0&0&30|15\\\\0&0&0|-2\\end{array}\\right)$\n",
    "\n",
    "Видно, что ранг матрицы системы $(rk(A))=3$ всё-таки меньше, чем ранг расширенной матрицы $(rk(A|y))=4$, а значит система не имеет конкретных решений — только приближённые. Найдём их:\n",
    "\n",
    "$\\vec{w}=\\left(A^{T} A\\right)^{-1} A^{T} \\vec{y}$\n",
    "\n",
    "Для оптимизации процесса считать будем на `Python`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.4        0.46666667 0.13333333]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([\n",
    "    [1, 1, 1, 1],\n",
    "    [1, 3, -2, 1],\n",
    "    [1, 9, 4, 1]\n",
    "]).T\n",
    "y = np.array([4, 5, 2, 2])\n",
    "w_hat = np.linalg.inv(A.T@A)@A.T@y\n",
    "print(w_hat) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, наш вектор оценок коэффициентов:\n",
    "\n",
    "$\\hat{\\vec{w}} = (\\hat{w}_0, \\hat{w}_1, \\hat{w}_2)^T = (2.4, 0.47, 0.13)^T$\n",
    "\n",
    "Чтобы сделать прогноз для нового наблюдения $x_{new}$, нам нужно поставить его в уравнение полинома с найденными коэффициентами:\n",
    "\n",
    "$\\vec{y}=2.4 +0.47 x_{new} + 0.13 x^{2}_{new}$\n",
    "\n",
    "Как вы понимаете, один фактор — это слишком тривиальная, далёкая от реальности ситуация. Давайте посмотрим, как выглядит уравнение квадратичной регрессии **для случая двух переменных**.\n",
    "\n",
    "Пусть у нас есть два фактора $\\vec{x}_1$ и $\\vec{x}_2$, от которых зависит целевая переменная :\n",
    "\n",
    "$\\vec{x}_1=\\left(\\begin{array}{}x_{11}\\\\x_{12}\\\\\\dots\\\\x_{1N}\\end{array}\\right)$, $\\vec{x}_2=\\left(\\begin{array}{}x_{21}\\\\x_{22}\\\\\\dots\\\\x_{2N}\\end{array}\\right)$, $\\vec{y}=\\left(\\begin{array}{}y_{1}\\\\y_{2}\\\\\\dots\\\\y_{N}\\end{array}\\right)$\n",
    "\n",
    "Всё то же самое: будем предполагать, что зависимость нелинейная, а точнее, квадратичная, то есть в качестве модели используется полином второй степени, задающий сложную трёхмерную поверхность, форма которой напрямую зависит от коэффициентов. Заметим, что в функцию уже будут включены попарные произведения факторов $\\vec{x}_1$ и $\\vec{x}_2$, а коэффициентов будет уже шесть:\n",
    "\n",
    "$\\vec{y}=w_{0}+w_{1} \\vec{x}_{1}+w_{2} \\vec{x}_{2}+w_{3} \\vec{x}_{1}^{2}+w_{4} \\vec{x}_{1} \\vec{x}_{2}+w_{5} \\vec{x}_{2}^{2}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Примечание**. Здесь, как и в предыдущем случае, запись $\\vec{x}_1\\vec{x}_2$ означает покоординатное (не скалярное!) произведение векторов $\\vec{x}_1$ и $\\vec{x}_2$.\n",
    "\n",
    "$\\left(\\begin{array}{}y_{1}\\\\y_{2}\\\\\\dots\\\\y_{N}\\end{array}\\right)=w_0+w_1\\left(\\begin{array}{}x_{11}\\\\x_{12}\\\\\\dots\\\\x_{1N}\\end{array}\\right)+w_2\\left(\\begin{array}{}x_{21}\\\\x_{22}\\\\\\dots\\\\x_{2N}\\end{array}\\right)+w_3\\left(\\begin{array}{}x_{11}^2\\\\x_{12}^2\\\\\\dots\\\\x_{1N}^2\\end{array}\\right)+w_4\\left(\\begin{array}{}x_{11}\\cdot x_{21}\\\\x_{12}\\cdot x_{22}\\\\\\dots\\\\x_{1N} \\cdot x_{2N}\\end{array}\\right)+w_5\\left(\\begin{array}{}x_{21}^2\\\\x_{22}^2\\\\\\dots\\\\x_{2N}^2\\end{array}\\right)$\n",
    "\n",
    "В матричном виде:\n",
    "\n",
    "$A\\vec{w}=\\vec{y}$\n",
    "\n",
    "$A=\\left(\\begin{array}{}1&x_{11}&x_{21}&x_{11}^2&x_{11}\\cdot x_{21}& x_{21}^2 \\\\ 1&x_{12}&x_{22}&x_{12}^2&x_{12}\\cdot x_{22}& x_{22}^2 \\\\ \\dots \\\\ 1&x_{1N}&x_{2N}&x_{1N}^2&x_{1N}\\cdot x_{2N}& x_{2N}^2 \\end{array}\\right)$\n",
    "\n",
    "$\\vec{w}=(w_0,w_1,w_2,w_3,w_4,w_5)^T$\n",
    "\n",
    "$\\vec{y}=(y_1,y_2,\\dots ,y_N)^T$\n",
    "\n",
    "Выглядит громоздко, но на деле ничего серьёзного. Если воспринимать все полиномиальные столбцы как обычные столбцы, состоящие из чисел, мы просто снова получим обычную переопределённую неоднородную СЛАУ (если $N$ значительно больше количества признаков $k$):\n",
    "\n",
    "$A=\\left(\\begin{array}{}1&z_{11}&z_{21}&z_{31}&z_{41}&z_{51}\\\\1&z_{12}&z_{22}&z_{32}&z_{42}&z_{52}\\\\ \\dots \\\\1&z_{1N}&z_{2N}&z_{3N}&z_{4N}&z_{5N} \\end{array}\\right)$\n",
    "\n",
    "> Сразу обратим внимание на то, что для того, чтобы система имела точное (была совместной) или хотя бы приближённое решение, нам необходимо, чтобы строк в матрице было как минимум шесть, причём все уравнения должны быть линейно независимыми. Иначе количество строк будет меньше количества столбцов, и тогда решений будет бесконечное множество (по первому следствию теоремы Кронекера — Капелли), а такой случай нам не подходит."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Что это значит на языке геометрии?\n",
    "\n",
    "Это значит, что нам нужно как минимум шесть точек в трёхмерном пространстве с осями $\\vec{x}_1$, $\\vec{x}_2$ и $\\vec{y}$, чтобы мы смогли провести через них нашу поверхность, которую задаёт уравнение:\n",
    "\n",
    "$\\vec{y}=w_{0}+w_{1} x_{1}+w_{2} x_{2}+w_{3} x_{1}^{2}+w_{4} x_{1} x_{2}+w_{5} x_{2}^{2}$\n",
    "\n",
    "### Пример № 2\n",
    "\n",
    "Построить квадратичную регрессию на целевую переменную $\\vec{y}$ из двух факторов $\\vec{x}_1$ и $\\vec{x}_2$, если:\n",
    "\n",
    "$\\vec{x}_1=\\left(\\begin{array}{} 1\\\\3\\\\-2\\\\1\\\\5\\\\13\\\\1 \\end{array}\\right)$, $\\vec{x}_2=\\left(\\begin{array}{} 3\\\\4\\\\5\\\\-2\\\\4\\\\11\\\\3 \\end{array}\\right)$ и $\\vec{y}=\\left(\\begin{array}{} 4\\\\5\\\\2\\\\2\\\\6\\\\8\\\\-1 \\end{array}\\right)$\n",
    "\n",
    "Записываем нашу модель:\n",
    "\n",
    "$\\vec{y}=w_{0}+w_{1} x_{1}+w_{2} x_{2}+w_{3} x_{1}^{2}+w_{4} x_{1} x_{2}+w_{5} x_{2}^{2}$\n",
    "\n",
    "$\\left(\\begin{array}{} 4\\\\5\\\\2\\\\2\\\\6\\\\8\\\\-1 \\end{array}\\right)=w_0+w_1\\left(\\begin{array}{} 1\\\\3\\\\-2\\\\1\\\\5\\\\13\\\\1 \\end{array}\\right)+w_2\\left(\\begin{array}{} 3\\\\4\\\\5\\\\-2\\\\4\\\\11\\\\3 \\end{array}\\right)+w_3\\left(\\begin{array}{} 1^2\\\\3^2\\\\-2^2\\\\1^2\\\\5^2\\\\13^2\\\\1^2 \\end{array}\\right)+w_4\\left(\\begin{array}{} 1\\cdot 3\\\\3\\cdot 4\\\\-2\\cdot 5\\\\1\\cdot (-2)\\\\5\\cdot 4\\\\13\\cdot 11\\\\1\\cdot 3 \\end{array}\\right)+w_5\\left(\\begin{array}{} 3^2\\\\4^2\\\\5^2\\\\-2^2\\\\4^2\\\\11^2\\\\3^2 \\end{array}\\right)$\n",
    "\n",
    "Записываем систему в матричном виде:\n",
    "\n",
    "$A\\vec{w}=\\vec{y}$\n",
    "\n",
    "$A=\\left(\\begin{array}{}1&z_{11}&z_{21}&z_{31}&z_{41}&z_{51}\\\\1&z_{12}&z_{22}&z_{32}&z_{42}&z_{52}\\\\1&z_{13}&z_{23}&z_{33}&z_{43}&z_{53}\\\\1&z_{13}&z_{23}&z_{33}&z_{43}&z_{53}\\\\1&z_{15}&z_{25}&z_{35}&z_{45}&z_{55}\\\\1&z_{16}&z_{26}&z_{36}&z_{46}&z_{56}\\\\1&z_{17}&z_{27}&z_{37}&z_{47}&z_{57}\\end{array}\\right)=\\left(\\begin{array}{}1&1&3&1&3&9\\\\1&3&4&9&12&16\\\\1&-2&5&4&-10&25\\\\1&1&-2&1&-2&4\\\\1&5&4&25&20&16\\\\1&13&11&169&143&121\\\\1&1&3&1&3&9\\end{array}\\right)$\n",
    "\n",
    "$\\vec{w}=(w_0,w_1,w_2,w_3,w_4,w_5)^T$\n",
    "\n",
    "$\\vec{y}=(4,5,2,2,6,8,-1)^T$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что первое и последнее уравнение системы противоречат друг другу. Можно не считать ранг —сразу понятно, что система будет переопределённой и нужно искать приблизительные решения по МНК:\n",
    "\n",
    "$\\vec{w}=\\left(A^{T} A\\right)^{-1} A^{T} \\vec{y}$\n",
    "\n",
    "Чтобы решить такую задачу, без Python и матричных вычислений точно не обойтись. Переведём наши условия в программную реализацию. С точки зрения программы самое сложное в этой задаче — правильно записать матрицу $A$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.25799015  2.37672337 -0.1322068  -0.10208147 -0.26501791  0.29722471]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([\n",
    "    [1, 1, 1, 1, 1, 1, 1],\n",
    "    [1, 3, -2, 1, 5, 13, 1],\n",
    "    [3, 4, 5, -2, 4, 11, 3],\n",
    "    [1, 9, 4, 1, 25, 169, 1],\n",
    "    [3, 12, -10, -2, 20, 143, 3],\n",
    "    [9, 16, 25, 4, 16, 121, 9]\n",
    "    \n",
    "]).T\n",
    "y = np.array([4, 5, 2, 2, 6, 8, -1])\n",
    "w_hat = np.linalg.inv(A.T@A)@A.T@y\n",
    "print(w_hat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, наш вектор приближений коэффициентов найден:\n",
    "\n",
    "$\\hat{\\vec{w}}=(-2.26, 2.38, -0.13, -0.1, -0.27, 0.3)^T$\n",
    "\n",
    ">Как вы понимаете, вручную считать коэффициенты полиномиальной регрессии — неблагодарное дело. А ведь мы с вами рассмотрели только случай полинома второй степени с двумя факторами. Расти может как степень полинома, так и количество факторов. Можно представить, какую размерность может приобрести система уравнений.\n",
    ">\n",
    ">Например, уравнение модели полинома третьей степени для случая двух факторов будет иметь следующий вид:\n",
    ">\n",
    ">$\\vec{y}=w_{0}+w_{1} \\vec{x}_{1}+w_{2} \\vec{x}_{2}+w_{3} \\vec{x}_{1}^{2}+w_{4} \\vec{x}_{1} \\vec{x}_{2}+w_{5} \\vec{x}_{2}^{2}+w_{6} \\vec{x}_{1}^{3}+w_{7} \\vec{x}_{1}^{2} \\vec{x}_{2}+w_{8} \\vec{x}_{2}^{3}+w_{9} \\vec{x}_{1} \\vec{x}_{2}^{2}$\n",
    ">\n",
    ">Количество неизвестных уже равно 10, а факторов пока ещё два. Как говорится, то ли ещё будет...\n",
    "\n",
    "**Примечание**. Кстати, для того чтобы определить количество коэффициентов в регрессии, есть формула:\n",
    "\n",
    "$c = \\frac{n!}{(n-d)!d!},$\n",
    "\n",
    "$n = k + d,$\n",
    "\n",
    "где $k$ — количество факторов, $d$ — степень полинома, а $!$ — символ факториала. Например, для двух факторов и пятой степени полинома будем иметь:\n",
    "\n",
    "$n=2+5=7$\n",
    "\n",
    "$c = \\frac{n!}{(n-d)!d!} = \\frac{7!}{(7-5)!5!} = \\frac{7!}{2!5!} = \\frac{1 \\cdot 2 \\cdot 3 \\cdot 4 \\cdot 5 \\cdot 6 \\cdot 7}{(1 \\cdot 2) \\cdot (1 \\cdot 2 \\cdot 3 \\cdot 4 \\cdot 5)} = \\frac{42}{2} = 21$\n",
    "\n",
    "То есть в матрице измерений $A$ **будет 21 столбец**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Конечно, вручную создавать полиномиальные столбцы в матрице наблюдений мы не будем. В модуле «ML-2. Обучение с учителем: регрессия» мы с вами уже знакомились с полиномиальными признаками, генерация которых реализована в классе [PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) из модуля preprocessing. \n",
    "\n",
    "### Пример № 3\n",
    "\n",
    "Строится полиномиальная регрессия второй степени, задано три фактора:\n",
    "\n",
    "$\\vec{x}_1=\\left(\\begin{array}{} 1\\\\3\\\\-2\\\\1\\\\5\\\\13\\\\1 \\end{array}\\right), \\vec{x}_2=\\left(\\begin{array}{} 3\\\\4\\\\5\\\\-2\\\\4\\\\11\\\\3 \\end{array}\\right), \\vec{x}_3=\\left(\\begin{array}{} 4\\\\5\\\\2\\\\2\\\\6\\\\8\\\\-1 \\end{array}\\right)$\n",
    "\n",
    "Создайте матрицу наблюдений $A_{poly}$ со сгенерированными полиномиальными признаками.\n",
    "\n",
    "Для начала составим обычную матрицу наблюдений $A$, расположив векторы в столбцах. Обратите внимание, что вектор из 1 мы не будем добавлять в матрицу (за нас это сделает генератор полиномиальных признаков):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  3  4]\n",
      " [ 3  4  5]\n",
      " [-2  5  2]\n",
      " [ 1 -2  2]\n",
      " [ 5  4  6]\n",
      " [13 11  8]\n",
      " [ 1  3 -1]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([\n",
    "    [1, 3, -2, 1, 5, 13, 1],\n",
    "    [3, 4, 5, -2, 4, 11, 3],\n",
    "    [4, 5, 2, 2, 6, 8, -1],\n",
    "]).T\n",
    "print(A)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Затем импортируем класс `PolynomialFeatures` из библиотеки `sklearn`. Создадим объект этого класса, указав при инициализации степень полинома равной `2`. Также укажем, что нам нужна генерация столбца из 1 (параметр `include_bias=True`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "poly = PolynomialFeatures(degree=2, include_bias=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осталось только вызвать метод `fit_transform()` от имени этого объекта и передать в него нашу матрицу наблюдений $A$. Для удобства выведем результат в виде `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2    3      4      5      6      7     8     9\n",
       "0  1.0   1.0   3.0  4.0    1.0    3.0    4.0    9.0  12.0  16.0\n",
       "1  1.0   3.0   4.0  5.0    9.0   12.0   15.0   16.0  20.0  25.0\n",
       "2  1.0  -2.0   5.0  2.0    4.0  -10.0   -4.0   25.0  10.0   4.0\n",
       "3  1.0   1.0  -2.0  2.0    1.0   -2.0    2.0    4.0  -4.0   4.0\n",
       "4  1.0   5.0   4.0  6.0   25.0   20.0   30.0   16.0  24.0  36.0\n",
       "5  1.0  13.0  11.0  8.0  169.0  143.0  104.0  121.0  88.0  64.0\n",
       "6  1.0   1.0   3.0 -1.0    1.0    3.0   -1.0    9.0  -3.0   1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "A_poly = poly.fit_transform(A)\n",
    "display(pd.DataFrame(A_poly))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, мы получили нашу матрицу $A_{poly}$. Давайте посмотрим на её столбцы:\n",
    "\n",
    "* столбец 0 — единичный, он отвечает за слагаемое с нулевой степенью полинома (любое число в степени 0 даёт единицу).\n",
    "\n",
    "* столбцы 1, 2 и 3 — это наши исходные признаки (векторы $\\vec{x}_1$, $\\vec{x}_2$  и $\\vec{x}_3$).\n",
    "\n",
    "* столбцы 4, 5 и 6 — произведения первого столбца со всеми столбцами: $\\vec{x}_1\\vec{x}_1=\\vec{x}_1^2$, $\\vec{x}_1\\vec{x}_2$ и $\\vec{x}_1\\vec{x}_3$ соответственно.\n",
    "\n",
    "* столбцы 7 и 8 — произведения второго столбца со столбцами 2 и 3: $\\vec{x}_2\\vec{x}_2=\\vec{x}_2^2$ и $\\vec{x}_2\\vec{x}_3$.\n",
    "\n",
    "* столбец 9 — произведение третьего столбца с самим собой: $\\vec{x}_3\\vec{x}_2=\\vec{x}_3^2$.\n",
    "\n",
    "Таким образом, при генерации полиномиальных признаков объект `PolynomialFeatures` сначала создаёт исходные факторы, затем умножает каждый из них на все факторы и повторяет процедуру. При этом, если комбинация $\\vec{x}_i\\vec{x}_j$ уже была сгенерирована ранее, то комбинация $\\vec{x}_i\\vec{x}_j$ не рассматривается.\n",
    "\n",
    "А теперь построим модель полиномиальной регрессии **на реальных данных**.\n",
    "\n",
    "Возьмём все те же данные о стоимости жилья в районах Бостона. Будем использовать следующие четыре признака: `LSTAT`, `CRIM`, `PTRATIO` и `RM`. С их помощью мы построим полиномиальную регрессию от первой до пятой степени включительно, а затем сравним результаты по значению средней абсолютной процентной ошибки (`MAPE`).\n",
    "\n",
    "Чтобы не дублировать код, объявим функцию `polynomial_regression()`. Она будет принимать на вход матрицу наблюдений, вектор ответов и степень полинома, а возвращать матрицу с полиномиальными признаками, вектор предсказаний и коэффициенты регрессии, найденные по МНК:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_regression(X, y, k):\n",
    "    poly = PolynomialFeatures(degree=k, include_bias=True)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    w_hat = np.linalg.inv(X_poly.T@X_poly)@X_poly.T@y\n",
    "    y_pred = X_poly @ w_hat\n",
    "    return X_poly, y_pred, w_hat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выделяем интересующие нас признаки и строим полиномы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = boston_data[['LSTAT', 'PTRATIO', 'RM', 'CRIM']]\n",
    "y = boston_data[['PRICE']]\n",
    " \n",
    "A_poly, y_pred, w_hat = polynomial_regression(A, y, 1)\n",
    "A_poly2, y_pred2, w_hat2 = polynomial_regression(A, y, 2)\n",
    "A_poly3, y_pred3, w_hat3 = polynomial_regression(A, y, 3)\n",
    "A_poly4, y_pred4, w_hat4 = polynomial_regression(A, y, 4)\n",
    "A_poly5, y_pred5, w_hat5 = polynomial_regression(A, y, 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на качество построенных регрессий, вычислив метрику:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE для полинома 1-й степени 18.20%\n",
      "MAPE для полинома 2-й степени  13.41%\n",
      "MAPE для полинома 3-й степени  12.93%\n",
      "MAPE для полинома 4-й степени  10.75%\n",
      "MAPE для полинома 5-й степени  163.95%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    " \n",
    "print('MAPE для полинома 1-й степени {:.2f}%'.format(mean_absolute_percentage_error(y, y_pred)*100))\n",
    "print('MAPE для полинома 2-й степени  {:.2f}%'.format(mean_absolute_percentage_error(y, y_pred2)*100))\n",
    "print('MAPE для полинома 3-й степени  {:.2f}%'.format(mean_absolute_percentage_error(y, y_pred3)*100))\n",
    "print('MAPE для полинома 4-й степени  {:.2f}%'.format(mean_absolute_percentage_error(y, y_pred4)*100))\n",
    "print('MAPE для полинома 5-й степени  {:.2f}%'.format(mean_absolute_percentage_error(y, y_pred5)*100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что видим? Полиномиальная регрессия первой степени (линейная регрессия) показывает наименьшее качество предсказания, так как зависимость между факторами и целевым признаком нелинейная. С повышением степени полинома процентная ошибка на обучающей выборке вроде бы падает, однако для полинома пятой степени она резко возрастает и начинает измеряться тысячами процентов. Это означает, что модель вообще не описывает зависимость в исходных данных — её прогноз не имеет никакого отношения к действительности.\n",
    "\n",
    "**Почему так происходит?**\n",
    "\n",
    "Проведём небольшое исследование. Для начала посмотрим на коэффициенты регрессии для полинома пятой степени. Смотреть на каждый из них неудобно, их слишком много (126, если быть точными), но можно взглянуть на минимум, максимум и среднее:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>126.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>799.393104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>21862.096310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-108158.224809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.653310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.000517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.782735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>216753.696598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               PRICE\n",
       "count     126.000000\n",
       "mean      799.393104\n",
       "std     21862.096310\n",
       "min   -108158.224809\n",
       "25%        -0.653310\n",
       "50%        -0.000517\n",
       "75%         0.782735\n",
       "max    216753.696598"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pd.DataFrame(w_hat5).describe())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что в степенях минимального и максимального коэффициентов явно что-то не так — коэффициенты **слишком огромные** (исчисляются миллионами).\n",
    "\n",
    "Теперь давайте взглянем на корреляционную матрицу для факторов, на которых мы строим полином пятой степени. Корреляцию со столбцом из единиц считать бессмысленно, поэтому мы не будем его рассматривать. Для удобства расчёта матрицы корреляций обернём матрицу  в DataFrame и воспользуемся методом `corr()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ранг корреляционной матрицы: 110\n",
      "Количество факторов: 125\n"
     ]
    }
   ],
   "source": [
    "# считаем матрицу корреляций (без столбца из единиц)\n",
    "C = pd.DataFrame(A_poly5[:, 1:]).corr()\n",
    "# считаем ранг корреляционной матрицы\n",
    "print('Ранг корреляционной матрицы:', np.linalg.matrix_rank(C))\n",
    "# считаем количество факторов (не включая столбец из единиц)\n",
    "print('Количество факторов:', A_poly5[:, 1:].shape[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Мы нашли корень проблемы: ранг корреляционной матрицы — 110, в то время как общее количество факторов (не считая единичного столбца) — 125, то есть ранг корреляционной матрицы не максимален. Это значит, что в корреляционной матрице присутствуют единичные корреляции, а в исходной матрице — линейно зависимые столбцы.\n",
    ">\n",
    ">Как так вышло? На самом деле всё очень просто: в процессе перемножения каких-то из столбцов при создании полинома пятой степени получился такой полиномиальный фактор, который линейно выражается через другие факторы.\n",
    ">\n",
    ">В результате при вычислении обратной матрицы  у нас получилось деление на число, близкое к 0, а элементы обратной матрицы получились просто огромными. Отсюда и появились явно неверные степени коэффициентов, которые дают далёкий от действительности прогноз, что приводит к отрицательной метрике.\n",
    "\n",
    "Кстати, заметим, что, например, для полинома четвёртой степени ранг матрицы корреляций максимален, то есть равен количеству факторов (не включая единичный столбец):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ранг корреляционной матрицы: 69\n",
      "Количество факторов: 69\n"
     ]
    }
   ],
   "source": [
    "# считаем матрицу корреляций (без столбца из единиц)\n",
    "C = pd.DataFrame(A_poly4[:, 1:]).corr()\n",
    "# считаем ранг корреляционной матрицы\n",
    "print('Ранг корреляционной матрицы:', np.linalg.matrix_rank(C))\n",
    "# считаем количество факторов (не включая столбец из единиц)\n",
    "print('Количество факторов:', A_poly4[:, 1:].shape[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэтому и коэффициенты регрессии полинома четвёртой степени находятся в адекватных пределах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>70.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-50.894226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>887.341157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-6925.403050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.187946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.000779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.322237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2305.046321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             PRICE\n",
       "count    70.000000\n",
       "mean    -50.894226\n",
       "std     887.341157\n",
       "min   -6925.403050\n",
       "25%      -0.187946\n",
       "50%      -0.000779\n",
       "75%       0.322237\n",
       "max    2305.046321"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pd.DataFrame(w_hat4).describe())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь посмотрим, что будет, если использовать для построения полиномиальной регрессии реализацию из библиотеки `sklearn`. Создадим функцию `polynomial_regression_sk` — она будет делать то же самое, что и прошлая функция, но средствами `sklearn`. Дополнительно будем смотреть также стандартное отклонение (разброс) по коэффициентам регрессии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE для полинома степени 1 — 18.20%, СКО — 2\n",
      "MAPE для полинома степени 2 — 13.41%, СКО — 5\n",
      "MAPE для полинома степени 3 — 12.93%, СКО — 9\n",
      "MAPE для полинома степени 4 — 10.74%, СКО — 304\n",
      "MAPE для полинома степени 5 — 9.02%, СКО — 17055\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def polynomial_regression_sk(X, y, k):\n",
    "    poly = PolynomialFeatures(degree=k, include_bias=False)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    lr = LinearRegression().fit(X_poly, y)\n",
    "    y_pred = lr.predict(X_poly)\n",
    "    return X_poly, y_pred, lr.coef_\n",
    "\n",
    "A = boston_data[['LSTAT', 'PTRATIO', 'RM', 'CRIM']]\n",
    "y = boston_data[['PRICE']]\n",
    "\n",
    "for k in range(1, 6):\n",
    "    A_poly, y_pred, w_hat = polynomial_regression_sk(A, y, k)\n",
    "    print(\n",
    "        \"MAPE для полинома степени {} — {:.2f}%, СКО — {:.0f}\".format(\n",
    "            k, mean_absolute_percentage_error(y, y_pred)*100, w_hat.std()\n",
    "        )\n",
    "\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очередная «магия» `sklearn` — построение полинома пятой степени прошло успешно.\n",
    "\n",
    "Почему так получилось, если, строя полином «руками», мы получали противоположный результат?\n",
    "\n",
    "На самом деле с этим «заклинанием» из библиотеки `sklearn` мы уже знакомились в предыдущих юнитах. Секрет в том, что в `sklearn` для построения линейной регрессии используется не сама матрица наблюдений $A$, а её сингулярное разложение, которое гарантированно является невырожденным — из него исключаются линейно зависимые факторы. Таким образом, даже несмотря на немаксимальный ранг корреляционной матрицы, построить полином пятой степени всегда получится.\n",
    "\n",
    "Однако коэффициенты полинома пятой степени обладают значительно бόльшим разбросом, чем другие модели. Разброс будет всё больше расти при увеличении степени полинома. Коэффициенты не будут отражать реальной зависимости в данных и будут построены так, чтобы компенсировать линейную зависимость факторов, то есть будут неустойчивыми.\n",
    "\n",
    "К тому же, как мы уже знаем, чем выше степень полинома, тем выше шанс переобучения: модель может быть настолько сложной, что попросту попытается пройти через все точки в обучающем наборе данных, не уловив общей закономерности. Пример такой переобученной модели представлен ниже:\n",
    "\n",
    "<img src=m2_img10.png>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Резюмируем ↓\n",
    "\n",
    "* Модель полиномиальной регрессии — более общий случай линейной регрессии, в котором зависимость целевой переменной от факторов нелинейная.\n",
    "\n",
    "* Поиск коэффициентов полинома аналогичен линейной регрессии — решение неоднородной СЛАУ. \n",
    "\n",
    "* Возможна ситуация, когда какие-то сгенерированные полиномиальные факторы могут линейно выражаться через другие факторы. Тогда ранг корреляционной матрицы будет меньше числа факторов и поиск по классическому МНК-алгоритму не будет успешным.\n",
    "\n",
    "* В `sklearn` для решения последней проблемы предусмотрена защита — использование сингулярного разложения матрицы $A$. Однако данная защита не решает проблемы неустойчивости коэффициентов регрессии.\n",
    "\n",
    "* Полиномиальная регрессия имеет сильную склонность к переобучению: чем выше степень полинома, тем сложнее модель и выше риск переобучения."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Задание 6.4\n",
    "\n",
    "С помощью классического МНК найдите коэффициенты полиномиальной регрессии, если используется полином второй степени и задан фактор $\\vec{x}$ и целевая переменная $\\vec{y}$.\n",
    "\n",
    "$\\vec{y} = w_0 + w_1 \\vec{x} + w_2 \\vec{x}^2$\n",
    "\n",
    "$\\vec{x}=\\left(\\begin{array}{c}1 \\\\3 \\\\-2 \\\\9\\end{array}\\right) \\vec{y}=\\left(\\begin{array}{c}3 \\\\7 \\\\-5 \\\\21\\end{array}\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.11446013  2.46095638 -0.01608801]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([\n",
    "    [1, 1, 1, 1],\n",
    "    [1, 3, -2, 9],\n",
    "    [1, 9, 4, 81]\n",
    "    \n",
    "]).T\n",
    "y = np.array([3, 7, -5, 21])\n",
    "w_hat = np.linalg.inv(A.T@A)@A.T@y\n",
    "print(w_hat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Регуляция <a class=\"anchor\" id=7></a>\n",
    "\n",
    "[к содержанию](#0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим модель полиномиальной регрессии третьей степени. Будем использовать данные о жилье в Бостоне и возьмём следующие четыре признака: `LSTAT`, `CRIM`, `PTRATIO` и `RM`.\n",
    "\n",
    "Для оценки качества модели будем использовать кросс-валидацию и сравнивать среднее значение метрики на тренировочных и валидационных фолдах. Кросс-валидацию организуем с помощью функции `cross_validate` из модуля `model_selection`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'PRICE']\n",
    "boston_data = pd.read_csv('housing.csv', header=None, delimiter=r\"\\s+\", names=column_names)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метрики используем среднюю абсолютную процентную ошибку — `MAPE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE на тренировочных фолдах: 12.64 %\n",
      "MAPE на валидационных фолдах: 24.16 %\n"
     ]
    }
   ],
   "source": [
    "# выделяем интересующие нас факторы\n",
    "X = boston_data[['LSTAT', 'PTRATIO', 'RM','CRIM']]\n",
    "y = boston_data[['PRICE']]\n",
    " \n",
    "# добавляем полиномиальные признаки\n",
    "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X = poly.fit_transform(X)\n",
    " \n",
    "# создаём модель линейной регрессии\n",
    "lr = LinearRegression()\n",
    " \n",
    "# оцениваем качество модели на кросс-валидации, метрика — MAPE\n",
    "cv_results = cross_validate(lr, X, y, scoring='neg_mean_absolute_percentage_error', cv=5, return_train_score=True)\n",
    "print('MAPE на тренировочных фолдах: {:.2f} %'.format(-cv_results['train_score'].mean()* 100))\n",
    "print('MAPE на валидационных фолдах: {:.2f} %'.format(-cv_results['test_score'].mean() * 100))\t"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что мы видим? Даже при, казалось бы, небольшой, третьей степени полинома мы получили переобучение: на тренировочной выборке $\\text{MAPE}=12.64\\%$, а вот на тестовой — $\\text{MAPE}=24.16\\%$. Показатели качества отличаются практически в два раза, что говорит о высоком разбросе модели. Ещё более удручающий результат мы получим, если воспользуемся полиномом большей степени (при желании вы можете проверить это самостоятельно).\n",
    "\n",
    "Как с этим справиться, мы тоже уже знаем.\n",
    "\n",
    "* Можно попробовать понизить сложность модели (снизить степень полинома). Но до какой степени? Можно постепенно перебирать степень полинома до тех пор, пока не получим адекватные результаты, но, согласитесь, процедура не очень приятная.\n",
    "\n",
    "* Можно воспользоваться методами **регуляризации**.\n",
    "\n",
    "О втором способе как раз и поговорим подробнее с математической точки зрения.\n",
    "\n",
    "Для начала вспомним, что такое регуляризация.\n",
    "\n",
    ">**Регуляризация** — это способ уменьшения переобучения моделей машинного обучения путём намеренного увеличения смещения модели для уменьшения её разброса.\n",
    "\n",
    "Регуляризация для линейной регрессии преследует сразу несколько целей. Однако далее мы увидим, что все эти цели на самом деле взаимосвязаны:\n",
    "\n",
    "* предотвратить переобучение модели;\n",
    "\n",
    "* включить в функцию потерь штраф за переобучение;\n",
    "\n",
    "* обеспечить существование обратной матрицы $(A^TA)^{-1}$;\n",
    "\n",
    "* не допустить огромных коэффициентов модели."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Мы знаем, что большие значения весов — прямое свидетельство переобучения модели линейной регрессии и её нестабильности. Идея регуляризации состоит в наложении ограничения на вектор весов (часто говорят — наложение штрафа за высокие веса). В качестве штрафа принято использовать **норму вектора весов**.\n",
    "\n",
    "Давайте запишем это на языке линейной алгебры. Вот задача минимизации длины вектора ошибок, о которой мы говорили, когда выводили формулу МНК:\n",
    "\n",
    "$\\left\\|\\vec{y} - A\\vec{w} \\right\\|^2 \\rightarrow min,$\n",
    "\n",
    "где $\\vec{y}$ — вектор истинных ответов, $A$ — матрица наблюдений, $\\vec{w}$ — вектор весов линейной регрессии $\\vec{w}=(w_0, w_1, w_2, …, w_k)^T$.\n",
    "\n",
    "Вот её приближённое решение по МНК:\n",
    "\n",
    "$\\vec{w}=\\left(A^{T} A\\right)^{-1} A^{T} \\vec{y}$\n",
    "\n",
    "Теперь в исходную задачу оптимизации добавим ограничение на норму вектора весов — она не должна превышать некоторого заранее заданного $b$:\n",
    "\n",
    "$\\left\\{\\begin{array}{}\\left\\|\\vec{y} - A\\vec{w} \\right\\|^2 \\rightarrow min, \\\\ (\\left\\|\\vec{w}\\right\\|_{Lp})^p \\leq b \\end{array}\\right.$\n",
    "\n",
    "где $\\left\\|\\vec{w}\\right\\|_{Lp}$ — норма вектора порядка $p>1$, которая определяется как:\n",
    "\n",
    "$\\|\\vec{w}\\|_{L_{p}}=\\sqrt[p]{\\sum_{i=0}^{k}\\left|w_{i}\\right|^{p}}$\n",
    "\n",
    ">**Примечание**. Обратите внимание на сумму под знаком корня. У нас она начинается с $i=0$. Однако иногда в литературе, например [здесь](https://dyakonov.org/2019/10/31/линейная-регрессия/), можно встретить $i=1$, то есть свободный член $w_0$ рекомендуется не регуляризировать (не ограничивать). На самом деле это утверждение эвристическое и может как выполняться, так и нет, в зависимости от особенностей реализации. Мы будем придерживаться реализации в `sklearn`, в которой  $w_0$ всё-таки включается в регуляризацию. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Порядок нормы $p$ в общем случае может быть любой — главное, чтобы она была больше 1. Однако на практике распространены только первая и вторая степени, так называемые $L_1$ - и $L_2$-**регуляризации**. О них мы поговорим ниже, а пока доведём решение задачи до конца и получим общую формулу.\n",
    "\n",
    "Поставленная задача оптимизации называется **условной** — мы ищем минимум при некотором условии. Мы ещё не умеем решать такие задачи, но обязательно научимся, а пока на минутку заглянем в теорию оптимизации и поговорим о **методе множителей Лагранжа**. Прелесть данного метода в том, что он позволяет свести условную задачу оптимизации к безусловной, то есть благодаря Лагранжу мы можем перейти от системы к одному уравнению.\n",
    "\n",
    "Метод множителей Лагранжа говорит, что записанная система с ограничением эквивалентна следующей записи:\n",
    "\n",
    "$L(\\vec{w}, \\alpha)=\\|\\vec{y}-A \\vec{w}\\|^{2}+\\alpha\\left(\\|\\vec{w}\\|_{L_{p}}\\right)^{p} \\rightarrow \\min,$\n",
    "\n",
    "где $L(\\vec{w},\\alpha)$ — функция Лагранжа, которая зависит не только от вектора весов модели $\\vec{w}$, но и от некоторой константы $\\alpha \\geq 0$— множителя Лагранжа.\n",
    "\n",
    "Это и есть финальный результат, на котором мы пока что остановимся. По сути, ничего особо не изменилось по сравнению с изначальной задачей оптимизации. Добавилось только одно слагаемое — $\\alpha ( \\| \\vec{w} \\|_{L_{p}})^p$. Заметим, что если $\\alpha=0$, то мы получаем исходную задачу $\\| \\vec{y} - A\\vec{w}  \\|^2  \\rightarrow min$. Далее мы увидим, что это маленькое слагаемое очень сильно поможет нам победить переобучение модели.\n",
    "\n",
    "В машинном обучении множитель Лагранжа  принято называть **коэффициентом регуляризации**. Он отвечает за «силу» регуляризации. Чем он больше, тем меньшие значения может принимать слагаемое $\\ {\\left({‖\\overrightarrow{w}‖}_{L_p}\\right)}^p$, то есть тем сильнее ограничения на норму весов. В этом и была наша цель — ограничить веса."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $L_2$-регуляризация  <a class=\"anchor\" id=7-1></a>\n",
    "\n",
    "[к содержанию](#0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Начнём мы, как ни странно, с $L_2$-регуляризации, так как она очень наглядно показывает, как регуляризация обеспечивает невырожденность матрицы $A^TA$.\n",
    "\n",
    ">$L_2$-регуляризация (`Ridge`), или регуляризация по `Тихонову` — это регуляризация, в которой порядок нормы $p=2$. \n",
    "\n",
    "Тогда, если подставить $p=2$ в наши формулы, то оптимизационная задача в случае $L_2$-регуляризации будет иметь вид:\n",
    "\n",
    "${‖\\overrightarrow{w}‖}_{L_2}=\\sqrt[2]{\\sum^k_{i=0}{}{|w}_i{|}^2=}\\sqrt{\\sum^k_{i=0}{}{(w}_i)^2}$\n",
    "\n",
    "$\\left\\{\\begin{array}{}\\left\\|\\vec{y} - A\\vec{w} \\right\\|^2 \\rightarrow min, \\\\ (\\left\\|\\vec{w}\\right\\|_{Lp})^2 \\leq b \\end{array}\\right.$\n",
    "$\\leftrightarrow$\n",
    "$\\left\\{\\begin{array}{}\\left\\|\\vec{y} - A\\vec{w} \\right\\|^2 \\rightarrow min, \\\\ \\sum^k_{i=0}{}{(w}_i)^2 \\leq b \\end{array}\\right.$\n",
    "\n",
    ">**Примечание**. Видно, что норма порядка $p=2$ на самом деле является знакомой нам длиной вектора. То есть в случае $L_2$-регуляризации мы накладываем ограничение на длину вектора весов $\\vec{w}$.\n",
    "\n",
    "В терминах функции Лагранжа задача будет выглядеть как:\n",
    "\n",
    "${‖\\overrightarrow{y}-A\\overrightarrow{w}‖}^2+α\\sum^k_{i=0}{}{(w}_i)^2→min$\n",
    "\n",
    "Как мы отметили ранее, у данной задачи даже есть аналитическое решение, полученное математиком Тихоновым, вот оно:\n",
    "\n",
    "${\\widehat{\\overrightarrow{w}}}_{ridge}={\\left(A^TA+\\alpha E\\right)}^{-1}A^T \\overrightarrow{y},$\n",
    "\n",
    "где $E$ — единичная матрица размера $dim (I) =(k+1, k+1)$ вида:\n",
    "\n",
    "$E=\\left(\\begin{array}{} 1&0&\\dots&0\\\\0&1&\\dots&0\\\\\\dots&\\dots&\\dots&\\dots\\\\0&0&\\dots&1 \\end{array}\\right)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Примечание**. В других реализациях аналитического решения регуляризации Тихонова, отличных от `sklearn`, где коэффициент $w_0$ не участвует в регуляризации, единичная матрица $E$ заменяется на матрицу $I$, в которой первый столбец и первая строка — **нулевые**. Это делается для того, чтобы исключить коэффициент $w_0$ из регуляризации:\n",
    "\n",
    "$I=\\left(\\begin{array}{} 0&0&\\dots&0\\\\0&1&\\dots&0\\\\\\dots&\\dots&\\dots&\\dots\\\\0&0&\\dots&1 \\end{array}\\right)$\n",
    "\n",
    "Что мы в итоге получаем? Преимущество этой формулы в том, что, если $\\alpha>0$, то матрица $A^TA+\\alpha E$ гарантированно является невырожденной, даже если матрица $A^TA$ таковой не является. Так получается за счёт того, что по диагонали матрицы $A^TA$ мы добавляем поправки, которые создают линейную независимость между столбцами матрицы.\n",
    "\n",
    "Продемонстрируем это на примере ↓"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример № 1\n",
    "\n",
    "Построить линейную регрессию с $L_2$-регуляризацией, если:\n",
    "\n",
    "$\\vec{x}_1=\\left(\\begin{array}{}1\\\\0\\\\-3\\\\2\\\\4\\end{array}\\right)$,$\\vec{x}_2=\\left(\\begin{array}{}2\\\\0\\\\-6\\\\4\\\\8\\end{array}\\right)$,$\\vec{y}=\\left(\\begin{array}{}4\\\\3\\\\-4\\\\2\\\\7\\end{array}\\right)$\n",
    "\n",
    "Коэффициент регуляризации $\\alpha=5$.\n",
    "\n",
    "Давайте составим матрицу наблюдений $A$ для нашей задачи:\n",
    "\n",
    "$A=\\left(\\begin{array}{}1&1&2\\\\1&0&0\\\\1&-3&-6\\\\1&2&4\\\\1&4&8\\end{array}\\right)$\n",
    "\n",
    "Найдём матрицу Грама $A^TA$: \n",
    "\n",
    "$A^TA=\\left(\\begin{array}{}1&1&1&1&1\\\\1&0&-3&2&4\\\\1&0&-6&4&8\\end{array}\\right)\\left(\\begin{array}{}1&1&2\\\\1&0&0\\\\1&-3&-6\\\\1&2&4\\\\1&4&8\\end{array}\\right)=\\left(\\begin{array}{}5&4&8\\\\4&30&60\\\\8&60&120\\end{array}\\right)$\n",
    "\n",
    "Очевидно, что матрица $A^TA$ вырождена: её второй и третий столбцы являются пропорциональными с коэффициентом 2. Значит, наша классическая формула МНК  (без сингулярного разложения) не сработает.\n",
    "\n",
    "$\\widehat{\\overrightarrow{w}}={\\left(A^TA\\right)}^{-1}A^T\\overrightarrow{y}$\n",
    "\n",
    "Проверим:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16704/2679778142.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# получаем оценку коэффициентов регрессии по МНК\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mw_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36minv\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36minv\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m    543\u001b[0m     \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'D->D'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'd->d'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0mextobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_linalg_error_extobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m     \u001b[0mainv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_umath_linalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mainv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36m_raise_linalgerror_singular\u001b[0;34m(err, flag)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Singular matrix\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_nonposdef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# матрица наблюдений (включая столбец единиц)\n",
    "A = np.array([\n",
    "    [1, 1, 1, 1, 1],\n",
    "    [1, 0, -3, 2, 4],\n",
    "    [2, 0, -6, 4, 8]\n",
    "]).T\n",
    "# вектор целевого признака\n",
    "y = np.array([4, 3, -4, 2, 7])\n",
    "# получаем оценку коэффициентов регрессии по МНК\n",
    "w_hat = np.linalg.inv(A.T@A)@A.T@y\n",
    "print(w_hat) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы ожидаемо получили ошибку, говорящую о том, что матрица $A$ вырождена. \n",
    "\n",
    "Теперь попробуем воспользоваться регуляризацией Тихонова. Для этого составляем матрицу $E$. Она будет размером 3x3 (количество параметров — 3):\n",
    "\n",
    "$\\alpha E=5\\cdot\\left(\\begin{array}{}1&0&0\\\\0&1&0\\\\0&0&1\\end{array}\\right)=\\left(\\begin{array}{}5&0&0\\\\0&5&0\\\\0&0&5\\end{array}\\right)$\n",
    "\n",
    "Добавим регуляризационное слагаемое к матрице $A^TA$:\n",
    "\n",
    "$A^TA+\\alpha E=\\left(\\begin{array}{}5&4&8\\\\4&30&60\\\\8&60&120\\end{array}\\right)+\\left(\\begin{array}{}5&0&0\\\\0&5&0\\\\0&0&5\\end{array}\\right)=\\left(\\begin{array}{}10&4&8\\\\4&35&60\\\\8&60&125\\end{array}\\right)$\n",
    "\n",
    "Видно, что матрица $A^TA+\\alpha E$ уже не будет вырожденной: её столбцы уже не являются линейно зависимыми, а значит решение будет существовать.\n",
    "\n",
    "Попробуем найти вектор оценок весов ${\\widehat{\\overrightarrow{w}}}_{ridge}$ по формуле:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6122449  0.29387755 0.5877551 ]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([\n",
    "    [1, 1, 1, 1, 1],\n",
    "    [1, 0, -3, 2, 4],\n",
    "    [2, 0, -6, 4, 8]\n",
    "]).T\n",
    "# вектор целевого признака\n",
    "y = np.array([4, 3, -4, 2, 7])\n",
    "# единичная матрица\n",
    "E = np.eye(3)\n",
    "# коэффициент регуляризации \n",
    "alpha = 5\n",
    "# получаем оценку коэффициентов регрессии по МНК с регуляризацией Тихонова\n",
    "w_hat_ridge = np.linalg.inv(A.T@A+alpha*E)@A.T@y\n",
    "print(w_hat_ridge) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Работает! Мы получили вектор весов:\n",
    "\n",
    "$\\widehat{\\overrightarrow{w}}={\\left(0.61,\\ 0.29,\\ 0.59\\right)}^T$\n",
    "\n",
    "***\n",
    "\n",
    ">Итак, мы посмотрели, как работает аналитическое решение -регуляризации. Однако в реализации `sklearn` для решения этой задачи поддерживается сразу несколько методов — как численных (координатный спуск, градиентный спуск или [LBFGS](https://ru.wikipedia.org/wiki/Алгоритм_Бройдена_—_Флетчера_—_Гольдфарба_—_Шанно)), так и аналитических (классическая регуляризация Тихонова или она же через SVD-разложение). По умолчанию метод выбирается автоматически. На простых данных все методы будут показывать примерно одинаковые результаты при одном и том же значении коэффициента регуляризации, однако на реальных данных, когда данные не стандартизированы и присутствует сильная мультиколлинеарность между факторами, результат работы каждого из методов решения задачи оптимизации может значительно отличаться. Имейте это в виду при построении модели. Подробнее о методах вы можете прочитать в документации.\n",
    "\n",
    "Напомним, что за реализацию линейной регрессии в `sklearn` отвечает класс `Ridge`. Основной параметр модели, на который стоит обратить внимание — `alpha`, коэффициент регуляризации из формулы Тихонова.\n",
    "\n",
    "Давайте обучим модель для решения нашей последней задачи, а затем проверим коэффициенты регрессии. Так как мы заранее заложили в матрицу  столбец из единиц, то, чтобы получить корректное решение, параметр `fit_intercept` следует установить в значение `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6122449  0.29387755 0.5877551 ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# матрица наблюдений (включая столбец единиц)\n",
    "A = np.array([\n",
    "    [1, 1, 1, 1, 1],\n",
    "    [1, 0, -3, 2, 4],\n",
    "    [2, 0, -6, 4, 8]\n",
    "]).T\n",
    "# вектор целевого признака\n",
    "y = np.array([4, 3, -4, 2, 7])\n",
    "# получаем оценку коэффициентов регрессии по МНК с регуляризацией Тихонова\n",
    "ridge = Ridge(alpha=5, fit_intercept=False)\n",
    "ridge.fit(A, y)\n",
    "print(ridge.coef_) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, посмотрим, как регуляризация поможет побороть переобучение модели полиномиальной регрессии на наборе данных о домах в Бостоне. Используем те же самые признаки: `LSTAT`, `CRIM`, `PTRATIO` и `RM`. \n",
    "\n",
    "→ Сразу отметим, что для успешной сходимости численных методов оптимизации, которые используются для решения задачи условной оптимизации, необходима стандартизация (нормализация) исходных данных, которая не требовалась для аналитического МНК в классической линейной регрессии (`LinearRegression`).\n",
    "\n",
    ">**Примечание**. Здесь под стандартизацией мы понимаем именно приведение распределения признака к нулевому среднему и единичному стандартному отклонению (`StandartScaler`), а не стандартизацию векторов, о которой мы говорили в этом модуле. Последнюю также можно использовать в качестве способа масштабирования данных, однако её реализации нет в `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воспользуемся моделью полиномиальной регрессии третьей степени с регуляризацией Тихонова (коэффициент регуляризации возьмём равным 20) и проверим её качество на кросс-валидации по метрике `MAPE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE на тренировочных фолдах: 12.54 %\n",
      "MAPE на валидационных фолдах: 17.02 %\n"
     ]
    }
   ],
   "source": [
    "# выделяем интересующие нас факторы\n",
    "X = boston_data[['LSTAT', 'PTRATIO', 'RM','CRIM']]\n",
    "y = boston_data[['PRICE']]\n",
    "# инициализируем стандартизатор StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# подгоняем параметры стандартизатора (вычисляем среднее и СКО)\n",
    "X = scaler.fit_transform(X)\n",
    "# добавляем полиномиальные признаки\n",
    "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X = poly.fit_transform(X)\n",
    "# создаём модель линейной регрессии c L2-регуляризацией\n",
    "ridge = Ridge(alpha=20, solver='svd')\n",
    "# оцениваем качество модели на кросс-валидации\n",
    "cv_results = cross_validate(ridge, X, y, scoring='neg_mean_absolute_percentage_error', cv=5, return_train_score=True)\n",
    "print('MAPE на тренировочных фолдах: {:.2f} %'.format(-cv_results['train_score'].mean()* 100))\n",
    "print('MAPE на валидационных фолдах: {:.2f} %'.format(-cv_results['test_score'].mean() * 100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам удалось уменьшить ошибку (`MAPE`) на валидационных фолдах кросс-валидации с 24.16% до 17.02% и сократить разницу в метриках, тем самым уменьшив разброс ответов модели.\n",
    "\n",
    "Задание 7.4\n",
    "\n",
    "Вычислите коэффициенты линейной регрессии с -регуляризацией, используя аналитическую формулу Тихонова, если:\n",
    "\n",
    "$\\vec{x}_1=\\left(\\begin{array}{} 5\\\\9\\\\4\\\\3\\\\5 \\end{array}\\right),\\vec{x}_2=\\left(\\begin{array}{} 15\\\\18\\\\18\\\\19\\\\19\\\\ \\end{array}\\right),\\vec{x}_3=\\left(\\begin{array}{} 7\\\\6\\\\7\\\\7\\\\7 \\end{array}\\right),\\vec{y}=\\left(\\begin{array}{} 24\\\\22\\\\35\\\\33\\\\36 \\end{array}\\right),$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.08523045 -1.70784126  1.91141216  0.7293992 ]\n"
     ]
    }
   ],
   "source": [
    "# матрица наблюдений (включая столбец единиц)\n",
    "A = np.array([\n",
    "    [1, 1, 1, 1, 1],\n",
    "    [5, 9, 4, 3, 5],\n",
    "    [15, 18, 18, 19, 19],\n",
    "    [7, 6, 7, 7, 7]\n",
    "]).T\n",
    "# вектор целевого признака\n",
    "y = np.array([24, 22, 35, 33, 36])\n",
    "# получаем оценку коэффициентов регрессии по МНК с регуляризацией Тихонова\n",
    "ridge = Ridge(alpha=1, fit_intercept=False)\n",
    "ridge.fit(A, y)\n",
    "print(ridge.coef_) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $L_1$-РЕГУЛЯРИЗАЦИЯ  <a class=\"anchor\" id=7-2></a>\n",
    "\n",
    "[к содержанию](#0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    ">$L_1$-регуляризацией, `Lasso` (`Least Absolute Shrinkage and Selection Operator`), называется регуляризация, в которой порядок нормы $p=1$.\n",
    "\n",
    "Тогда оптимизационная задача в случае $L_1$-регуляризации будет иметь вид:\n",
    "\n",
    "${‖\\overrightarrow{w}‖}_{L_1}=\\sqrt[1]{\\sum^k_{i=0}{}{|w}_i{|}^1=}\\sum^k_{i=0}{}{|w}_i|$\n",
    "\n",
    "$\\left\\{\\begin{array}{}\\left\\|\\vec{y} - A\\vec{w} \\right\\|^2 \\rightarrow min, \\\\ (\\left\\|\\vec{w}\\right\\|_1)^1 \\leq b \\end{array}\\right.$\n",
    "$\\leftrightarrow$\n",
    "$\\left\\{\\begin{array}{}\\left\\|\\vec{y} - A\\vec{w} \\right\\|^2 \\rightarrow min, \\\\ \\sum^k_{i=0}{}{(w}_i)^1 \\leq b \\end{array}\\right.$\n",
    "\n",
    ">Примечание. Таким образом, в случае $L_1$-регуляризации мы ограничиваем сумму модулей весов модели. Такая величина называется [нормой Манхэттена](https://ru.wikipedia.org/wiki/Расстояние_городских_кварталов) (расстоянием городских кварталов).\n",
    "\n",
    "Запишем полученную систему в терминах метода Лагранжа:\n",
    "\n",
    "${‖\\overrightarrow{y}-A\\overrightarrow{w}‖}^2+α\\sum^k_{i=0}{}{|w}_i|→min$\n",
    "\n",
    "***\n",
    "\n",
    "→ Можно показать, что данная задача имеет аналитическое решение, однако в реализации `sklearn` оно даже не заявлено как возможное для использования в связи с нестабильностью взятия производной от функции модуля, поэтому мы не будем его рассматривать. Ознакомиться с ним вы можете [здесь](http://www.machinelearning.ru/wiki/images/7/7e/VetrovSem11_LARS.pdf).\n",
    "\n",
    "В `sklearn` $L_1$-регуляризация реализована в классе [Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html), а заданная выше оптимизационная задача решается **алгоритмом координатного спуска** (`Coordinate` `Descent`).\n",
    "\n",
    "Давайте посмотрим, как работает Lasso на «игрушечном» примере, а затем применим его для набора данных о домах в Бостоне."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример № 2\n",
    "\n",
    "Построить линейную регрессию с $L_1$-регуляризацией, если:\n",
    "\n",
    "$\\vec{x}_1=\\left(\\begin{array}{} 1\\\\0\\\\-3\\\\2\\\\4 \\end{array}\\right), \\vec{x}_2=\\left(\\begin{array}{} 2\\\\0\\\\-6\\\\4\\\\8 \\end{array}\\right), \\vec{y}=\\left(\\begin{array}{} 4\\\\3\\\\-4\\\\2\\\\7 \\end{array}\\right)$\n",
    "\n",
    "Коэффициент регуляризации $\\alpha=0.1$.\n",
    "\n",
    "Составим матрицу наблюдений $A$ для нашей задачи:\n",
    "\n",
    "$A=\\left(\\begin{array}{} 1&1&2\\\\1&0&0\\\\1&-3&-6\\\\1&2&4\\\\1&4&8 \\end{array}\\right)$\n",
    "\n",
    "Из примера №1 мы уже знаем, что матрица Грама $A^TA$ будет вырождена, а значит классического МНК-решения (не беря в расчёт сингулярное разложение) не получится.\n",
    "\n",
    "Попробуем найти коэффициенты регрессии с помощью $L_1$-регуляризации. Для этого подадим нашу матрицу наблюдений $A$ и вектор целевого признака $\\vec{y}$ в модель `Lasso`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.14925373 0.         0.71921642]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "# матрица наблюдений (включая столбец единиц)\n",
    "A = np.array([\n",
    "    [1, 1, 1, 1, 1],\n",
    "    [1, 0, -3, 2, 4],\n",
    "    [2, 0, -6, 4, 8]\n",
    "]).T\n",
    "# вектор целевого признака\n",
    "y = np.array([4, 3, -4, 2, 7])\n",
    "# получаем оценку коэффициентов регрессии с помощью L1-регуляризации\n",
    "lasso = Lasso(alpha=0.1, fit_intercept=False)\n",
    "lasso.fit(A, y)\n",
    "print(lasso.coef_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот наша оценка вектора весов:\n",
    "\n",
    "$\\widehat{\\overrightarrow{w}}={\\left(1.15,\\ 0,\\ 0.72\\right)}^T$\n",
    "\n",
    "Сразу обращаем внимание, что, в отличие от регуляризации Тихонова, $L_1$-регуляризация «занулила» коэффициент, стоящий при факторе $\\vec{x}_1$. Это произошло не случайно, так как это особенность данного метода. Как говорится, «не баг, а фича», причём очень важная. Коэффициенты, стоящие при коллинеарных или высококоррелированных факторах, зануляются. Также чем выше коэффициент регуляризации, тем больше вероятность того, что коррелированные или малозначащие факторы будут исключены из модели. Чуть позже мы рассмотрим геометрическую интерпретацию и поймём, почему так происходит.\n",
    "\n",
    "А пока давайте применим $L_1$-регуляризацию к нашей полиномиальной модели третьей степени, прогнозирующей типичную цену на дома в районах Бостона.\n",
    "\n",
    "Так как метод координатного спуска, который применяется для поиска коэффициентов, является численным, то необходима стандартизация исходных данных, чтобы обеспечить ему сходимость. Возьмём в качестве коэффициента регуляризации $\\alpha=0.1$ и проверим качество полученной модели с помощью кросс-валидации по метрике `MAPE`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE на тренировочных фолдах: 12.44 %\n",
      "MAPE на валидационных фолдах: 16.44 %\n"
     ]
    }
   ],
   "source": [
    "# выделяем интересующие нас факторы\n",
    "X = boston_data[['LSTAT', 'PTRATIO', 'RM','CRIM']]\n",
    "y = boston_data[['PRICE']]\n",
    "\n",
    "# инициализируем стандартизатор StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# подгоняем параметры стандартизатора (вычисляем среднее и СКО)\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# добавляем полиномиальные признаки\n",
    "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X = poly.fit_transform(X)\n",
    "\n",
    "# создаём модель линейной регрессии c L1-регуляризацией\n",
    "lasso = Lasso(alpha=0.1, max_iter=10000)\n",
    "\n",
    "# оцениваем качество модели на кросс-валидации\n",
    "cv_results = cross_validate(lasso, X, y, scoring='neg_mean_absolute_percentage_error', cv=5, return_train_score=True)\n",
    "print('MAPE на тренировочных фолдах: {:.2f} %'.format(-cv_results['train_score'].mean()* 100))\n",
    "print('MAPE на валидационных фолдах: {:.2f} %'.format(-cv_results['test_score'].mean() * 100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что с помощью $L_1$-регуляризации удалось уменьшить ошибку модели (`MAPE`) на валидационных фолдах с 24.16% до 16.44% и сократить разницу в метриках на тренировочных и валидационных фолдах даже лучше, чем с этим справилась $L_2$-регуляризация. Однако на самом деле мы просто удачно выбрали коэффициент регуляризации — при других значениях могли получиться совершенно другие результаты."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELASTIC-NET <a class=\"anchor\" id=7-2></a>\n",
    "\n",
    "[к содержанию](#0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Последний вид регуляризации (хотя их на самом деле больше), который мы рассмотрим, называется `Elastic`-`Net` (эластичная сетка). Это комбинация $L_1$- и $L_2$-регуляризации.\n",
    "\n",
    "Идея `Elastic-Net` состоит в том, что мы вводим ограничение как на норму весов порядка $p=1$, так и на норму порядка $p=2$. Тогда оптимизационная задача будет иметь вид:\n",
    "\n",
    "$\\left\\{\\begin{array}{}\\left\\|\\vec{y} - A\\vec{w} \\right\\|^2 \\rightarrow min, \\\\ \\\\ (\\left\\|\\vec{w}\\right\\|_{L_1})^1 \\leq b_1 \\\\ \\\\ (\\left\\|\\vec{w}\\right\\|_{L_2})^2 \\leq b_2 \\end{array}\\right.$\n",
    "$\\leftrightarrow$\n",
    "$\\left\\{\\begin{array}{}\\left\\|\\vec{y} - A\\vec{w} \\right\\|^2 \\rightarrow min, \\\\ \\\\ \\sum^k_{i=0}{}{|w_i|} \\leq b_1 \\\\ \\\\ \\sum^k_{i=0}{}{(w}_i)^2 \\leq b_2 \\end{array}\\right.$\n",
    "\n",
    "Немного модифицировав формулу функции Лагранжа, которая получается в результате такой задачи условной оптимизации, можно получить финальный результат:\n",
    "\n",
    "${\\|\\overrightarrow{y}-A\\overrightarrow{w}\\|}^2+\\alpha \\cdot \\lambda \\sum^k_{i=0}{}{|w}_i|+\\frac{\\alpha \\cdot (1-\\lambda )}{2}\\sum^k_{i=0}{}{(w}_i)^2\\to min$\n",
    "\n",
    "Здесь коэффициенты $\\alpha$ и $\\lambda$ отвечают за вклад слагаемых регуляризации.\n",
    "\n",
    "* Если $\\alpha=0$,  получаем классическую МНК-задачу оптимизации.\n",
    "* Если $\\lambda=1$, получаем `Lasso`-регрессию.\n",
    "* Если $\\alpha \\neq 0$, $\\lambda=0$, получаем `Ridge`-регрессию с коэффициентом $\\alpha/2$.\n",
    "\n",
    "Попробуйте самостоятельно подставить эти значения и убедиться в этом.\n",
    "\n",
    "Аналитического решения у этой задачи нет, поэтому для её решения в `sklearn`, как и для модели Lasso, используется **координатный спуск.**\n",
    "\n",
    "В sklearn эластичная сетка реализована в классе [ElasticNet](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html) из пакета с линейными моделями — `linear_model`. За коэффициент $\\alpha$ отвечает параметр `alpha`, за коэффициент $\\lambda$ — `l1_ratio`.\n",
    "\n",
    ">Некоторые рекомендации от разработчиков `ElasticNet`:\n",
    ">\n",
    ">* Использование параметра `l1_ratio <0.01` приводит к нестабильным результатам.\n",
    ">* Вместо использования `ElasticNet` с `alpha=0` лучше используйте `LinearRegression`, так как там применяется аналитическое решение, которое позволяет получать более точные решения, чем численный координатный спуск.\n",
    "П\n",
    "\n",
    "о традиции рассмотрим «игрушечный» пример работы с `Elastic-Net`, а затем применим эту модель к нашей задаче о домах в Бостоне."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример № 3\n",
    "\n",
    "Построить линейную регрессию с **Elastic-Net**-регуляризацией, если:\n",
    "\n",
    "$\\vec{x}_1=\\left(\\begin{array}{} 1\\\\0\\\\-3\\\\2\\\\4 \\end{array}\\right), \\vec{x}_2=\\left(\\begin{array}{} 2\\\\0\\\\-6\\\\4\\\\8 \\end{array}\\right), \\vec{y}=\\left(\\begin{array}{} 4\\\\3\\\\-4\\\\2\\\\7 \\end{array}\\right)$\n",
    "\n",
    "Решить задачу с тремя комбинациями коэффициентов регуляризации:\n",
    "1. $\\alpha=0.1$ и $\\lambda=0.2$\n",
    "2. $\\alpha=0.1$ и $\\lambda=0.7$\n",
    "3. $\\alpha=0.1$ и $\\lambda=1$\n",
    "\n",
    "Составим матрицу наблюдений $A$ для нашей задачи:\n",
    "\n",
    "$A=\\left(\\begin{array}{} 1&1&2\\\\1&0&0\\\\1&-3&-6\\\\1&2&4\\\\1&4&8 \\end{array}\\right)$\n",
    "\n",
    "Мы уже знаем, что матрица Грама $A^TA$ будет вырождена, а значит классического МНК-решения (не беря в расчёт сингулярное разложение) не получится.\n",
    "\n",
    "Сразу переходим к построению регрессии с помощью **ElasticNet**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Случай 1: $\\alpha=0.1$ и $\\lambda=0.2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.13492457 0.19525842 0.6237965 ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "# матрица наблюдений (включая столбец единиц)\n",
    "A = np.array([\n",
    "    [1, 1, 1, 1, 1],\n",
    "    [1, 0, -3, 2, 4],\n",
    "    [2, 0, -6, 4, 8]\n",
    "]).T\n",
    "# вектор целевого признака\n",
    "y = np.array([4, 3, -4, 2, 7])\n",
    "# получаем оценку коэффициентов регрессии \n",
    "lasso = ElasticNet(alpha=0.1, l1_ratio=0.2, fit_intercept=False)\n",
    "lasso.fit(A, y)\n",
    "print(lasso.coef_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получили оценку вектора коэффициентов:\n",
    "\n",
    "${\\widehat{\\overrightarrow{w}}}_{en_1}={\\left(1.13,\\ 0.2,\\ 0.62\\right)}^T$\n",
    "\n",
    "Обратим внимание, что зануления коэффициентов коллинеарных факторов $\\vec{x}_1$ и $\\vec{x}_2$ не произошло. Каждый из них вошёл в уравнение регрессии с ненулевым коэффициентом."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Случай 2: $\\alpha=0.1$ и $\\lambda=0.7$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.14379753 0.         0.71993025]\n"
     ]
    }
   ],
   "source": [
    "# получаем оценку коэффициентов регрессии\n",
    "lasso = ElasticNet(alpha=0.1, l1_ratio=0.7, fit_intercept=False)\n",
    "lasso.fit(A, y)\n",
    "print(lasso.coef_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получили оценку вектора коэффициентов:\n",
    "\n",
    "${\\widehat{\\overrightarrow{w}}}_{en_2}={\\left(1.14,\\ 0,\\ 0.72\\right)}^T$\n",
    "\n",
    "Обратим внимание, что произошло зануление коэффициентов. Это неспроста, так как мы понизили влияние $L_2$-регуляризации и одновременно повысили влияние $L_1$-регуляризации, которая, как мы уже знаем, приводит к исключению линейно зависимых факторов."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Случай 3: $\\alpha=0.1$ и $\\lambda=1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.14925373 0.         0.71921642]\n"
     ]
    }
   ],
   "source": [
    "# получаем оценку коэффициентов регрессии\n",
    "lasso = ElasticNet(alpha=0.1, l1_ratio=1, fit_intercept=False)\n",
    "lasso.fit(A, y)\n",
    "print(lasso.coef_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получили оценку вектора коэффициентов:\n",
    "\n",
    "${\\widehat{\\overrightarrow{w}}}_{en_3}={\\left(1.14,\\ 0,\\ 0.72\\right)}^T$\n",
    "\n",
    "В округлениях значения не заметно, однако если присмотреться к коэффициентам более внимательно, можно увидеть, что мы получили в точности те же значения, которые получали для модели `Lasso` в примере № 2. Неудивительно, ведь мы обнулили влияние $L_2$-регуляризации, выставив `l1_ratio=1`. По сути, мы использовали чистую модель `Lasso`.\n",
    "\n",
    "?\n",
    "Возникает вопрос: какой набор коэффициентов линейной регрессии всё-таки подходит лучше?\n",
    "\n",
    "Ответить на него можно, только вычислив метрику качества и сравнив ошибки прогнозов каждой из полученных моделей. Мы уверены, вы можете сделать это самостоятельно.\n",
    "\n",
    "Нам осталось только попробовать применить `Elastic-Net` к данным о недвижимости в Бостоне.\n",
    "\n",
    "Как и для других моделей с регуляризацией, для `Elastic-Net` также лучше заранее позаботиться о стандартизации данных. В качестве коэффициентов регуляризации возьмём ,  . Качество модели проверим с помощью кросс-валидации на пяти фолдах, метрика — `MAPE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE на тренировочных фолдах: 12.65 %\n",
      "MAPE на валидационных фолдах: 15.70 %\n"
     ]
    }
   ],
   "source": [
    "# выделяем интересующие нас факторы\n",
    "X = boston_data[['LSTAT', 'PTRATIO', 'RM','CRIM']]\n",
    "y = boston_data[['PRICE']]\n",
    "# инициализируем стандартизатор StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# подгоняем параметры стандартизатора (вычисляем среднее и СКО)\n",
    "X = scaler.fit_transform(X)\n",
    "# добавляем полиномиальные признаки\n",
    "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X = poly.fit_transform(X)\n",
    "# создаём модель линейной регрессии c L1- и L2-регуляризациями\n",
    "lasso = ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=10000)\n",
    "# оцениваем качество модели на кросс-валидации\n",
    "cv_results = cross_validate(lasso, X, y, scoring='neg_mean_absolute_percentage_error', cv=5, return_train_score=True)\n",
    "print('MAPE на тренировочных фолдах: {:.2f} %'.format(-cv_results['train_score'].mean()* 100))\n",
    "print('MAPE на валидационных фолдах: {:.2f} %'.format(-cv_results['test_score'].mean() * 100)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, `Elastic-Net` позволил нам уменьшить значение `MAPE` на валидационных фолдах с 24.16% до 15.7%. Отличный результат! Он получился лучше, чем у моделей `Ridge` и `Lasso`, но опять же скажем, что так бывает не всегда.\n",
    "\n",
    "***\n",
    "\n",
    "→ На практике при использовании моделей с регуляризацией стоит подбирать значения коэффициентов регуляризации с помощью методов подбора гиперпараметров, которые мы изучали в модуле «ML-7. Оптимизация гиперпараметров модели». Только после подбора гиперпараметров можно сделать вывод, какая из моделей показывает наилучшие результаты для решения конкретной задачи. Надеемся, вы помните, как подбираются гиперпараметры (если нет, освежите знания в модуле ML-7)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *ГЕОМЕТРИЧЕСКАЯ ИНТЕРПРЕТАЦИЯ РЕГУЛЯРИЗАЦИИ\n",
    "\n",
    "Напоследок поговорим о геометрической интерпретации регуляризации. В дальнейшем, изучая теорию оптимизации, мы увидим, что задача условной оптимизации\n",
    "\n",
    "$\\left\\{\\begin{array}{}\\left\\|\\vec{y} - A\\vec{w} \\right\\|^2 \\rightarrow min, \\\\ \\\\ (\\left\\|\\vec{w}\\right\\|_{L_p})^p \\leq b\\end{array}\\right.$\n",
    "\n",
    "геометрически означает поиск минимума функции\n",
    "\n",
    "$L(\\overrightarrow{w})= \\left\\|\\overrightarrow{y}-A\\overrightarrow{w} \\right\\|^2=\\sum^N_{i=1}{}(y_i-({\\overrightarrow{x}}_i,\\ \\overrightarrow{w}))^2$, которая отражает выпуклую поверхность, на пересечении с фигурой, которая образуется функцией $\\psi (\\overrightarrow{w})=( \\left\\|\\overrightarrow{w} \\right\\| _{L_p})^p$, ограниченной некоторым числом $b$.\n",
    "\n",
    "* В случае $L_1$-регуляризации выражение $\\sum^k_{i=0}{}{|w}_i|\\le b$ задаёт в пространстве параметров $w$ внутренность ромба с центром в начале координат:\n",
    "\n",
    "${|w}_0|+{|w}_1|+...+{|w}_k|=b$ — уравнение ромба\n",
    "\n",
    "* В случае $L_2$-регуляризации выражение $\\sum^k_{i=0}{}{(w}_i)^2\\le b$ задаёт окружность с центром в начале координат:\n",
    "\n",
    "${(w}_0)^2+{(w}_1)^2+...+{(w}_k)^2=b$— уравнение окружности\n",
    "\n",
    "Рассмотрим случай, когда фактор всего один ($k=1$), а в уравнении линейной регрессии присутствуют только два параметра: $w_0$ и $w_1$. Как мы знаем, в математике всё, что справедливо для меньших размерностей, справедливо и для бόльших.\n",
    "\n",
    "Посмотрим на рисунок ниже. На самом деле мы уже видели его, когда изучали общую постановку задачи регрессии."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=m2_img11.png>\n",
    "\n",
    "Концентрическими кругами обозначены линии равного уровня функции $L(w)$. Для каждого конкретного набора данных она будет иметь разный вид, но смысл будет тем же. **Голубой** областью обозначены ромб и окружность, которые задаёт $L_1$- и $L_2$-норма вектора весов соответственно.\n",
    "\n",
    "* Если бы мы использовали классическую линейную регрессию, то МНК приводил бы нас в точку истинного минимума функции $L(w)$ — в центр, из которого исходят концентрические круги. Это была бы некоторая комбинация параметров $w_0$ и $w_1$.\n",
    "\n",
    "* В случае, когда мы используем модель линейной регрессии с регуляризацией, мы будем пытаться найти такую комбинацию $w_0$ и $w_1$, которая доставляет минимум функции $L(w)$, но при этом не выходит за границы ромба (или окружности). Таким образом, вместо истинного минимума мы находим так называемый **псевдоминимум**.\n",
    "\n",
    "Заметим, что у ромба вероятность коснуться концентрического круга одной из своих вершин больше, чем у окружности — своей верхней/нижней/правой/левой точкой. Точка касания в вершине ромба — это точка, в которой либо $w_0=0$ либо $w_1=0$. То есть $L_1$-регуляризация склонна с большей вероятностью занулять коэффициенты линейной регрессии, чем $L_2$-регуляризация.\n",
    "\n",
    ">Величина диагонали ромба и радиуса окружности зависят от величины коэффициента регуляризации $\\alpha$: чем больше $\\alpha$, тем меньше ромб/окружность, а значит тем дальше псевдоминимум будет находиться от истинного минимума, и наоборот."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Практика. Полиноминальная регрессия и регуляция <a class=\"anchor\" id=8></a>\n",
    "\n",
    "[к содержанию](#0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ранее, в юните 5, мы смогли построить модель линейной регрессии, которая прогнозирует выработку газа на скважине. Для этого мы с помощью матрицы корреляций и рассуждений отобрали некоррелированные, значимые для предсказания признаки. Далее мы будем использовать именно их (см. задание 5.5).\n",
    "\n",
    "Мы хотим попробовать улучшить наш результат — метрику `MAPE`. Для этого воспользуемся моделью полиномиальной регрессии третьей степени. Однако теперь мы знаем, что полиномиальным моделям очень легко переобучиться под исходную выборку, поэтому для контроля качества модели мы будем использовать кросс-валидацию.\n",
    "\n",
    "**Задание 8.1**\n",
    "\n",
    "Сгенерируйте полиномиальные признаки третьего порядка на факторах, которые вы выбрали для обучения моделей. Для этого воспользуйтесь генератором полиномов `PolynomialFeatures` из библиотеки `sklearn`. Параметр `include_bias` установите в значение `False`.\n",
    "\n",
    "1. Сколько факторов у вас получилось после генерации полиномиальных признаков?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Well</th>\n",
       "      <th>Por</th>\n",
       "      <th>Perm</th>\n",
       "      <th>AI</th>\n",
       "      <th>Brittle</th>\n",
       "      <th>TOC</th>\n",
       "      <th>VR</th>\n",
       "      <th>Prod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>12.08</td>\n",
       "      <td>2.92</td>\n",
       "      <td>2.80</td>\n",
       "      <td>81.40</td>\n",
       "      <td>1.16</td>\n",
       "      <td>2.31</td>\n",
       "      <td>4165.196191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>12.38</td>\n",
       "      <td>3.53</td>\n",
       "      <td>3.22</td>\n",
       "      <td>46.17</td>\n",
       "      <td>0.89</td>\n",
       "      <td>1.88</td>\n",
       "      <td>3561.146205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Well    Por  Perm    AI  Brittle   TOC    VR         Prod\n",
       "0     1  12.08  2.92  2.80    81.40  1.16  2.31  4165.196191\n",
       "1     2  12.38  3.53  3.22    46.17  0.89  1.88  3561.146205"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('unconv.zip')\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Por</th>\n",
       "      <th>AI</th>\n",
       "      <th>Brittle</th>\n",
       "      <th>VR</th>\n",
       "      <th>Prod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.08</td>\n",
       "      <td>2.80</td>\n",
       "      <td>81.40</td>\n",
       "      <td>2.31</td>\n",
       "      <td>4165.196191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.38</td>\n",
       "      <td>3.22</td>\n",
       "      <td>46.17</td>\n",
       "      <td>1.88</td>\n",
       "      <td>3561.146205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Por    AI  Brittle    VR         Prod\n",
       "0  12.08  2.80    81.40  2.31  4165.196191\n",
       "1  12.38  3.22    46.17  1.88  3561.146205"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop(columns=['Well','Perm','TOC'])\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Кол-во полипризнаков : 34\n",
      "MAPE на тренировочных фолдах: 1.77 %\n",
      "MAPE на валидационных фолдах: 2.68 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "A = data.drop('Prod',axis=1)\n",
    "y=data['Prod']\n",
    "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "A_poly = poly.fit_transform(A)\n",
    "print('Кол-во полипризнаков :',A_poly.shape[1])\n",
    "model = LinearRegression()\n",
    "cv_results = cross_validate(model, A_poly, y, scoring='neg_mean_absolute_percentage_error', cv=5, return_train_score=True)\n",
    "print('MAPE на тренировочных фолдах: {:.2f} %'.format(-cv_results['train_score'].mean()* 100))\n",
    "print('MAPE на валидационных фолдах: {:.2f} %'.format(-cv_results['test_score'].mean() * 100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 8.2**\n",
    "\n",
    "Теперь попробуем воспользоваться линейной регрессией с регуляризацией. Для начала возьмём $L_1$-регуляризацию.\n",
    "\n",
    "Обучите модель `Lasso` из библиотеки `sklearn` на полученных полиномиальных факторах, предварительно стандартизировав факторы с помощью `StandardScaler`. Коэффициент регуляризации выставите равным 5.\n",
    "\n",
    "Оцените среднее значение метрики `MAPE`, используя кросс-валидацию на пяти фолдах.\n",
    "\n",
    "Чему равны средние значения метрики `MAPE` на тренировочных и валидационных фолдах? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE на тренировочных фолдах: 1.8\n",
      "MAPE на валидационных фолдах: 2.3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "data = pd.read_csv('unconv.zip')\n",
    "\n",
    "X = data.drop(['Prod', 'Perm', 'TOC', 'Well'], axis=1)\n",
    "y = data['Prod'].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X_poly = poly.fit_transform(X_scaled)\n",
    "\n",
    "lasso = Lasso(alpha=5)\n",
    "lasso.fit(X_poly, y)\n",
    "\n",
    "# оцениваем качество модели на кросс-валидации\n",
    "cv_results = cross_validate(lasso, X_poly, y, scoring='neg_mean_absolute_percentage_error', cv=5, return_train_score=True)\n",
    "print('MAPE на тренировочных фолдах: {:.1f}'.format(-cv_results['train_score'].mean() * 100))\n",
    "print('MAPE на валидационных фолдах: {:.1f}'.format(-cv_results['test_score'].mean() * 100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 8.3**\n",
    "\n",
    "Проделаем то же самое с $L_2$-регуляризацией.\n",
    "\n",
    "Обучите модель `Ridge` из библиотеки `sklearn` на полученных полиномиальных факторах, предварительно стандартизировав факторы с помощью `StandardScaler`. Коэффициент регуляризации выставите равным 1.\n",
    "\n",
    "Оцените среднее значение метрики `MAPE`, используя кросс-валидацию на пяти фолдах.\n",
    "\n",
    "Чему равны средние значения метрики `MAPE` на тренировочных и валидационных фолдах? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE на тренировочных фолдах: 1.8\n",
      "MAPE на валидационных фолдах: 2.7\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge(alpha=1)\n",
    "ridge.fit(X_poly, y)\n",
    "\n",
    "# оцениваем качество модели на кросс-валидации\n",
    "cv_results = cross_validate(ridge, X_poly, y, scoring='neg_mean_absolute_percentage_error', cv=5, return_train_score=True)\n",
    "print('MAPE на тренировочных фолдах: {:.1f}'.format(-cv_results['train_score'].mean() * 100))\n",
    "print('MAPE на валидационных фолдах: {:.1f}'.format(-cv_results['test_score'].mean() * 100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Итоги <a class=\"anchor\" id=9></a>\n",
    "\n",
    "[к содержанию](#0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ДОПОЛНИТЕЛЬНЫЕ МАТЕРИАЛЫ:\n",
    "\n",
    "[Ещё одно объяснение метода наименьших квадратов](http://www.mathprofi.ru/metod_naimenshih_kvadratov.html)\n",
    "\n",
    "[Более расширенное понятие регуляризации](https://neerc.ifmo.ru/wiki/index.php?title=Регуляризация) (включая регуляризацию через SVD-разложение)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
