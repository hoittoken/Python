{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MATH&ML-6 Математический анализ в контексте задачи оптимизации. Часть 3\n",
    "###  Содержание <a class=\"anchor\" id=0></a>\n",
    "\n",
    "- [1. О чём лекция](#2)\n",
    "- [2. Градиентный спуск: применения и классификации](#2)\n",
    "- [3. Метод Ньютона](#3)\n",
    "- [4. Квазиньютоновские методы](#4)\n",
    "- [5. Линейное программирование](#5)\n",
    "- [6. Практика: Линейное программирование](#6)\n",
    "- [7. Практика: Дополнительные методы оптимизации](#7)\n",
    "- [8. Итоги](#8)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. О чём лекция <a class=\"anchor\" id=1></a>\n",
    "\n",
    "[к содержанию](#0)\n",
    "\n",
    "* рассмотрим, какие вариации существуют у уже известного вам алгоритма градиентного спуска, и узнаем, в чём суть **обратного распространения ошибки**;\n",
    "\n",
    "* познакомимся с **методом Ньютона** и **квазиньютоновскими методами BFGS** и **L-BFGS**;\n",
    "\n",
    "* разберём область применения задач **линейного программирования** и попрактикуемся в их решении;\n",
    "\n",
    "* узнаем, что такое **метод отжига** и **метод координатного спуска**.\n",
    "\n",
    ">Из предыдущего модуля вы помните, что в каких-то методах оптимизации мы использовали лишь значение функции, где-то — считали градиент, а где-то — находили матрицы производных. Эти особенности дают возможность поделить все алгоритмы на **три группы**:\n",
    ">\n",
    ">* методы нулевого порядка (их работа основана на оценке значений самой целевой функции в разных точках);\n",
    ">\n",
    ">* методы первого порядка (при работе они используют первые производные в дополнение к информации о значении функции);\n",
    ">\n",
    ">* методы второго порядка (для них необходимо оценивать и значение функции, и значение градиента, и гессиан (матрицу Гессе)).\n",
    "\n",
    "**Примечание**. Иногда в литературе можно встретить термины «оракул первого порядка» или «оракул нулевого порядка». Так обозначают компоненты алгоритма, которые находят информацию на каждом шаге для метода соответствующего порядка.\n",
    "\n",
    "В данном модуле мы будем рассматривать методы разных порядков и практически в каждом юните будем практиковаться в их применении. В конце мы обобщим полученные знания и сравним, какие методы и в каких случаях показывают наилучшие результаты.\n",
    "\n",
    "→ Важно отметить, что некоторые алгоритмы оптимизации мы пока не сможем использовать по прямому назначению: мы опробуем их на известных вам моделях линейной или логистической регрессии или на функциях одной или нескольких переменных, но основная сфера их применения — это искусственные нейронные сети. Тем не менее, понимание всего пула алгоритмов поможет вам в будущем без проблем подбирать необходимый метод вне зависимости от того, задачу какой сложности вы решаете: минимизируете простейшую функцию или обучаете нейронную сеть со сложной архитектурой."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**Задание 1.1**\n",
    "\n",
    "Пусть прибыль в вашей компании выражается следующей функцией, которая зависит от параметра  — количества производимых товаров:\n",
    "\n",
    "$f(x)=-x^{4}+6 x^{3}-4 x^{2}+80$\n",
    "\n",
    "Найдите максимально возможную прибыль, которую вы можете получить, варьируя количество произведённых товаров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=0 => f=80 \n",
      "x=1/2 => f=1275/16 \n",
      "x=4 => f=144\n"
     ]
    }
   ],
   "source": [
    "from sympy import Symbol, Eq, diff, solve\n",
    "\n",
    "x = Symbol('x')\n",
    "fun = -x**4+6*x**3-4*x**2+80\n",
    "df = diff(fun)\n",
    "sol = solve(Eq(df,0))\n",
    "\n",
    "print(f'x={sol[0]} => f={fun.subs(x,sol[0])} \\nx={sol[1]} => f={fun.subs(x,sol[1])} \\nx={sol[2]} => f={fun.subs(x,sol[2])}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 1.4**\n",
    "\n",
    "Допустим, вы хотите произвести некоторое количество товара, которое зависит от часов работы двух ключевых сотрудников следующим образом:\n",
    "\n",
    "$f(x, y)=x^{2}+2 y^{2}$\n",
    "\n",
    "Однако вы можете оплатить этим сотрудникам не более 20 часов работы.\n",
    "\n",
    "Какое наибольшее количество товаров вы сможете произвести в таком случае?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Первый работник: 13 ч/часов\n",
      "Воторой работник: 7 ч/часов\n",
      "Кол-во товаров: 267 шт\n"
     ]
    }
   ],
   "source": [
    "from sympy import symbols, Eq, solve, diff\n",
    "\n",
    "x, y, z = symbols('x y z') \n",
    "f = x**2+2*y**2 + z*(x+y-20) # запишем функцию Лагранжа\n",
    "f_1 = x**2+2*y**2 # запишем исходную функцию\n",
    "\n",
    "f_x = diff(f,x) # найдём частные производные от ф-и Ларанжа\n",
    "\n",
    "f_y = diff(f,y)\n",
    "f_z = diff(f,z)\n",
    "\n",
    "sol = solve([Eq(f_x,0), Eq(f_y,0), Eq(f_z,0)], [x, y, z]) # разрешим систему уравнений собраную из частных производных\n",
    "print(f'Первый работник: {round(sol[x])} ч/часов\\nВоторой работник: {round(sol[y])} ч/часов')\n",
    "print(f'Кол-во товаров: {f_1.subs([(x,round(sol[x])),(y,round(sol[y]))])} шт')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Градиентный спуск: применения и классификации <a class=\"anchor\" id=2></a>\n",
    "\n",
    "[к содержанию](#0)\n",
    "\n",
    "Чтобы лучше понимать, какую роль играют методы оптимизации в DS и почему их так много, важно понимать основные **принципы работы нейронных сетей**. Также в современных статьях и обзорах разных методов оптимизации они практически всегда рассматриваются в контексте применения в нейронных сетях."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Процесс обучения человеческого мозга очень сложен, и наука пока не может дать достаточно подробный ответ на вопрос о том, как мы получаем и усваиваем знания, как принимаем решения. Однако той информации, которую уже удалось получить, оказалось достаточно, чтобы по аналогии создать модели искусственных нейронных сетей.\n",
    "\n",
    "Люди учатся через пробы и ошибки, через процесс **синаптической пластичности**. Точно так же, как связи в мозге укрепляются и формируются по мере того, как мы переживаем новые события, мы обучаем искусственные нейронные сети, вычисляя ошибки нейросетевых предсказаний и усиливая или ослабляя внутренние связи между нейронами на основе этих ошибок.\n",
    "\n",
    ">**Синаптическая пластичность** — это понятие, которое используется для описания того, как формируются и укрепляются нейронные связи после получения новой информации.\n",
    "Для лучшего понимания удобно воспринимать нейронную сеть как функцию, которая принимает входные данные для получения итогового прогноза. Переменными этой функции являются параметры, или веса, нейрона.\n",
    "\n",
    "Следовательно, ключевым моментом для решения задачи, которую мы ставим для нейронной сети, будет **корректировка значений весов** таким образом, чтобы они аппроксимировали или наилучшим образом представляли набор данных.\n",
    "\n",
    "На изображении ниже показана простая нейронная сеть, которая получает входные данные $(X_1, \\ X_2, \\  X_3, \\ X_n)$. Эти входные данные передаются нейронам в слое, содержащем веса $(W_1, \\ W_2, \\  W_3, \\ W_n)$. Входные данные и веса подвергаются операции умножения, и результат суммируется с помощью специальной функции, а функция активации регулирует конечный результат модели.\n",
    "\n",
    "<img src=m6_img1.png width=700>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того чтобы оценить, насколько эффективно работает наша нейронная сеть, необходим показатель оценки разницы между предсказанием нейронной сети и фактическим значением целевой функции, позволяющий корректировать параметры сети так, чтобы разница между прогнозом и реальностью была как можно меньше. В `Data Science` эту разницу часто называют **функцией стоимости**.\n",
    "\n",
    "Ниже представлена модель работы простейшей нейронной сети:\n",
    "\n",
    "<img src=m6_img2.png width=700>\n",
    "\n",
    "На этом изображении мы можем видеть модель простой нейронной сети из плотно связанных нейронов, которая классифицирует рукописные представления цифр 0, 1, 2, 3. Каждый нейрон в выходном слое соответствует цифре. Чем выше активация соединения с нейроном, тем выше вероятность, выдаваемая нейроном. Вероятность соответствует вероятности того, что цифра, переданная вперёд по сети, связана с активированным нейроном.\n",
    "\n",
    "Когда цифра 3 передаётся через сеть, мы ожидаем, что соединения (представленные на диаграмме стрелками), ответственные за классификацию этой цифры, будут иметь более высокую активацию, что приводит к более высокой вероятности для выходного нейрона, связанного с цифрой 3.\n",
    "\n",
    ">Если объяснить происходящее более простыми словами, то механизм работы примерно такой:\n",
    ">\n",
    ">1. На первом слое мы выделяем из цифры какие-то первичные признаки (округлости, палочки).\n",
    ">\n",
    ">2. На втором слое мы выделяем уже какие-то паттерны (фрагменты цифры).\n",
    ">\n",
    ">3. На третьем слое паттерны складываются в целую цифру и мы можем предсказать результат.\n",
    ">\n",
    ">Каждый раз элементы, более похожие на элементы цифры 3, дают более высокую активацию.\n",
    "\n",
    "За активацию нейрона отвечает несколько компонентов, и мы должны менять их в процессе обучения нашей нейронной сети, повышая качество её предсказания."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Допустим, мы используем уже хорошо известную вам среднеквадратичную ошибку как меру для оценки качества модели. В этом случае мы каждый раз вычисляем её, получаем результат и отправляем его обратно для коррекции весов в сети. Здесь как раз и появляется понятие **обратного распространения ошибки**.\n",
    "\n",
    "Обратное распространение (`backpropagation`) — это механизм, с помощью которого компоненты, влияющие на итоговый результат, итеративно корректируются для уменьшения функции стоимости.\n",
    "\n",
    "Важнейшим математическим процессом, связанным с обратным распространением, является вычисление производных. Операции обратного распространения вычисляют частную производную функции стоимости по отношению к весам и активациям предыдущего слоя, чтобы определить, какие значения влияют на градиент функции стоимости.\n",
    "\n",
    ">В процессе минимизации  функции ошибки мы постоянно вычисляем значение градиентов и таким образом приходим к локальному минимуму. На каждом этапе обучения нейронной сети её веса пересчитываются с помощью найденного градиента, причём скорость обучения (или, мы уже называли её ранее, темп обучения или шаг градиентного спуска) определяет коэффициент, с которым вносятся изменения в значения весов. Это повторяется на каждом шаге обучения нейронной сети. Нашей целью является постоянное приближение к локальному минимуму.\n",
    "\n",
    "Принцип обратного распространения ошибки заключается в том, что сначала мы устанавливаем какие-то случайные веса (параметры) для модели, находим итоговую ошибку и движемся по сети обратно, корректируя веса (для этого вычисляем частные производные, т. е. градиент).\n",
    "\n",
    "Данный процесс выглядит следующим образом:\n",
    "\n",
    "<img src=m6_gif1.gif>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Распространение ошибок и использование частных производных для корректировки весов происходит до тех пор, пока не будут скорректированы все параметры в сети вплоть до первого слоя.\n",
    "\n",
    ">Если вам хочется больше узнать про механизм обратного распространения ошибки, рекомендуем обратиться к [этой статье](https://brilliant.org/wiki/backpropagation/).\n",
    "\n",
    "Как вы можете видеть, даже в самом простом примере нейронной сети много переменных и действий, которые с ними происходят. Из-за этого ландшафт функции потерь для нейронных сетей становится очень сложным. К примеру, ландшафт для нейронной сети с 56 слоями может выглядеть так:\n",
    "\n",
    "<img src=m6_img3.png width=400>\n",
    "\n",
    "В предыдущем модуле мы без проблем решили задачу с помощью классического градиентного спуска, и результат совпал с итогом, полученным с помощью реализации алгоритма в стандартной библиотеке `Python`. Но функция потерь там была совершенно другой. Когда дело доходит до искусственных нейронных сетей, алгоритмы оптимизации сталкиваются с множеством проблем."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Основные проблемы при реализации градиентного спуска:\n",
    ">\n",
    ">* Классический градиентный спуск **склонен застревать** в точках локального минимума и даже в седловых точках, словом — везде, где градиент равен нулю. Это мешает найти глобальный минимум.\n",
    ">\n",
    ">* Обычно у оптимизируемой функции **очень сложный ландшафт**: где-то она совсем пологая, где-то более крутой обрыв. В таких ситуациях градиентный спуск показывает не лучшие результаты. Так происходит потому, что в алгоритме градиентного спуска фиксированный шаг, а нам в идеале хотелось бы его изменять в зависимости от формы функции прямо в процессе обучения.\n",
    ">\n",
    ">* Много проблем возникает из-за **темпа обучения**: при низком алгоритм сходится невероятно медленно, при быстром — «пролетает» мимо минимумов.\n",
    ">\n",
    ">* При обучении градиентного спуска координаты в некоторых измерениях могут редко изменяться, что приводит к плохой обобщающей способности алгоритма. Можно попытаться придать каждого признаку бόльшую важность, но в таком случае есть серьёзный **риск переобучить модель**.\n",
    "\n",
    "Тем не менее, нельзя отрицать, что градиентный спуск — невероятно эффективный и популярный алгоритм. Допустим, он прекрасно подходит для минимизации среднеквадратичной ошибки в случае решения задачи регрессии. Однако в силу его несовершенств, которые очень явно проявляются в сложных моделях (например, в нейронных сетях), были созданы некоторые его модификации, которые позволяют решать задачи с большей результативностью.\n",
    "\n",
    ">Обычно выделяют три основных вариации градиентного спуска:\n",
    ">\n",
    ">* `Batch Gradient Descent`;\n",
    ">\n",
    ">* `Stochastic Gradient Descent`;\n",
    ">\n",
    ">* `Mini-batch Gradient Descent`.\n",
    ">\n",
    "\n",
    "Далее мы рассмотрим все эти модификации и обсудим их различия. Важно понимать, что, несмотря на то что про их применение часто говорят именно в контексте нейронных сетей, они прекрасно подходят и для использования с обычными методами машинного обучения. К примеру, для стандартной линейной регрессии также подходит и обычный градиентный спуск."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BATCH GRADIENT DESCENT\n",
    "\n",
    "Первая вариация — это `Batch Gradient Descent`. По-русски её называют **пакетным градиентным спуском** (Пакетным его называют по той причине, что он использует всю выборку (весь пакет) на каждом шаге, для того чтобы получить результат.), или *ванильным градиентным спуском* (хотя англоязычную вариацию Vanilla Gradient Descent чаще не переводят). По сути, это и есть классический градиентный спуск, который мы с вами рассматривали в предыдущем модуле.\n",
    "\n",
    "<img src=m6_img4.png width=700>\n",
    "\n",
    "Часто в различных источниках шаг `Batch Gradient Descent` записывается в следующих обозначениях:\n",
    "\n",
    "$\\theta=\\theta-\\eta \\cdot \\nabla_{\\theta} J(\\theta)$\n",
    "\n",
    ">**Примечание**. На самом деле совершенно не важно, какими буквами выражать те или иные объекты, но мы будем использовать в этом модуле обозначения, которые часто встречаются в научных статьях и различной литературе."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь $\\theta$ — вектор с параметрами функции, $\\eta$ — шаг градиента, $\\nabla_{\\theta} J(\\theta)$ — градиент функции, найденный по её параметрам.\n",
    "\n",
    "Таким образом, на каждом шаге градиентный спуск находит направление наискорейшего убывания функции и движется чётко по нему. Поэтому для несложных функций он достаточно быстро сходится. Его обучение можно изобразить следующим образом:\n",
    "\n",
    "<img src=m6_img5.png width=700>\n",
    "\n",
    "Такой градиентный спуск достаточно хорошо работает, если мы рассматриваем выпуклые или относительно гладкие функции ошибки. Такие, к примеру, мы можем наблюдать у **линейной** или **логистической регрессии**. Мы обсуждали, что градиентный спуск не умеет находить глобальный минимум среди прочих, но, например, для выпуклой функции он может это сделать. Поэтому с выпуклыми функциями (допустим, со среднеквадратичной ошибкой для линейной регрессии) нам удобнее всего использовать именно его.\n",
    "\n",
    ">Но всё усложняется, когда мы хотим применить его при обучении нейронных сетей. Как уже говорилось, у таких моделей функция потерь имеет много локальных экстремумов, в каждом из которых градиентный спуск может застрять. Когда данных очень много, а оптимизируемая функция очень сложная, данный алгоритм применять затруднительно, так как поиск градиента по всем наблюдениям делает задачу очень затратной в плане вычислительных ресурсов."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STOCHASTIC GRADIENT DESCENT\n",
    "\n",
    "Представим, что мы реализуем градиентный спуск для набора данных объёмом 10 000 наблюдений и у нас десять переменных. Среднеквадратичную ошибку считаем по всем точкам, то есть для 10 000 наблюдений. Производную необходимо посчитать по каждому параметру, поэтому фактически за каждую итерацию мы будем выполнять не менее 100 000 вычислений. И если, допустим, у нас 1000 итераций, то нам нужно 100000*1000=100000000 вычислений. Это довольно много, поэтому градиентный спуск на сложных моделях и при использовании больших наборов данных работает крайне долго.\n",
    "\n",
    "Чтобы преодолеть эту проблему, придумали **стохастический градиентный спуск**. Слово *«стохастический»* можно воспринимать как синоним слова *«случайный»*. Где же при использовании градиентного спуска может возникнуть случайность? При выборе данных. При реализации стохастического спуска вычисляются градиенты не для всей выборки, а только для случайно выбранной единственной точки.\n",
    "\n",
    "<img src=m6_img6.png width=700>\n",
    "\n",
    "Это значительно сокращает вычислительные затраты.\n",
    "\n",
    "В виде формулы это можно записать следующим образом:\n",
    "\n",
    "$\\theta=\\theta-\\eta \\cdot \\nabla_{\\theta} J\\left(\\theta ; x^{(i)} ; y^{(i)}\\right)$\n",
    "\n",
    "На визуальном представлении ниже можно увидеть, что стохастический спуск создаёт много колебаний при сходимости. Это происходит как раз за счёт того, что берётся не вся выборка, а только один объект, и между объектами может быть достаточно большая разница. Чем меньше выборка, тем меньше стабильности при реализации.\n",
    "\n",
    "<img src=m6_img7.png width=700>\n",
    "\n",
    ">Стохастический градиентный спуск **очень часто используется в нейронных сетях** и сокращает время машинных вычислений, одновременно повышая сложность и производительность крупномасштабных задач."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MINI-BATCH GRADIENT DESCENT\n",
    "\n",
    "Третья вариация градиентного спуска — `Mini-batch Gradient Descent`. Также можно называть его **мини-пакетным градиентным спуском**. По сути, эта модификация сочетает в себе лучшее от классической реализации и стохастического варианта. На данный момент это наиболее популярная реализация градиентного спуска, которая используется в глубоком обучении (т. е. в обучении нейронных сетей).\n",
    "\n",
    "В ходе обучения модели с помощью мини-пакетного градиентного спуска обучающая выборка разбивается на пакеты (**батчи**), для которых рассчитывается ошибка модели и пересчитываются веса.\n",
    "\n",
    "<img src=m6_img8.png width=700>\n",
    "\n",
    ">То есть, с одной стороны, мы используем все преимущества обычного градиентного спуска, а с другой — уменьшаем сложность вычислений и повышаем их скорость по аналогии со стохастическим спуском. Кроме того, алгоритм работает ещё быстрее за счёт возможности применения векторизованных вычислений.\n",
    "\n",
    "Формализовать это можно следующим образом:\n",
    "\n",
    "$\\theta=\\theta-\\eta \\cdot \\nabla_{\\theta} J\\left(\\theta ; x^{(i: i+n)} ; y^{(i: i+n)}\\right)$\n",
    "\n",
    "Как показывает визуализация ниже, амплитуда колебаний при сходимости алгоритма больше, чем в классическом градиентном спуске, но меньше, чем в стохастическом:\n",
    "\n",
    "<img src=m6_img9.png width=700>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|BATCH GRADIENT DESCENT|STOCHASTIC GRADIENT DESCENT|MINI-BATCH GRADIENT DESCENT|\n",
    "| - | - | - |\n",
    "|Рассматриваются все обучающие данные|Рассматривает только один объект|Рассматривается подвыборка|\n",
    "|Затрачивает много времени на работу|Работает быстрее пакетного|Работает быстрее двух других|\n",
    "|Плавное обновление параметров модели|Сильные колебания в обновлении параметров модели|Колебания зависят от размера подвыборки (увеличиваются с уменьшением её объема)|"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 2.7**\n",
    "\n",
    "Давайте потренируемся применять стохастический градиентный спуск для решения задачи линейной регрессии. Мы уже рассмотрели его реализацию «с нуля», однако для решения практических задач можно использовать готовые библиотеки.\n",
    "\n",
    "Загрузите стандартный датасет об алмазах из библиотеки `Seaborn`:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделите выборку на обучающую и тестовую (объём тестовой возьмите равным 0.33), значение `random_state` должно быть равно 42.\n",
    "\n",
    "Теперь реализуйте алгоритм линейной регрессии со стохастическим градиентным спуском (класс `SGDRegressor`). Отберите с помощью `GridSearchCV` оптимальные параметры по следующей сетке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.23</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>E</td>\n",
       "      <td>SI2</td>\n",
       "      <td>61.5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.95</td>\n",
       "      <td>3.98</td>\n",
       "      <td>2.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>Premium</td>\n",
       "      <td>E</td>\n",
       "      <td>SI1</td>\n",
       "      <td>59.8</td>\n",
       "      <td>61.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.89</td>\n",
       "      <td>3.84</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.23</td>\n",
       "      <td>Good</td>\n",
       "      <td>E</td>\n",
       "      <td>VS1</td>\n",
       "      <td>56.9</td>\n",
       "      <td>65.0</td>\n",
       "      <td>327</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.07</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.29</td>\n",
       "      <td>Premium</td>\n",
       "      <td>I</td>\n",
       "      <td>VS2</td>\n",
       "      <td>62.4</td>\n",
       "      <td>58.0</td>\n",
       "      <td>334</td>\n",
       "      <td>4.20</td>\n",
       "      <td>4.23</td>\n",
       "      <td>2.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.31</td>\n",
       "      <td>Good</td>\n",
       "      <td>J</td>\n",
       "      <td>SI2</td>\n",
       "      <td>63.3</td>\n",
       "      <td>58.0</td>\n",
       "      <td>335</td>\n",
       "      <td>4.34</td>\n",
       "      <td>4.35</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   carat      cut color clarity  depth  table  price     x     y     z\n",
       "0   0.23    Ideal     E     SI2   61.5   55.0    326  3.95  3.98  2.43\n",
       "1   0.21  Premium     E     SI1   59.8   61.0    326  3.89  3.84  2.31\n",
       "2   0.23     Good     E     VS1   56.9   65.0    327  4.05  4.07  2.31\n",
       "3   0.29  Premium     I     VS2   62.4   58.0    334  4.20  4.23  2.63\n",
       "4   0.31     Good     J     SI2   63.3   58.0    335  4.34  4.35  2.75"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('diamonds.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.001, 'eta0': 0.001, 'l1_ratio': 0.0, 'learning_rate': 'constant', 'loss': 'squared_error', 'max_iter': 21, 'penalty': 'elasticnet', 'random_state': 42}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.044"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('diamonds.csv')\n",
    "\n",
    "df.drop(['depth', 'table', 'x', 'y', 'z'], axis=1, inplace=True)\n",
    "df = pd.get_dummies(df, drop_first=True)\n",
    "\n",
    "df['carat'] = np.log(1+df['carat'])\n",
    "df['price'] = np.log(1+df['price'])\n",
    "\n",
    "X_cols = [col for col in df.columns if col!='price']\n",
    "X = df[X_cols]\n",
    "y = df['price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "parameters = {\n",
    "    \"loss\": [\"squared_error\", \"epsilon_insensitive\"],\n",
    "    \"penalty\": [\"elasticnet\"],\n",
    "    \"alpha\": np.logspace(-3, 3, 15),\n",
    "    \"l1_ratio\": np.linspace(0, 1, 11),\n",
    "    \"max_iter\": np.logspace(0, 3, 10).astype(int),\n",
    "    \"random_state\": [42],\n",
    "    \"learning_rate\": [\"constant\"],\n",
    "    \"eta0\": np.logspace(-4, -1, 4)\n",
    "}\n",
    "\n",
    "sgd = SGDRegressor(random_state=42)\n",
    "sgd_cv = GridSearchCV(estimator=sgd, param_grid=parameters, n_jobs=-1)\n",
    "sgd_cv.fit(X_train, y_train)\n",
    "\n",
    "print(sgd_cv.best_params_)\n",
    "\n",
    "sgd = SGDRegressor(**sgd_cv.best_params_)\n",
    "\n",
    "sgd.fit(X_train, y_train)\n",
    "sgd.score(X_train, y_train) # r2\n",
    "ls = sgd.predict(X_test)\n",
    "\n",
    "round(mean_squared_error(y_test, ls), 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *БОНУС: АЛГОРИТМЫ, ОСНОВАННЫЕ НА ГРАДИЕНТНОМ СПУСКЕ\n",
    "\n",
    "Градиентный спуск настолько популярен и хорошо применим для решения различных задач, что послужил основой множества дополнительных методов. Поговорим про некоторые из них.\n",
    "\n",
    "Иногда в данных присутствуют очень редко встречающиеся входные параметры.\n",
    "\n",
    ">Например, если мы классифицируем письма на «Спам» и «Не спам», таким параметром может быть очень специфическое слово, которое встречается в спаме намного реже других слов-индикаторов. Или, если мы говорим о распознавании изображений, это может быть какая-то очень редкая характеристика объекта.\n",
    "\n",
    "В таком случае нам хотелось бы иметь для каждого параметра свою скорость обучения: чтобы для часто встречающихся она была низкой (для более точной настройки), а для совсем редких — высокой (это повысит скорость сходимости). То есть нам очень важно уметь обновлять параметры модели, учитывая то, насколько типичные и значимые признаки они кодируют.\n",
    "\n",
    "Решение этой задачи предложено в рамках алгоритма `AdaGrad` (его название обозначает, что это адаптированный градиентный спуск). В нём обновления происходят по следующему принципу:\n",
    "\n",
    "$G_t = G_t + g^2_t$\n",
    "\n",
    "$\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{G_t + \\epsilon}} g_t$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь мы храним сумму квадратов градиентов для каждого параметра. Таким образом, параметры, которые сильно обновляются каждый раз, начинают обновляться слабее. Скорость обучения в таком алгоритме будет постоянно затухать. Мы будем начинать с больших шагов, и с приближением к точке минимума шаги будут уменьшаться — это улучшит скорость сходимости.\n",
    "\n",
    ">Данный алгоритм достаточно популярен и работает лучше стохастического градиентного спуска. Его использует и компания Google в своих алгоритмах классификации изображений.\n",
    "\n",
    "Однако снижение скорости обучения в `AdaGrad` иногда происходит слишком радикально, и она практически обнуляется. Чтобы решить эту проблему, были созданы алгоритмы `RMSProp`, `AdaDelta`, `Adam` и некоторые другие.\n",
    "\n",
    "Если вам интересно подробнее узнать о перечисленных алгоритмах и окунуться в процесс оптимизации нейронных сетей, рекомендуем обратиться к следующим статьям:\n",
    "\n",
    "* [\"An overview of gradient descent optimization algorithms\"](https://ruder.io/optimizing-gradient-descent/index.html#adagrad)\n",
    "\n",
    "* [«Методы оптимизации нейронных сетей»](https://habr.com/ru/post/318970/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Метод Ньютона <a class=\"anchor\" id=3></a>\n",
    "\n",
    "[к содержанию](#0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод Ньютона используется во многих алгоритмах машинного обучения. Часто в литературе его сравнивают с градиентным спуском, так как два этих алгоритма очень популярны. Вы уже сталкивались с методом Ньютона, но не знали об этом.\n",
    "\n",
    "В [документации](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) для функции `LogisticRegression` из библиотеки `scikit-learn` представлено пять вариантов алгоритмов оптимизации, которые можно использовать при обучении модели:\n",
    "\n",
    "* 'newton-cg';\n",
    "\n",
    "* 'lbfgs';\n",
    "\n",
    "* 'liblinear';\n",
    "\n",
    "*  'sag';\n",
    "\n",
    "*  'saga'.\n",
    "\n",
    "Последние два являются вариациями стохастического градиентного спуска (а значит вам уже понятен принцип их работы), а с первыми тремя нам только предстоит познакомиться. В этом юните мы рассмотрим алгоритм `'newton-cg'`, в следующем — `'lbfgs'`, а в седьмом юните — `'liblinear'`. Вы будете понимать суть всех методов, представленных в самой популярной библиотеке для машинного обучения, и выбирать подходящий, исходя из особенностей поставленной задачи.\n",
    "\n",
    "Начнём с метода Ньютона. Этот алгоритм работает быстрее, чем градиентный спуск, и тратит меньше времени для достижения минимума, однако у него есть и определённые недостатки, о которых мы поговорим позже.\n",
    "\n",
    "Метод Ньютона изначально появился как метод решения уравнений вида $f(x)=0$.\n",
    "\n",
    "Проиллюстрируем принцип его работы геометрически. Пусть у нас есть график некоторой функции. Проведём к нему касательную в точке $x_n$.\n",
    "\n",
    "<img src=m6_img10.png>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тогда эта касательная имеет наклон, равный $f'(x_n)$, и проходит через точку $x_n, f(x_n)$. В таком случае мы можем сказать, что уравнение этой касательной: $y = f'(x_n) (x - x_n) + f(x_n)$.\n",
    "\n",
    "Так как нам необходимо решить уравнение, то нужно попасть в такую точку $x_{n+1}$, чтобы в ней значение координаты по оси ординат было нулевым, то есть в точку с координатами $x =  x_{n+1}$ и $y=0$.\n",
    "\n",
    "Подставим это в наше уравнение касательной:\n",
    "\n",
    "$y=f^{\\prime}\\left(x_{n}\\right)\\left(x-x_{n}\\right)+f\\left(x_{n}\\right)$\n",
    "\n",
    "$x=x_{n+1}, y=0$\n",
    "\n",
    "$0=f^{\\prime}\\left(x_{n}\\right) \\cdot\\left(x_{n+1}-x_{n}\\right)+f\\left(x_{n}\\right)$\n",
    "\n",
    "$f^{\\prime}\\left(x_{n}\\right) \\cdot\\left(x_{n+1}-x_{n}\\right)=-f\\left(x_{n}\\right)$\n",
    "\n",
    "$x_{n+1}-x_{n}=-\\frac{f\\left(x_{n}\\right)}{f^{\\prime}\\left(x_{n}\\right)}$\n",
    "\n",
    "$x_{n+1}=x_{n}{-\\frac{f\\left(x_{n}\\right)}{f^{\\prime}\\left(x_{n}\\right)}}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно посмотреть на это и в анимации: \n",
    "\n",
    "<img src=m6_gif2.gif>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы видим, как для $x$ вычисляется $f(x)$, строится касательная, и в точке пересечения касательной с осью $Ox$ строится новая точка, к которой также строится касательная, и так далее. Математически доказано, что таким образом $x_i$ приближается к значению, где $f(x)=0$. \n",
    "\n",
    "Формально первый  шаг этого алгоритма мы можем записать следующим образом:\n",
    "\n",
    "$x_1 = x_0 - \\frac{f(x_0)}{f'(x_0)}$\n",
    "\n",
    "Все остальные шаги можно обобщить с помощью следующей зависимости:\n",
    "\n",
    "$x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}$\n",
    "\n",
    "Шаги могут повторяться сколько угодно раз до достижения необходимой точности.\n",
    "\n",
    "Давайте посмотрим, как с использованием этого метода можно решить уравнение ↓"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Пример № 1**\n",
    "\n",
    "Найти корень уравнения $x^2 - 4x - 7 = 0$, который находится рядом с точкой $x=5$, с точностью до тысячных.\n",
    "\n",
    "Функция: $f(x) = x^2 - 4x - 7 = 0$\n",
    "\n",
    "Начальная точка: $x_0 = 5$\n",
    "\n",
    "Производная для функции: $f'(x) = 2x - 4$\n",
    "\n",
    "Начнём поочерёдно совершать шаги и переходить в следующие точки, используя формулу, которую мы рассмотрели ранее:\n",
    "\n",
    "$x_1 = 5 - \\frac{5^2 - 4 \\times 5 - 7}{2 \\times 5 - 4} = 5 - \\frac{(-2)}{6}  = \\frac{16}{3} \\approx 5.33333$\n",
    "\n",
    "$x_2 = \\frac{16}{3} - \\frac{(\\frac{16}{3})^2 - 4 (\\frac{16}{3}) - 7}{2 (\\frac{16}{3}) - 4} = \\frac{16}{3} - \\frac{\\frac{1}{9}}{\\frac{20}{3}} = \\frac{16}{3} - \\frac{1}{60} = \\frac{319}{60} \\approx 5.31667$\n",
    "\n",
    "$x_3 = \\frac{319}{60} - \\frac{(\\frac{319}{60})^2 - 4 (\\frac{319}{60}) - 7}{2 (\\frac{319}{60}) - 4} = \\frac{319}{60} - \\frac{\\frac{1}{3600}}{\\frac{398}{60}} \\approx 5.31662$\n",
    "\n",
    "Мы видим, что в последних двух точках мы уже находимся примерно **в одном и том же месте**. На всякий случай проверим ещё одну:\n",
    "\n",
    "$x_4 = 5.31662 - \\frac{(5.31662)^2 - 4 (5.31662) - 7}{2 (5.31662) - 4} = 5.31662$\n",
    "\n",
    "Действительно, мы нашли точку с точностью до тысячных, в которой останемся — это и будет нашим ответом."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Пример № 2**\n",
    "\n",
    "Найти корни сложного полинома $f(x) = 6x^5 - 5x^4 - 4x^3 + 3x^2$.\n",
    "\n",
    "Как мы знаем, к сожалению, для полинома пятой степени нет формулы поиска корней, поэтому будем использовать численные методы. В этих случаях приходится прибегать к числовому линейному приближению.\n",
    "\n",
    "Ниже представлен график нашего полинома. У него три корня: в точках 0, 1 и где-то между ними. Как найти третий корень?\n",
    "\n",
    "<img src=m6_img11.png width=400>\n",
    "\n",
    "В методе Ньютона мы берём случайную точку $x_0$, а затем проводим касательную в ней. Точка $x_1$, где эта касательная пересекает ось абсцисс, станет нашим следующим предположением. Так что теперь мы  строим уже касательную в этой точке, и так далее . Мы продолжаем до тех пор, пока не достигнем необходимой точности. В целом, мы можем сделать приближение настолько близким к нулю, насколько хотим."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 3.1**\n",
    "\n",
    "Найдите третий корень полинома $f(x) = 6x^5 - 5x^4 - 4x^3 + 3x^2$, взяв за точку старта $0.7$. Введите получившееся значение **с точностью до трёх знаков после точки-разделителя**.\n",
    "\n",
    "Попробуйте реализовать алгоритм с использованием Python на основе алгоритма градиентного спуска, изученного в предыдущем модуле."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "x= 0.700, a_0= 0.700000000000000, a_1 0.5\n",
      "x= 0.630, a_0= 0.629633507853403, a_1 0.700000000000000\n",
      "x= 0.629, a_0= 0.628668078167331, a_1 0.629633507853403\n",
      "x= 0.629, a_0= 0.628666978777900, a_1 0.628668078167331\n",
      "x= 0.629, a_0= 0.628666978776461, a_1 0.628666978777900\n",
      "stop\n"
     ]
    }
   ],
   "source": [
    "from sympy import Symbol, Eq, diff, solve\n",
    "\n",
    "x = Symbol('x')\n",
    "f = 6*x**5 - 5*x**4 - 4*x**3 + 3*x**2\n",
    "print(len(solve(Eq(f,0))))\n",
    "df = f.diff(x)\n",
    "\n",
    "a_0 = 0.5\n",
    "a_1 = 0\n",
    "a_new = 0\n",
    "i = 0\n",
    "while True:\n",
    "    if i > 10:\n",
    "        break\n",
    "    elif round(a_0,7) == round(a_1,7):\n",
    "        print('stop')\n",
    "        break\n",
    "    a_new = a_0 - (f.subs(x,a_0)/df.subs(x,a_0))\n",
    "    a_1 = a_0\n",
    "    i += 1\n",
    "    a_0 = a_new\n",
    "    \n",
    "    print(f'x= {round(a_new, 3)}, a_0= {a_0}, a_1 {a_1}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлично, мы научились искать приближённые значения для корней уравнения. Но как же это поможет нам найти **минимум** или **максимум** для функции?\n",
    "\n",
    "Дело в том, что в задаче оптимизации можно решать не $f(x) = 0$, а $f'(x) = 0$ — тогда мы найдём потенциальные точки экстремума.\n",
    "\n",
    "В многомерном случае по аналогичным рассуждениям производная превращается в градиент, а вторая производная превращается в гессиан (матрица вторых производных или, как мы её называли в предыдущем модуле, матрица Гессе). Поэтому в формуле появится обратная матрица.\n",
    "\n",
    "Для многомерного случая формула выглядит следующим образом:\n",
    "\n",
    "$x^{(n+1)} = x^{(n)} - \\left [Hf(x^{(n)})  \\right ]^{-1} \\nabla f(x^{(n)})$\n",
    "\n",
    "Можно заметить, что эта формула совпадает с формулой для градиентного спуска, но вместо умножения на `learning rate` (темп обучения) используется умножение на обратную матрицу к гессиану. Благодаря этому функция может сходиться за меньшее количество итераций, так как мы учитываем информацию о выпуклости функции через гессиан. Можно увидеть это на иллюстрации работы двух методов, где метод Ньютона явно сходится быстрее:\n",
    "\n",
    "<img src=m6_img12.png width=900>\n",
    "\n",
    "Метод Ньютона, если считать в количестве итераций, в многомерном случае (с гессианом) работает быстрее градиентного спуска."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оптимизировать функцию $f(x) = x^3 - 3x^2 -45x  + 40$\n",
    "\n",
    "Находим производную функции: $f^{\\prime}=3 x^{2} - 6x - 45$\n",
    "\n",
    "Находим вторую производную: $f^{\\prime \\prime}=6x - 6$\n",
    "\n",
    "Сразу определим их в `Python`, чтобы можно было параллельно решить задачу и с помощью программирования:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func1(x):\n",
    "    return 3*x**2 - 6*x -45\n",
    "def func2(x):\n",
    "    return 6*x - 6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь необходимо взять какую-нибудь изначальную точку. Например, пусть это будет точка $x=42$. Также нам необходима точность — её возьмем равной $0.0001$. На каждом шаге будем переходить в следующую точку по уже упомянутой выше формуле:\n",
    "\n",
    "$x^{(n+1)}=x^{(n)}-\\frac{f^{\\prime}\\left(x^{(n)}\\right)}{f^{\\prime \\prime}\\left(x^{(n)}\\right)}$\n",
    "\n",
    "Например, в нашем случае следующая после $42$ точка будет рассчитываться следующим образом:\n",
    "\n",
    "$x_{2}=42-\\frac{f^{\\prime}\\left(x_{1}\\right)}{f^{\\prime \\prime}\\left(x_{2}\\right)}$\n",
    "\n",
    "$f^{\\prime}\\left(x_{1}\\right)=3 x^{2}-6 x-45 \\mid _{x_{1}=42}\\;= 3 \\cdot 42^{2}-6 \\cdot 42-45=4995$\n",
    "\n",
    "$f^{\\prime \\prime}\\left(x_{1}\\right)=6 x-6 \\mid _{x_{1}=42} \\; = \\;6.42-6=246$\n",
    "\n",
    "$x_{2}=42-\\frac{4995}{246} \\approx 42-20.305=21.695$\n",
    "\n",
    "Третья точка будет вычисляться по аналогичному принципу:\n",
    "\n",
    "$x_3 = 21.695 - \\frac{f'(21.695)}{f''(21.695)}$\n",
    "\n",
    "Но, к счастью, нам совсем не обязательно высчитывать всё вручную — воспользуемся Python и распишем наш алгоритм:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.695121951219512\n",
      "11.734125501243229\n",
      "7.1123493600499685\n",
      "5.365000391507974\n",
      "5.015260627016227\n",
      "5.000029000201801\n",
      "5.000000000105126\n",
      "5.000000000000001\n"
     ]
    }
   ],
   "source": [
    "initial_value = 42\n",
    "iter_count = 0\n",
    "x_curr = initial_value\n",
    "epsilon = 0.0001\n",
    "f = func1(x_curr)\n",
    "\n",
    "def func1(x):\n",
    "    return 3*x**2 - 6*x -45\n",
    "def func2(x):\n",
    "    return 6*x - 6\n",
    "\n",
    "while (abs(f) > epsilon):\n",
    "    f = func1(x_curr)\n",
    "    f_prime = func2(x_curr)\n",
    "    x_curr = x_curr - (f)/(f_prime)\n",
    "    iter_count += 1\n",
    "    print(x_curr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно объединить всё в одну функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def newtons_method(f, der, eps, init):\n",
    "    iter_count = 0\n",
    "    x_curr = init\n",
    "    f = f(x_curr)\n",
    "    while (abs(f) > eps):\n",
    "        f = f(x_curr)\n",
    "        f_der = der(x_curr)\n",
    "        x_curr = x_curr - (f)/(f_prime)\n",
    "        iter_count += 1\n",
    "    return x_curr\n",
    " \n",
    "from scipy.optimize import newton\n",
    "newton(func=func1,x0=50,fprime=func2, tol=0.0001)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Достоинства метода Ньютона\n",
    "\n",
    "1. Если мы минимизируем квадратичную функцию, то с помощью метода Ньютона можно попасть в минимум целевой функции за один шаг.\n",
    "\n",
    "2. Также этот алгоритм сходится за один шаг, если в качестве минимизируемой функции выступает функция из класса поверхностей вращения (т. е. такая, у которой есть симметрия).\n",
    "\n",
    "3. Для несимметричной функции метод не может обеспечить сходимость, однако скорость сходимости  всё равно превышает скорость методов, основанных на градиентном спуске.\n",
    "\n",
    "## Недостатки метода Ньютона\n",
    "\n",
    "1. Этот метод **очень чувствителен к изначальным условиям**.\n",
    "\n",
    "При использовании градиентного спуска мы всегда гарантированно движемся по антиградиенту в сторону минимума. В методе Ньютона происходит подгонка параболоида к локальной кривизне, и затем алгоритм движется к неподвижной точке данного параболоида. Из-за этого мы можем попасть в максимум или седловую точку. Особенно ярко это видно на невыпуклых функциях с большим количеством переменных, так как у таких функций седловые точки встречаются намного чаще экстремумов.\n",
    "\n",
    ">Поэтому здесь необходимо обозначить ограничение: метод Ньютона стоит применять только для задач, в которых **целевая функция выпуклая**.\n",
    "\n",
    "Впрочем, это не является проблемой. В линейной регрессии или при решении задачи классификации с помощью метода опорных векторов или логистической регрессии мы как раз ищем минимум у выпуклой целевой функции, то есть данный алгоритм подходит нам во многих случаях.\n",
    "\n",
    "2. Также метод Ньютона может быть **затратным с точки зрения вычислительной сложности**, так как требует вычисления не только градиента, но и гессиана и обратного гессиана (при делении на матрицу необходимо искать обратную матрицу).\n",
    "\n",
    ">Если у задачи много параметров, то расходы на память и время вычислений становятся астрономическими. Например, при наличии 50 параметров нужно вычислять более 1000 значений на каждом шаге, а затем предстоит ещё более 500 операций нахождения обратной матрицы. Однако метод всё равно используют, так как выгода от быстрой сходимости перевешивает затраты на вычисления.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Несмотря на его ограниченное практическое применение, метод Ньютона по-прежнему представляет большую ценность. Он имеет большое преимущество перед градиентным спуском **в силу своей быстроты и отсутствия необходимости в настройке гиперпараметра шага** (мы помним, что в градиентном спуске выбор шага — довольно непростая задача, а здесь можно обойтись без этого). Причём преимущество в быстроте очень ощутимое: в сравнении на реальных данных метод Ньютона находит решение задачи за 3 итерации, а градиентный спуск — за 489. То есть мы сильно выигрываем в скорости сходимости, а для анализа данных это очень важно, ведь экономия времени и вычислительных ресурсов позволяет решать задачи быстрее.\n",
    "\n",
    "?Мы увидели, какой эффективной может быть оптимизация второго порядка при правильном использовании. Но что, если бы мы могли каким-то образом использовать эффективность, полученную при использовании производных второго порядка, но при этом избежать вычислительных затрат на вычисление обратного гессиана? Другими словами — можем ли мы создать алгоритм, который будет своего рода **гибридом между градиентным спуском и методом Ньютона**, где мы сможем получать более быструю сходимость, чем градиентный спуск, но меньшие вычислительные затраты на каждую итерацию, чем в методе Ньютона?\n",
    "\n",
    "Оказывается, такой алгоритм существует. Точнее, целый класс таких методов оптимизации, называемых **квазиньютоновскими методами**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 3.6**\n",
    "\n",
    "Дана функция $f(x) = x^3 - 72x - 220$. Найдите решение уравнения $f(x)=0$ для поиска корня в окрестностях точки $x_0=12$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.21111111111111\n",
      "9.756461564762278\n",
      "9.727252176332618\n",
      "9.72713442131889\n",
      "9.727134419408875\n"
     ]
    }
   ],
   "source": [
    "initial_value = 12\n",
    "iter_count = 0\n",
    "x_curr = initial_value\n",
    "epsilon = 0.0001\n",
    "f = func1(x_curr)\n",
    "\n",
    "def func1(x):\n",
    "    return x**3 - 72*x - 220\n",
    "def func2(x):\n",
    "    return 3*x**2 - 72\n",
    "\n",
    "while (abs(f) > epsilon):\n",
    "    f = func1(x_curr)\n",
    "    f_prime = func2(x_curr)\n",
    "    x_curr = x_curr - (f)/(f_prime)\n",
    "    iter_count += 1\n",
    "    print(x_curr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 3.7**\n",
    "\n",
    "Найдите положительный корень для уравнения $x^2 + 9x - 5 = 0$. В качестве стартовой точки возьмите $x_0 = 2.2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.139130434782609\n",
      "-0.7211696705674668\n",
      "-0.5286253887576147\n",
      "-0.5249391626429637\n",
      "-0.524937810560627\n"
     ]
    }
   ],
   "source": [
    "def func1(x):\n",
    "    return x**2 - 9*x - 5\n",
    "def func2(x):\n",
    "    return 2*x - 9\n",
    "\n",
    "initial_value = 2.2\n",
    "iter_count = 0\n",
    "x_curr = initial_value\n",
    "epsilon = 0.0001\n",
    "f = func1(x_curr)\n",
    "\n",
    "while (abs(f) > epsilon):\n",
    "    f = func1(x_curr)\n",
    "    f_prime = func2(x_curr)\n",
    "    x_curr = x_curr - (f)/(f_prime)\n",
    "    iter_count += 1\n",
    "    print(x_curr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 3.9**\n",
    "\n",
    "С помощью метода Ньютона найдите точку минимума для функции $f(x) = 8x^3 - 2x^2 - 450$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.916719200750909"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_value = 42\n",
    "iter_count = 0\n",
    "x_curr = initial_value\n",
    "epsilon = 0.0001\n",
    "f = func1(x_curr)\n",
    "\n",
    "def func1(x):\n",
    "    return 8*x**3-2*x**2-450\n",
    "def func2(x):\n",
    "    return 24*x**2-4*x\n",
    "\n",
    "from scipy.optimize import newton\n",
    "newton(func=func1,x0=42,fprime=func2, tol=0.0001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Квазиньютоновские методы <a class=\"anchor\" id=4></a>\n",
    "\n",
    "[к содержанию](#0)\n",
    "\n",
    "В отличие от градиентного спуска, метод Ньютона использует на каждой итерации не только **градиент**, но и **матрицу Гессе**. Это обеспечивает более быструю сходимость к минимуму, но в то же время это приводит к **слишком большим вычислительным затратам**. В данном юните мы рассмотрим класс методов, в которых решается  эта проблема, — класс **квазиньютоновских методов**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напомним, что в методе Ньютона мы обновляем точку на каждой итерации в соответствии со следующим правилом:\n",
    "\n",
    "$\\mathbf{x}_{k+1}=\\mathbf{x}_{k}-\\left[H\\left(\\mathbf{x}_{k}\\right)\\right]^{-1} \\nabla f\\left(\\mathbf{x}_{k}\\right)$ \n",
    "\n",
    "где $H=\\begin{pmatrix} \\frac{d^2f}{dx^2_1} & \\frac{d^2f}{dx_2dx_1} & \\dots & \\frac{d^2f}{dx_1dx_n} \\\\ \\\\ \\frac{d^2f}{dx_2dx_1} & \\frac{d^2f}{dx^2_1} & \\dots & \\frac{d^2f}{dx_2dx_n} \\\\ \\\\ \\dots & \\dots & \\dots & \\dots  \\\\ \\\\ \\frac{d^2f}{dx_ndx_1} & \\frac{d^2f}{dx_ndx_2} & \\dots & \\frac{d^2f}{dx^2_n}  \\end{pmatrix}$ - Гессиан (матрица Гессе)\n",
    "\n",
    "$\\nabla f=\\begin{pmatrix} \\frac{df}{dx_1} \\\\ \\\\ \\frac{df}{dx_2} \\\\  \\dots \\\\ \\\\ \\frac{df}{dx_n} \\end{pmatrix}$ - Градиент\n",
    "\n",
    "На каждом шаге здесь вычисляется гессиан, а также его обратная матрица.\n",
    "\n",
    ">В квазиньютоновских методах вместо вычисления гессиана мы просто аппроксимируем его матрицей, которая обновляется от итерации к итерации с использованием информации, вычисленной на предыдущих шагах. Так как вместо вычисления большого количества новых величин мы использует найденные ранее значения, квазиньютоновский алгоритм тратит гораздо меньше времени и вычислительных ресурсов.\n",
    "\n",
    "Формально это описывается следующим образом:\n",
    "\n",
    "$x_{k+1} = x_k - H_k \\nabla f(x_k)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном случае вместо обратного гессиана появляется матрица $H_k$, которая строится таким образом, чтобы максимально точно аппроксимировать настоящий обратный гессиан.\n",
    "\n",
    "Математически это записывается так:\n",
    "\n",
    "$H_k - \\left [\\nabla^2 f(x_k) \\right ]^{-1} \\to 0 \\ при \\ k \\to \\infty$\n",
    "\n",
    "Здесь имеется в виду, что разница между матрицей в квазиньютоновском методе и обратным гессианом **стремится к нулю**.\n",
    "\n",
    ">Эта матрица обновляется на каждом шаге, и для этого существуют разные способы. Для каждого из способов есть своя модификация квазиньютоновского метода. Эти способы объединены ограничением: процесс обновления матрицы должен быть достаточно эффективным и не должен требовать вычислений гессиана. То есть, по сути, на каждом шаге мы должны получать информацию о гессиане, не находя непосредственно сам гессиан.\n",
    "\n",
    "Если вас интересует математическая сторона обновления и аппроксимации матрицы, прочитайте [эту статью](http://www.machinelearning.ru/wiki/images/6/65/MOMO17_Seminar6.pdf). В силу того, что понимание этой части метода требует очень серьёзной математической подготовки, мы опустим её. Однако можем заверить вас, что для успешного использования алгоритма и его понимания знание всех математических выводов не требуется."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Три самые популярные схемы аппроксимации:\n",
    "\n",
    "* симметричная коррекция ранга 1 (`SR1`);\n",
    "\n",
    "* схема Дэвидона — Флетчера — Пауэлла (`DFP`);\n",
    "\n",
    "* схема Бройдена — Флетчера — Гольдфарба — Шанно (`BFGS`).\n",
    "\n",
    "Последняя схема (`BFGS`) самая известная, стабильная и считается наиболее эффективной. На ней мы и остановимся. Своё название она получила из первых букв фамилий создателей и исследователей данной схемы: Чарли Джорджа Бройдена, Роджера Флетчера, Дональда Гольдфарба и Дэвида Шанно.\n",
    "\n",
    "### У этой схемы есть две известных вариации:\n",
    "\n",
    "* `L-BFGS`\n",
    "\n",
    "* `L-BFGS-B`\n",
    "\n",
    "Обе этих вариации необходимы в случае большого количества переменных для экономии памяти (так как во время их реализации хранится ограниченное количество информации). По сути, они работают одинаково, и `L-BFGS-B` является лишь улучшенной версией `L-BFGS` для работы с ограничениями.\n",
    "\n",
    "Метод `BFGS` очень устойчив и на данный момент считается одним из наиболее эффективных. Поэтому, если, например, применить функцию `optimize` без указания метода в библиотеке `SciPy`, то по умолчанию будет использоваться именно `BFGS` либо одна из его модификаций, указанных выше. Также данный метод используется в библиотеке `sklearn` при решении задачи логистической регрессии."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Алгоритм `BFGS`\n",
    "\n",
    "1. Реализация алгоритма начинается с того, что мы задаём начальную точку $x_0$, выбираем точность алгоритма, а также изначальную аппроксимацию для обратного гессиана функции. \n",
    "\n",
    "Здесь как раз таится главная **проблема**: нет никакого универсального рецепта для выбора этого приближения. Можно поступить по-разному: использовать гессиан, вычисленный в изначальной точке, взять единичную матрицу (такая настройка стоит по умолчанию в некоторых функциях библиотек `Python`) или другую матрицу, если она невырождена и хорошо обусловлена.\n",
    "\n",
    "2. Когда мы определили, откуда будем начинать, необходимо понять, как попасть в следующую точку. Поэтому на втором шаге мы вычисляем направление для поиска следующей точки:\n",
    "\n",
    "$p_k = -H_k * \\nabla f_k$\n",
    "\n",
    "\n",
    "3. Далее находим следующую точку, используя соотношение:\n",
    "\n",
    "$x_{k+1} = x_k + k * p_k$\n",
    "\n",
    "Здесь важным вопросом является нахождение коэффициента $k$ , который регулирует шаг. Его подбирают линейным поиском в соответствии с условиями, о которых можно подробно прочитать [здесь](https://en.wikipedia.org/wiki/Wolfe_conditions).\n",
    "\n",
    "Нам важно лишь понимать суть: мы находим такое значение $k$, при котором получим минимальное значение функции $f(x_k + k * p_k)$ (так как мы хотим попасть в минимум). Также важно отметить, что в расчёте коэффициента $k$ участвуют две константы $0 \\leq c_1 \\leq c_2 \\leq 1$. Обычно в качестве их значений берут $0.001$ и $0.9$ (это считается хорошей эвристикой, показывающей высокие результаты).\n",
    "\n",
    "4. На следующем шаге необходимо определить два следующих вектора:\n",
    "\n",
    "$s_{k}=x_{k+1}-x_{k}$\n",
    "\n",
    "$y_{k}=\\nabla f_{k+1}-\\nabla f_{k}$\n",
    "\n",
    "Здесь $s_k$ — шаг алгоритма, a $y_k$ — изменение градиента на данной итерации. Они не требовались для перехода в следующую точку и нужны строго для того, чтобы найти следующее приближение обратной матрицы гессиана.\n",
    "\n",
    "5. После того как мы их нашли, обновляем гессиан, руководствуясь следующей формулой:\n",
    "\n",
    "$H_{k+1}=\\left(I-k * s_{k} * y_{k}^{T}\\right) H_{k}\\left(I-k * y_{k} * s_{k}^{T}\\right)+* s_{k} * s_{k}^{T}$\n",
    "\n",
    "Не стоит её пугаться. Как уже было сказано выше, эта формула —  результат очень серьёзных и длительных математических исследований. Её не требуется знать наизусть или уметь реализовывать вручную. Однако для полноты повествования мы не можем не привести её.\n",
    "\n",
    "В данной формуле за $I$ обозначена единичная матрица, $s_k$ и $y_k$ мы вычислили на предыдущем шаге, а $k$ вычисляется следующим образом:\n",
    "\n",
    "$k=\\frac{1}{y_{k}^{T} s_{k}}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм довольно сложный, поэтому давайте рассмотрим **пример** ↓\n",
    "\n",
    ">Сразу оговоримся, что несколько шагов в этом алгоритме мы приведём без ручных расчётов (например, нахождение следующей аппроксимации гессиана) в силу их высокой сложности. Постарайтесь сильнее всего сконцентрироваться на шагах решения и понять логику работы алгоритма и последовательность действий. Мы начнём реализовывать алгоритм «вручную», а затем вы сможете самостоятельно завершить решение задачи с использованием `Python` (разумеется, далее будет указано, как это сделать). К сожалению, серьёзные и эффективные методы настолько сложны, что решать с их помощью задачи, используя только лист бумаги, ручку и калькулятор (как мы могли это делать, например, с методом Лагранжа), уже невозможно.\n",
    "\n",
    "Будем искать экстремум для функции следующего вида:\n",
    "\n",
    "$f(x, y)=x^{2}-x y+y^{2}+9 x-6 y+20$\n",
    "\n",
    "В качестве начальной точки выберем следующую:\n",
    "\n",
    "$x_0 = (1,1)$\n",
    "\n",
    "Находим градиент для нашей функции:\n",
    "\n",
    "$\\begin{aligned} f_{x}^{\\prime} &=\\left(x^{2}-x y+y^{2}+9 x-6 y+20\\right)^{\\prime}_{ x}=\\\\ &=2 x-y+0+9-0+0=2 x-y+9 \\\\ f^{\\prime}_{ y} &=\\left(x^{2}-x y+y^{2}+9 x-6 y+20\\right)^{\\prime} _{y}=\\\\ &=0-x+2 y+0-6+0=-x+2 y-6 \\end{aligned}$\n",
    "\n",
    "$\\nabla f=\\left(\\begin{array}{c}2 x-y+9 \\\\ -x+2 y-6\\end{array}\\right)$\n",
    "\n",
    "Начинаем первую итерацию с точки $x_0 = (1,1)$. Вычисляем для неё градиент:\n",
    "\n",
    "$\\nabla f=\\left(\\begin{array}{c}2 x-y+9 \\\\ -x+2 y-6\\end{array}\\right) \\quad=\\left(\\begin{array}{c}2 \\cdot 1-1+9 \\\\ -1 \\times 2 \\cdot 1-6\\end{array}\\right)=\\left(\\begin{array}{c}10 \\\\ -5\\end{array}\\right)$\n",
    "\n",
    "Теперь необходимо выяснить, стоит ли заканчивать поиск (ведь, возможно, мы уже в минимуме). Для этого находим длину вектора градиента:\n",
    "\n",
    "$\\left|\\nabla f(x_0) \\right| = \\sqrt{10^2 + (-5)^2} = 11.18$\n",
    "\n",
    "Сравниваем полученный результат с точностью, которая нам необходима. Допустим, мы хотим достигнуть точности $0.001$:\n",
    "\n",
    "$\\left|\\nabla f(x_0) \\right| = 11.18 > 0.001$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Итак, **точность не достигнута**, так как градиент в экстремуме должен быть равен или очень близок к нулю. Это значит, что надо искать дальше.\n",
    "\n",
    "Теперь необходимо определить, в каком направлении искать нужную точку. Для этого выполняем следующие вычисления, согласно нашему алгоритму:\n",
    "\n",
    "$p_0 = -H_0 * \\nabla f(x_0) = - \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ \\end{pmatrix} \\begin{pmatrix} 10 \\\\ -5 \\end{pmatrix} = \\begin{pmatrix} -10 \\\\ 5 \\end{pmatrix}$\n",
    "\n",
    "$x_0 + \\alpha_0 * p_0 = (1,1) + \\alpha_0 (10,-5) = (1 - 10 \\alpha_0, 1 + 5 \\alpha_0)$\n",
    "\n",
    "$f(\\alpha_0) = (1 - 10 \\alpha_0)^2 - (1 - 10 \\alpha_0) (1 + 5 \\alpha_0) + (1 + 5 \\alpha_0)^2 + 9 (1 - 10 \\alpha_0) - 6 (1 + 5 \\alpha_0) + 20$\n",
    "\n",
    "Упрощаем выражение:\n",
    "\n",
    "$1-20 a_{0}+100 a_{0}^{2}-1+10 a_{0}-5 a_{0}+50 a_{0}^{2}+1+10 a_{0}+25 a_{0}^{2}+9-90 a_{0}-6-30 a_{0}+20= 175 a_{0}^{2}-125 a_{0}+24$\n",
    "\n",
    "Находим производную от результата:\n",
    "\n",
    "$\\frac{\\partial f}{\\partial a_0}=350a_{0}-125=0 \\Rightarrow a_{0}=0.357$\n",
    "\n",
    "Теперь мы можем найти следующую точку:\n",
    "\n",
    "$x_{1}=x_{0}+{a }_{0} * p_{0}=(-2.571,2.786)$\n",
    "\n",
    "$s_{0}=x_{1}-x_{0}=(-2.571,2.786)-(1,1)=(-3.571,1.786)$\n",
    "\n",
    "Вычисляем значение градиента в следующей найденной точке, т. е. в $x_1$:\n",
    "\n",
    "$\\nabla f\\left(x_{1}\\right)=\\left(\\begin{array}{l}1.071 \\\\ 2.143\\end{array}\\right)$\n",
    "\n",
    "$y_{0}=\\nabla f\\left(x_{1}\\right)-\\nabla f\\left(x_{0}\\right)=(1.071,2.143)-(10,-5)=(-8.929,7.143)$\n",
    "\n",
    "Находим приближение гессиана:\n",
    "\n",
    "$H_{1}=\\left(\\begin{array}{ll}0.694 & 0.367 \\\\ 0.367 & 0.709\\end{array}\\right)$\n",
    "\n",
    "Проверяем, стоит ли остановиться в этой точке:\n",
    "\n",
    "$\\left|\\nabla f(x_1) \\right| = 2.396 > 0.001$\n",
    "\n",
    "К сожалению, необходимая точность всё ещё не достигнута.\n",
    "\n",
    "Перечисленные выше шаги стоит повторять до тех пор, пока не будет достигнуто значение, близкое к нулю (меньшее, чем изначально заданная точность).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Разумеется, при решении прикладных задач не понадобится делать ничего подобного, ведь мы умеем программировать и знаем, что в библиотеках Python есть все необходимые методы.\n",
    "\n",
    "Давайте рассмотрим, как с помощью функций `Python` мы сможем применить квазиньютоновские методы для оптимизации функции:\n",
    "\n",
    "$f(x, \\ y)=x^2+y^2$\n",
    "\n",
    "Подгрузим необходимые библиотеки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим функцию, которую будем оптимизировать. Вместо отдельных $x$ и $y$ можно взять координаты единого вектора:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    return x[0]**2.0 + x[1]**2.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь определим градиент для функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_func(x):\n",
    "    return np.array([x[0] * 2, x[1] * 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Статус оптимизации Optimization terminated successfully.\n",
      "Количество оценок: 3\n",
      "Решение: f([0. 0.]) = 0.00000\n"
     ]
    }
   ],
   "source": [
    "# Зададим начальную точку:\n",
    "x_0 = [1.0, 1.0]\n",
    "\n",
    "# Определим алгоритм:\n",
    "result = minimize(func, x_0, method='BFGS', jac=grad_func)\n",
    "\n",
    "# Выведем результаты:\n",
    "print('Статус оптимизации %s' % result['message'])\n",
    "print('Количество оценок: %d' % result['nfev'])\n",
    "solution = result['x']\n",
    "evaluation = func(solution)\n",
    "print('Решение: f(%s) = %.5f' % (solution, evaluation))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, мы получили, что минимум функции достигается в точке $(0, \\ 0)$. Значение функции в этой точке также равно нулю.\n",
    "\n",
    "Можно повторить то же самое с вариацией  `L-BFGS-B`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Статус оптимизации CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL\n",
      "Количество оценок: 3\n",
      "Решение: f([0. 0.]) = 0.00000\n"
     ]
    }
   ],
   "source": [
    "# определяем нашу функцию\n",
    "def func(x):\n",
    "    return x[0]**2.0 + x[1]**2.0\n",
    " \n",
    "#  определяем градиент функции\n",
    "def grad_func(x):\n",
    "    return np.array([x[0] * 2, x[1] * 2])\n",
    " \n",
    "# определяем начальную точку\n",
    "x_0 = [1, 1]\n",
    "# реализуем алгоритм L-BFGS-B\n",
    "result = minimize(func, x_0, method='L-BFGS-B', jac=grad_func)\n",
    "# получаем результат\n",
    "print('Статус оптимизации %s' % result['message'])\n",
    "print('Количество оценок: %d' % result['nfev'])\n",
    "solution = result['x']\n",
    "evaluation = func(solution)\n",
    "print('Решение: f(%s) = %.5f' % (solution, evaluation))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Иногда количество итераций у двух модификаций различается, но ответ совпадает. Бывает также, что одна из вариаций может не сойтись, а другая — достичь экстремума, поэтому советуем не воспринимать их как взаимозаменяемые алгоритмы. На практике лучше пробовать разные варианты: если у вас не сошёлся алгоритм `BFGS`, можно попробовать `L-BFGS-B`, и наоборот. Также можно экспериментировать одновременно с обоими алгоритмами, чтобы выбрать тот, который будет сходиться для функции за меньшее число итераций и тем самым экономить время.\n",
    "\n",
    "→ Важно понимать, что для некоторых функций не из всех стартовых точек получается достичь сходимости метода. Тогда их можно перебирать, к примеру, с помощью цикла.\n",
    "\n",
    "## Задание 4.1\n",
    "\n",
    "Найдите точку минимума для функции $f(x,y) = x^2 - xy + y^2 + 9x - 6y + 20$.\n",
    "\n",
    "В качестве стартовой возьмите точку $(-400, \\ -400)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Статус оптимизации Optimization terminated successfully.\n",
      "Количество оценок: 11\n",
      "Решение: f([-4.  1.]) = -1.00000\n"
     ]
    }
   ],
   "source": [
    "# определяем нашу функцию\n",
    "def func(x):\n",
    "    return x[0]**2.0 + x[1]**2.0 - x[0]*x[1] + 9*x[0] - 6*x[1] + 20\n",
    " \n",
    "#  определяем градиент функции\n",
    "def grad_func(x):\n",
    "    return np.array([x[0] * 2 - x[1] + 9, x[1] * 2 - x[0] - 6])\n",
    " \n",
    "# определяем начальную точку\n",
    "x_0 = [-400, -400]\n",
    "# реализуем алгоритм L-BFGS-B\n",
    "result = minimize(func, x_0, method='BFGS', jac=grad_func)\n",
    "# получаем результат\n",
    "print('Статус оптимизации %s' % result['message'])\n",
    "print('Количество оценок: %d' % result['nfev'])\n",
    "solution = result['x']\n",
    "evaluation = func(solution)\n",
    "print('Решение: f(%s) = %.5f' % (solution, evaluation))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 4.4\n",
    "\n",
    "Найдите минимум функции $f(x) = x^2 - 3x +45$ с помощью квазиньютоновского метода `BFGS`.\n",
    "\n",
    "В качестве стартовой точки возьмите $x=10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Статус оптимизации CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL\n",
      "Количество оценок: 3\n",
      "Решение: f([1.5]) = 42.75000\n"
     ]
    }
   ],
   "source": [
    "# определяем нашу функцию\n",
    "def func(x):\n",
    "    return x**2 - 3*x + 45\n",
    " \n",
    "#  определяем градиент функции\n",
    "def grad_func(x):\n",
    "    return np.array(2*x - 3)\n",
    " \n",
    "# определяем начальную точку\n",
    "x_0 = 10\n",
    "# реализуем алгоритм L-BFGS-B\n",
    "result = minimize(func, x_0, method='L-BFGS-B', jac=grad_func)\n",
    "# получаем результат\n",
    "print('Статус оптимизации %s' % result['message'])\n",
    "print('Количество оценок: %d' % result['nfev'])\n",
    "solution = result['x']\n",
    "evaluation = func(solution)\n",
    "print('Решение: f(%s) = %.5f' % (solution, evaluation))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 4.7\n",
    "\n",
    "Найдите минимум функции $f(x,y) = x^4 + 6y^2 + 10$, взяв за стартовую точку $(100, \\ 100)$.\n",
    "\n",
    "Какой алгоритм сошелся быстрее?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество оценок BFGS: 37\n",
      "Количество оценок L-BFGS-B: 40\n",
      "Быстрее BFGS\n",
      "Решение: f([-9.52718297e-03 -2.32170510e-06]) = 10.00000\n"
     ]
    }
   ],
   "source": [
    "# определяем нашу функцию\n",
    "def func(x):\n",
    "    return x[0]**4 + 6*x[1]**2 + 10\n",
    " \n",
    "#  определяем градиент функции\n",
    "def grad_func(x):\n",
    "    return np.array([4*x[0]**3, 12*x[1]])\n",
    " \n",
    "# определяем начальную точку\n",
    "x_0 = [100, 100]\n",
    "# реализуем алгоритм L-BFGS-B\n",
    "result = minimize(func, x_0, method='BFGS', jac=grad_func)\n",
    "result_1 = minimize(func, x_0, method='L-BFGS-B', jac=grad_func)\n",
    "\n",
    "print('Количество оценок BFGS: %d' % result['nfev'])\n",
    "print('Количество оценок L-BFGS-B: %d' % result_1['nfev'])\n",
    "if result['nfev'] > result_1['nfev']:\n",
    "    print('Быстрее L-BFGS-B')\n",
    "    solution = result['x']\n",
    "    evaluation = func(solution)\n",
    "else:\n",
    "    print('Быстрее BFGS')\n",
    "    solution = result_1['x']\n",
    "    evaluation = func(solution)\n",
    "    \n",
    "print('Решение: f(%s) = %.5f' % (solution, evaluation))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Линейное программирование <a class=\"anchor\" id=5></a>\n",
    "\n",
    "[к содержанию](#0)\n",
    "\n",
    "**Линейное программирование** — это метод оптимизации для системы линейных ограничений и линейной целевой функции. Целевая функция определяет оптимизируемую величину, и цель линейного программирования состоит в том, чтобы **найти значения переменных**, которые максимизируют или минимизируют целевую функцию."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Линейное программирование полезно применять для многих задач, требующих оптимизации ресурсов:\n",
    "\n",
    "* В производстве — чтобы рассчитать человеческие и технические ресурсы и минимизировать стоимость итоговой продукции.\n",
    "\n",
    "* При составлении бизнес-планов — чтобы решить, какие продукты продавать и в каком количестве, чтобы максимизировать прибыль.\n",
    "\n",
    "* В логистике — чтобы определить, как использовать транспортные ресурсы для выполнения заказов за минимальное время.\n",
    "\n",
    "* В сфере общепита — чтобы составить расписание для официантов.\n",
    "\n",
    ">Задача линейного программирования — это задача оптимизации, в которой целевая функция и функции-ограничения линейны, а все переменные неотрицательны.\n",
    "\n",
    "Есть и разновидность задачи линейного программирования, в которой мы знаем, что все переменные могут принимать только целочисленные значения. Например, если переменные в нашей задаче — это количество людей или произведённых на заводе изделий, то они, разумеется, не могут быть дробными.\n",
    "\n",
    ">**Целочисленным линейным программированием (ЦЛП)** называется вариация задачи линейного программирования, когда все переменные — целые числа.\n",
    "\n",
    "Давайте рассмотрим алгоритм решения задачи линейного программирования на конкретном примере ↓"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**Пример № 1**\n",
    "\n",
    "Фабрика игрушек производит игрушки-антистресс и игрушки-вертушки.\n",
    "\n",
    "Для изготовления игрушки-антистресс необходимо потратить 2 доллара и 3 часа, для изготовления игрушки-вертушки — 4 доллара и 2 часа.\n",
    "\n",
    "На этой неделе в бюджете у фабрики есть 220 долларов, и оплачено 150 трудочасов для производства указанных игрушек.\n",
    "\n",
    "Если одну игрушку-антистресс можно продать за 6 долларов, а игрушку-вертушку — за 7, то сколько экземпляров каждого товара необходимо произвести на этой неделе, чтобы максимизировать прибыль?\n",
    "\n",
    "***\n",
    "\n",
    "Такая задача идеально подходит для использования методов линейного программирования по следующим причинам:\n",
    "\n",
    "* все условия являются линейными;\n",
    "\n",
    "* значения переменных каким-то образом ограничены;\n",
    "\n",
    "* цель состоит в том, чтобы найти значения переменных, которые максимизируют некоторую величину.\n",
    "\n",
    "Обратите внимание, что производство каждой детали связано с затратами, как временными, так и финансовыми. Изготовление каждой игрушки-антистресс стоит 2 доллара, а изготовление каждой вертушки — 4 доллара. У фабрики есть только 220 долларов, чтобы потратить их на производство изделий. Отсюда возникает **ограничение на возможное количество изготовленных товаров**.\n",
    "\n",
    "Обозначим за $x$ количество произведённых игрушек-антистресс, за $y$ — количество произведённых игрушек-вертушек Тогда это ограничение можно записать в виде следующего неравенства:\n",
    "\n",
    "$2x+4y \\leq 220$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также существует ограничение на то, сколько времени мы можем потратить на производство игрушек. На изготовление каждой игрушки-антистресс уходит 3 часа, а на изготовление каждой игрушки-вертушки — 2 часа. На этой неделе у фабрики есть только 150 рабочих часов, так что **производство ограничено по времени**. Это ограничение можно записать в виде неравенства:\n",
    "\n",
    "$3x+2y \\leq 150$\n",
    "\n",
    "К этим ограничениям мы также можем добавить ограничения из соображений здравого смысла. Невозможно произвести отрицательное количество игрушек, поэтому необходимо обозначить следующие ограничения:\n",
    "\n",
    "$x \\geq 0$\n",
    "\n",
    "$y \\geq 0$\n",
    "\n",
    "Итак, мы записали все необходимые ограничения. Они образуют систему неравенств:\n",
    "\n",
    "$\\left\\{\\begin{aligned} 2 x+4 y & \\leq 220 \\\\ 3 x+2 y & \\leq 150 \\\\ x & \\geq 0 \\\\ y & \\geq 0  \\end{aligned}\\right.$\n",
    "\n",
    "Если представить систему этих неравенств в [графическом виде](http://mathprofi.ru/lineinye_neravenstva.html), получим многоугольник:\n",
    "\n",
    "<img src=m6_img13.png width=400>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Закрашенная область является **областью допустимых решений** этой задачи. Наша цель — найти внутри этого многоугольника такую точку, которая даст **наилучшее решение нашей задачи** (максимизацию или минимизацию целевой функции).\n",
    "\n",
    "Итак, мы поняли, что нам нужно найти максимальное значение прибыли (целевой функции) для точек внутри области допустимых значений, однако самой целевой функции у нас пока нет. Давайте составим её. На изготовление каждой игрушки-антистресс требуется 2 доллара, а продать её можно за 6. Получается, что чистая прибыль от продажи составляет 4 доллара. Чтобы изготовить игрушку-вертушку, мы потратим 4 доллара, а продадим её за 7. Значит, чистая прибыль для вертушки составляет 3 доллара. Исходя из этого, получаем целевую функцию для суммарной прибыли:\n",
    "\n",
    "$p(x, y)=4x+3y$\n",
    "\n",
    "Нам необходимо найти максимально возможное значение этой функции с учётом того, что точка, в которой оно будет достигаться, должна удовлетворять условиям системы, которую мы написали ранее.\n",
    "\n",
    "Для того чтобы решить задачу, выразим одну переменную через другую (так удобнее строить графики линейной функции в стандартной системе координат):\n",
    "\n",
    "$y=-\\frac{4}{3} x+\\frac{P}{3}$\n",
    "\n",
    "На графике ниже наша линия изображена **красным** цветом. Она может двигаться вверх и вниз в зависимости от значения $P$ (вы можете видеть три прямых — это три разных положения одной и той же прямой).\n",
    "\n",
    "Попробуем найти такую точку, для которой значение $y$ будет наибольшим. Нас это интересует, так как мы хотим максимизировать значение $P$, а при максимально возможном $P$ прямая поднимется настолько высоко, насколько это возможно, и пересечение с точкой ординат тоже будет находиться максимально высоко.\n",
    "\n",
    "<img src=m6_img14.png width=400>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Линия, которая максимизирует точку пересечения c осью ординат, проходит через точку $(20, \\ 45)$ — это точка пересечения первых двух ограничений. Все остальные прямые, которые проходят выше, не проходят через область допустимых решений. Все остальные «нижние» прямые проходят более чем через одну точку в допустимой области и не максимизируют пересечение прямой с осью ординат, так как находятся ниже.\n",
    "\n",
    "Таким образом, получаем, что завод должен произвести 20 игрушек-антистресс и 45 игрушек-вертушек. Это даст прибыль в размере 215 долларов:\n",
    "\n",
    "$20 \\cdot 4+45 \\cdot 3 = 215$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Пример № 2**\n",
    "\n",
    "Фермер кормит своих коров специальной смесью. Для её изготовления он использует два вида корма: на 1 кг кукурузного корма приходится 100 г белка и 750 г крахмала, на 1 кг пшеничного — 150 г белка и 700 г крахмала.\n",
    "\n",
    "Каждой корове необходимо давать не более 7 кг корма в сутки, иначе у неё заболит живот и придётся тратить деньги на ветеринара. При этом, чтобы давать оптимально полезное и вкусное молоко, каждая корова должна ежедневно потреблять как минимум 650 г белка и 4000 г крахмала.\n",
    "\n",
    "Известно, что кукурузный корм стоит 0.4 доллара за 1 кг, а пшеничный — 0.45 долларов за 1 кг.\n",
    "\n",
    "Какая кормовая смесь будет минимизировать затраты и в то же время позволит получать качественное молоко?\n",
    "\n",
    "Обозначим за $c$ количество кукурузного корма в смеси, а за $w$ — количество пшеничного. Тогда ограничения можно выразить следующим образом:\n",
    "\n",
    "$\\left\\{\\begin{aligned} 0.1 c+0.15 w & \\geq 0.65 \\\\ 0.75 c+0.7 w & \\geq 4 \\\\ c+w & \\leq 7 \\\\ c & \\geq 0 \\\\ w & \\geq 0 \\end{aligned}\\right.$\n",
    "\n",
    "**Минимизировать** мы будем целевую функцию, отражающую затраты на корм и имеющую следующий вид:\n",
    "\n",
    "$f(c, w)=0.40 c+0.45 w$\n",
    "\n",
    "Снова изобразим условие задачи графически. Начнём с многоугольника области допустимых значений:\n",
    "\n",
    "<img src=m6_img15.png width=400>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получается, что нам необходимо найти точку, в которой пересекаются две прямые, обозначающие условия. Можно записать это следующим образом (точка пересечения — решение системы):\n",
    "\n",
    "$\\left\\{\\begin{array}{l} 0.1 c+0.15 w=0.65 \\\\ 0.75 c+0.7 w=4 \\end{array}\\right.$\n",
    "\n",
    "Найдя эту точку (для этого можно воспользоваться методом Гаусса или программированием), можно найти и итоговую стоимость смеси:\n",
    "\n",
    "$f(3.411,2.059)=\\$ 2.29$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Пример № 1\n",
    "\n",
    "Вы отвечаете за рекламу в компании.\n",
    "\n",
    "Затраты на рекламу в месяц **не должны превышать 10 000 руб**. Один показ рекламы в интернете стоит **1 рубль**, а на телевидении — **90 рублей**. При этом на телевидении нельзя купить больше **20 показов**.\n",
    "\n",
    "Практика показывает, что 1 показ телерекламы приводит в среднем 300 клиентов, а 1 показ в интернете — 0.5 клиента.\n",
    "\n",
    "**Вам необходимо привести как можно больше клиентов.**\n",
    "\n",
    "Обозначим за $x_1$ количество показов в интернете, за $x_2$ — количество показов на телевидении. Будем максимизировать число приведённых клиентов $0.5x_1+300x_2$.\n",
    "\n",
    "Составим задачу минимизации с ограничением на количество показов и затраты. Наша целевая функция примет следующий вид:\n",
    "\n",
    "$\\min \\left(-0.5 x_{1}-300 x_{2}\\right)$\n",
    "\n",
    ">Мы заменили максимизацию на минимизацию, так как готовим задачу для решения стандартным алгоритмом, а по умолчанию задача оптимизации сводится к минимизации. Поэтому перед применением функций Python формулируйте задачу именно в формате поиска минимума.\n",
    "\n",
    "Ограничения можно выразить так:\n",
    "\n",
    "$\\left\\{\\begin{array}{l}x_{2} \\leq 20 \\\\ x_{1}+90 x_{2} \\leq 10000\\end{array}\\right.$\n",
    "\n",
    "Разумеется, будем помнить, что количество показов рекламы всегда неотрицательное.\n",
    "\n",
    "Представим наши данные в векторном виде. Пусть $c$ — это вектор приведённых клиентов:\n",
    "\n",
    "$c=\\left(\\begin{array}{c}-0.5 \\\\ -300\\end{array}\\right)$\n",
    "\n",
    "За $A$ и $b$ мы возьмём такие матрицы, чтобы с их помощью можно было представить систему ограничений:\n",
    "\n",
    "$A=\\left(\\begin{array}{cc}0 & 1 \\\\ 1 & 90\\end{array}\\right), \\ b=\\left(\\begin{array}{c}20 \\\\ 10000\\end{array}\\right)$\n",
    "\n",
    "Тогда наша задача формулируется следующим образом:\n",
    "\n",
    "$\\operatorname{min} c^{T} x$\n",
    "\n",
    "$A x \\leq b$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пример № 2\n",
    "\n",
    "Есть $n$ задач и $n$ человек, которые могут их выполнить.\n",
    "\n",
    "Каждая задача должна быть сделана одним человеком, и каждый должен сделать ровно одну задачу.\n",
    "\n",
    "Выполнение задачи $j$ человеком $i$ будет стоить $c_{ij}$.\n",
    "\n",
    "Вам необходимо сделать все задачи как можно дешевле.\n",
    "\n",
    "***\n",
    "\n",
    "Так как очевидно, что люди и задачи могут быть выражены только целыми числами, будем формулировать задачу как задачу целочисленного программирования.\n",
    "\n",
    "Пусть $x_{ij}=1$, если задачу $j$ выполнил человек $i$. Если работы выполнил кто-то другой, то $x_{ij}$.\n",
    "\n",
    "Распределение работ по людям мы можем представить в виде таблицы-матрицы, например, следующим образом:\n",
    "\n",
    "<img src=m6_img16.png width=300>\n",
    "\n",
    "Здесь на пересечениях столбцов-работ и строк-людей указана стоимость работы, если её выполнит выбранный человек. Например, второй человек выполнит первую работу за 1 денежную единицу, вторую — за 5 денежных единиц, а третью — за 2 денежных единицы.\n",
    "\n",
    "Если мы выберем первого человека для выполнения первой работы, второго — для третьей и третьего — для второй (как и закрашено на рисунке выше), тогда в матрице распределения работ мы обозначим соответствующие элементы за 1, а остальные — за 0.\n",
    "\n",
    "Теперь сформулируем задачу. Минимизируем суммарную стоимость $c_{ij} \\cdot x_{ij}$:\n",
    "\n",
    "$\\min \\sum_{i, j} c_{i j} x_{i j}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь запишем условия:\n",
    "\n",
    "1. $x$ равен либо $0$, либо $1$: $x_{ij} \\leq 1$.\n",
    "\n",
    "2. Каждый человек должен взять ровно одну задачу: $\\forall i: \\sum_{j} x_{i j}=1$.\n",
    "\n",
    "3. Каждую задачу должен взять ровно один человек: $\\forall j: \\sum_{i} x_{i j}=1$.\n",
    "\n",
    ">Мы сформулировали задачи линейного программирования в чётком математическом виде. Самое время переходить к следующему юниту и учиться решать задачи с использованием программирования →\n",
    "\n",
    "## ДОПОЛНИТЕЛЬНО\n",
    "\n",
    "Если вам интересно изучить более сложные, но очень известные задачи оптимизации, к которым применимо линейное программирование, рекомендуем прочитать следующие статьи:\n",
    "\n",
    "* [Задача коммивояжёра](https://habr.com/ru/post/560468/)\n",
    "\n",
    "* [Транспортная задача](https://habr.com/ru/post/573224/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Практика: Линейное программирование <a class=\"anchor\" id=6></a>\n",
    "\n",
    "[к содержанию](#0)\n",
    "\n",
    "В языке `Python` есть множество библиотек, с помощью которых можно решить задачу линейного программирования. Вот основные, которые мы рассмотрим в данном юните:\n",
    "\n",
    "* `SciPy` (`scipy.optimize.linprog`)\n",
    "\n",
    "* `CVXPY`\n",
    "\n",
    "* `PuLP`\n",
    "\n",
    "У каждой библиотеки есть свои особенности использования, и большинство задач можно решить с помощью любой из них. Давайте посмотрим все варианты, чтобы у вас всегда был выбор — решим по одной задаче для каждой библиотеки."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Пример № 1. SciPy** (`scipy.optimize.linprog`)\n",
    ">\n",
    ">У нас есть 6 товаров с заданными ценами на них и заданной массой.\n",
    ">\n",
    ">Вместимость сумки, в которую мы можем положить товары, заранее известна и равна 15 кг.\n",
    ">\n",
    ">Какой товар и в каком объёме необходимо взять, чтобы сумма всех цен товаров была максимальной?\n",
    "\n",
    "Создадим переменные на основе предложенных данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [4, 2, 1, 7, 3, 6] #стоимости товаров\n",
    "weights = [5, 9, 8, 2, 6, 5] #вес товаров\n",
    "C = 15 #вместимость сумки\n",
    "n = 6 #количество товаров"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сформулируем задачу линейного программирования. Максимизируем произведение стоимости на количество, учитывая, что произведение веса на искомое количество товаров должно укладываться во вместимость сумки:\n",
    "\n",
    "$\\max \\sum v_{i} x_{i}$\n",
    "\n",
    "$\\sum w_{i} x_{i} \\leq C$\n",
    "\n",
    "Из предыдущего юнита мы уже знаем, что в векторно-матричной форме наша задача должна формулироваться в следующем виде:\n",
    "\n",
    "$\\operatorname{min} c^{T} x$\n",
    "\n",
    "$A x \\leq b$\n",
    "\n",
    "Получается, что в наших обозначениях мы имеем следующее:\n",
    "\n",
    "$c=-v, \\ A=w^{T}, \\ b=(C)$\n",
    "\n",
    "Здесь нам необходимо вспомнить линейную алгебру, так как очень важно, чтобы векторы были в нужных нам размерностях, иначе мы не сможем использовать матричное умножение. Вектор $A$ размера $6$ мы превращаем в матрицу размера $(1, \\ 6)$ с помощью функции `expand_dims()`. Создаём все необходимые переменные:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "c = - np.array(values) #изменяем знак, чтобы перейти от задачи максимизации к задаче минимизации\n",
    "A = np.array(weights)  #конвертируем список с весами в массив\n",
    "A = np.expand_dims(A, 0) #преобразуем размерность массива\n",
    "b = np.array([C]) #конвертируем вместимость в массив"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Передаём подготовленные переменные в оптимизатор `SciPy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     con: array([], dtype=float64)\n",
       "     fun: -52.50000000003077\n",
       " message: 'Optimization terminated successfully.'\n",
       "     nit: 5\n",
       "   slack: array([-2.2495783e-11])\n",
       "  status: 0\n",
       " success: True\n",
       "       x: array([6.18738532e-14, 1.05853306e-12, 1.21475943e-13, 7.50000000e+00,\n",
       "       4.00246692e-13, 4.71394162e-13])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import linprog\n",
    "linprog(c=c, A_ub=A, b_ub=b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем искомое значение функции — $-52.5$ (в выводе значение с минусом, но мы меняем знак, возвращаясь к задаче максимизации). $x=(0, 0, 0, 7.5, 0, 0)$. Таким образом, мы взяли только самую дорогую, четвёртую вещь. Она одна весит 2 кг, а если взять её 7.5 раз, то получится как раз 15 кг. Отлично, задача решена."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример № 2. `CVXPY`\n",
    "\n",
    "Снова решим задачу из примера № 1, но уже предположим, что товары нельзя дробить, и будем решать задачу целочисленного линейного программирования.\n",
    "\n",
    "`SciPy` не умеет решать такие задачи, поэтому будем использовать новую библиотеку `CVXPY`.\n",
    "\n",
    ">Важно! С установкой этот библиотеки порой возникают проблемы. Если вы столкнулись с трудностями, посоветуйтесь с ментором или воспользуйтесь `Google Colaboratory`.\n",
    "\n",
    "```python\n",
    "x = cvxpy.Variable(shape=n, integer = True)\n",
    "```\n",
    "Далее зададим ограничения, используя матричное умножение:\n",
    "```python\n",
    "A = A.flatten() # Преобразуем размерность массива\n",
    "constraint = cvxpy.sum(cvxpy.multiply(A, x)) <= C\n",
    "total_value = cvxpy.sum(cvxpy.multiply(x, c))\n",
    "```\n",
    "Переходим непосредственно к решению задачи:\n",
    "```python\n",
    "problem = cvxpy.Problem(cvxpy.Minimize(total_value), constraints=[constraint])\n",
    "```\n",
    "Вызываем получившееся решение:\n",
    "```python\n",
    "problem.solve()\n",
    "```\n",
    "В результате получаем бесконечность. Это совершенно нереалистично.\n",
    "\n",
    "В таком случае будем рассматривать только положительные значения $x$:\n",
    "\n",
    "$x \\leq 0$\n",
    "\n",
    "В переформулированном виде задача будет решаться следующим образом:\n",
    "```python\n",
    "x = cvxpy.Variable(shape=n, integer=True)\n",
    "constraint = cvxpy.sum(cvxpy.multiply(A, x)) <= C\n",
    "x_positive = x >= 0\n",
    "total_value = cvxpy.sum(cvxpy.multiply(x, c))\n",
    "\n",
    "problem = cvxpy.Problem(\n",
    "    cvxpy.Minimize(total_value), constraints=[constraint, x_positive]\n",
    ")\n",
    "\n",
    "problem.solve()\n",
    "x.value\n",
    "```\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь мы уже получаем $49$, и берём только четвёртый товар в количестве семи штук. Можно увидеть, что результат, в целом, очень близок к первому, когда мы использовали библиотеку `SciPy` — различие лишь в добавлении целочисленности. Значит, у нас получилось решить задачу, когда мы добавили недостающее условие.\n",
    "\n",
    "А что если мы можем брать не любое количество товаров, а только один или не брать их вовсе? Задаём $x$ типа `boolean`.\n",
    "\n",
    "$x=0$ или $x=1$\n",
    "\n",
    "Программное решение такой задачи имеет следующий вид:\n",
    "\n",
    "```python\n",
    "x = cvxpy.Variable(shape=n, boolean=True)\n",
    "constraint = cvxpy.sum(cvxpy.multiply(A, x)) <= C\n",
    "x_positive = x >= 0\n",
    "total_value = cvxpy.sum(cvxpy.multiply(x, c))\n",
    "\n",
    "problem = cvxpy.Problem(\n",
    "    cvxpy.Minimize(total_value), constraints=[constraint, x_positive]\n",
    ")\n",
    "\n",
    "problem.solve()\n",
    "x.value\n",
    "```\n",
    "\n",
    "Получим стоимость, равную $17$, взяв первый, четвёртый и шестой товары.\n",
    "\n",
    ">Обратите внимание, что, используя `SciPy`, мы могли не указывать явно, что $x$ только положительные, так как в линейном программировании считаются только неотрицательные $x$.\n",
    ">\n",
    ">А вот `CVXPY` универсальна. Мы просто задали функцию, не указывая, что это линейное программирование. `CVXPY` «поняла», что это задача оптимизации, и использовала нужные алгоритмы. Поэтому здесь ограничение на положительные $x$ мы указывали явно."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "## Пример № 3. `PuLP`\n",
    "\n",
    "В нашей каршеринговой компании две модели автомобилей: модель $A$ и модель $B$. Автомобиль $A$ даёт прибыль в размере 20 тысяч в месяц, а автомобиль $B$ — 45 тысяч в месяц. Мы хотим заказать на заводе новые автомобили и максимизировать прибыль. Однако на производство и ввод в эксплуатацию автомобилей понадобится время:\n",
    "\n",
    "* Проектировщику требуется 4 дня, чтобы подготовить документы для производства каждого автомобиля типа $A$, и 5 дней — для каждого автомобиля типа $B$.\n",
    "\n",
    "* Заводу требуется 3 дня, чтобы изготовить модель $A$, и 6 дней, чтобы изготовить модель $B$.\n",
    "\n",
    "* Менеджеру требуется 2 дня, чтобы ввести в эксплуатацию в компании автомобиль $A$, и 7 дней — автомобиль $B$.\n",
    "\n",
    "* Каждый специалист может работать суммарно 30 дней.\n",
    "\n",
    "Целевая функция будет выглядеть следующим образом:\n",
    "\n",
    "$20000A + 45000B$\n",
    "\n",
    "Также запишем ограничения:\n",
    "\n",
    "$A, \\ B \\geq 0$\n",
    "\n",
    "$4A + 5B \\leq 30$\n",
    "\n",
    "$3A +6B \\leq 30$\n",
    "\n",
    "$2A + 7B \\leq 30$\n",
    "\n",
    ">Заметьте, что здесь мы снова пишем обычные неравенства, а не условия в матричном виде. Дело в том, что для данной библиотеки так «удобнее», так как она принимает все условия в «первичном» виде."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Все вычисления в `Google Colaboratory`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Практика: Дополнительные методы оптимизации <a class=\"anchor\" id=7></a>\n",
    "\n",
    "[к содержанию](#0)\n",
    "\n",
    "# МЕТОД ИМИТАЦИИ ОТЖИГА"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В первую очередь познакомимся с методом имитации отжига (`simulated annealing`).\n",
    "\n",
    "Данный метод можно использовать, когда нужно оптимизировать достаточно сложную и «неудобную» функцию, то есть, такую, для которой не получится применить градиентные или другие оптимизационные методы.\n",
    "\n",
    "Метод имитации отжига удобен для применения, так как это метод нулевого порядка, а значит у него намного меньше ограничений.\n",
    "\n",
    "Метод отжига используется также в ускорении обучения **нейронных сетей**.\n",
    "\n",
    ">Идея для алгоритма имитации отжига взята из реальной жизни. Во время отжига материал (обычно это металл) нагревают до определённой температуры, а затем медленно охлаждают. Когда материал горячий, у него снижается твёрдость и его проще обрабатывать. Когда он остывает, то становится более твёрдым и менее восприимчивым к изменениям.\n",
    "\n",
    "Наша задача — получить максимально холодный твёрдый материал. Для этого нужна крепкая кристаллическая решётка, в которой все атомы находятся на местах с минимальной энергией. Смысл в том, что, когда металл сильно нагревается, атомы его кристаллической решётки вынужденно покидают свои положения. Когда начинается охлаждение, они стремятся вернуться в состояние, где будет минимальный уровень определённой энергии. То есть нам было нужно, чтобы металл стал холодным и твёрдым, но мы сначала нагрели его (то есть «ухудшили» ситуацию относительно необходимой), чтобы в итоге (после охлаждения) получить намного более крепкую кристаллическую решётку.\n",
    "\n",
    ">Метод имитации отжига относится к **эвристическим методам**. Это алгоритмы, которые основаны на некоторой математической и логической интуиции и возникают без фундаментальных теоретических предпосылок.\n",
    "\n",
    "Для таких методов обычно нет исследований и доказательств эффективности для различных случаев, однако есть практические наблюдения, демонстрирующие высокую эффективность. Случается и такое, что эвристический метод доказанно ложный с точки зрения математики, но его всё равно используют в отдельных случаях, где он внезапно показывает высокое качество.\n",
    "\n",
    "Обобщая, можно сказать, что **эвристический подход** — это подход, не имеющий под собой математического обоснования (а возможно, даже имеющий опровержение для каких-то случаев), но при этом являющийся эффективным и полезным.\n",
    "\n",
    "Метод отжига является именно таким: с одной стороны, он может быстро находить решения экспоненциально сложных задач, а с другой — не всегда сходится к решению, а также может приводить к поиску только локальных минимумов. Однако метод отжига — действенное средство для решения многих сложных оптимизационных задач. Давайте познакомимся с ним ↓\n",
    "\n",
    ">**Идея метода** заключается в том, чтобы иногда позволять значению функции в точке «ухудшаться» (то есть удаляться от минимума) по аналогии с отжигом, где мы «ухудшали» состояние металла (т. е. нагревали его), чтобы в итоге достичь наилучшего результата."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Представим, что мы ищем минимум для такой функции:\n",
    "\n",
    "<img src=m6_img17.png width=500>\n",
    "\n",
    "Если мы будем минимизировать такую функцию с помощью градиентного спуска, то неизбежно застрянем в локальном минимуме. Метод отжига может позволить функции удалиться от минимума, и тогда мы сможем «перепрыгнуть» препятствие и достичь глобального минимума."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Метод отжига реализуется по следующему алгоритму:\n",
    "\n",
    "1. Выбор изначальной точки. Обычно начальная точка выбирается случайным образом. Также нужно выбрать начальную и конечную температуру.\n",
    "\n",
    "2. Основной алгоритм:\n",
    "\n",
    "* Выбираем случайную точку рядом с нашей точкой.\n",
    "\n",
    "* Оцениваем, переходим ли мы в эту новую точку:\n",
    "\n",
    "* * Если значение функции в новой точке «лучше» значения функции в нашей точке, то переходим.\n",
    "\n",
    "* * Если значение функции в новой точке «хуже» значения функции в нашей точке, то всё равно с некоторой вероятностью переходим. Чем ниже температура, тем ниже эта вероятность.\n",
    "\n",
    "* Уменьшаем температуру. Если температура стала ниже конечной, алгоритм прекращает работу.\n",
    "\n",
    "**Математически** и в более строгом виде это **можно записать следующим образом**:\n",
    "\n",
    "1. Выбираем начальную точку $x_0$ и определяем счётчик итераций $k$. Также определяем начальную температуру $t_0$, конечную температуру $t_{min}$ и функцию температуры $T(k)$, по которой температура будет уменьшаться с количеством итераций.\n",
    "\n",
    "2. На каждой итерации $k$ выбираем случайную точку $x^*$ из окрестности точки $x_k$.\n",
    "\n",
    "3. Если мы видим, что значение функции в точке $x^*$ меньше значения функции в точке $x_k$, то переходим в новую точку:\n",
    "\n",
    "$f\\left(x^{*}\\right)<f\\left(x_{k}\\right)\\to x_{k+1}=x^{*}$\n",
    "\n",
    "Если же значение функции в точке $x^*$ больше значения функции в точке $x_k$, то с вероятностью $e^{\\frac{f(x_{k})-f(x^*)}{t_{k}}}$ мы всё равно переходим в новую точку:\n",
    "\n",
    "$f(x^*)>f(x_k)\\;\\; \\& \\;\\;p<e^{\\frac{f(x_{k})-f(x^*)}{t_{k}}}\\;\\to\\; x_{k+1}=x^{*}$\n",
    "\n",
    "4. Понижаем температуру:\n",
    "\n",
    "$t_{k+1}=T(k)$\n",
    "\n",
    "Если температура $t_{k+1}$ меньше конечной температуры $t_{min}$, то алгоритм прекращает работу.\n",
    "\n",
    "5. Увеличиваем счётчик итераций на 1:\n",
    "\n",
    "$k=k+1$\n",
    "\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы применяем этот метод, то нам предстоит ответить на два вопроса:\n",
    "\n",
    "* Как выбирать начальную точку $x_0$?\n",
    "\n",
    "* Как выбирать функцию температуры $T(k)$?\n",
    "\n",
    "Первое значение, как правило, определяется случайно. Второе — в зависимости от того, насколько долго мы хотим, чтобы наш алгоритм работал. **Главное правило**: функция температуры $T(k)$ обязательно должна быть убывающей.\n",
    "\n",
    "У метода отжига есть ряд **плюсов и минусов**.\n",
    "\n",
    "# +\n",
    "\n",
    "* Не использует градиент. Это значит, что его можно использовать для функций, которые не являются непрерывно дифференцируемыми.\n",
    "\n",
    "* Можно использовать даже для дискретных функций. Причём именно для дискретных функций метод подходит очень хорошо.\n",
    "\n",
    "* Прост в реализации и использовании.\n",
    "\n",
    "# -\n",
    "\n",
    "* Сложно настраивать под задачу из-за множества параметров.\n",
    "\n",
    "* Нет гарантий сходимости.\n",
    "\n",
    "* Выполнение может занимать много времени\n",
    "\n",
    "Метод отжига используется не очень часто, однако он довольно известен, и следует иметь о нём общее представление.\n",
    "\n",
    "Если вам интересно подробнее узнать про метод отжига и посмотреть на его имплементацию в `Python`, рекомендуем ознакомиться со [следующей статьёй](https://dev.to/cesarwbr/how-to-implement-simulated-annealing-algorithm-in-python-4gid)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## МЕТОД КООРДИНАТНОГО СПУСКА\n",
    "\n",
    "Ещё один метод, про который необходимо узнать и с которым вы сталкивались ранее, — это метод координатного спуска.\n",
    "\n",
    ">Это тип алгоритма оптимизации, который обычно используется для «сильно выпуклого» случая регрессии — регрессионной функции Lasso и для регрессионной модели Elastic Net.\n",
    "\n",
    "Метод координатного спуска можно считать одним из простейших методов оптимизации , который  в целом достаточно эффективно ищет локальные минимумы для гладких функций.\n",
    "\n",
    "При поиске минимума с помощью этого метода мы всегда изменяем положение точки в направлении осей координат, т. е. всегда изменяется только одна координата, и благодаря этому задача становится одномерной.\n",
    "\n",
    "Если визуализировать работу алгоритма, то мы увидим, что за счёт того, что каждый шаг происходит параллельно одной или другой координатной оси, ход алгоритма становится похож на «лесенку»:\n",
    "\n",
    "<img src=m6_img18.png width=400>\n",
    "\n",
    "По сути, мы просто выбираем некоторую точку и правило изменения координаты и движемся в соответствии с ним до локального минимума.\n",
    "\n",
    "Посмотрим на основные **отличия координатного и градиентного спусков**:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| КООРДИНАТНЫЙ СПУСК | ГРАДИЕНТНЫЙ СПУСК |\n",
    "| - | - |\n",
    "| При минимизации меняет одну координату, фиксируя другие. | Меняет значения сразу всех координат. |\n",
    "| Используется для сильно выпуклой функции. Применимо для функций, у которых нет решений в замкнутой форме (например, `Lasso`-регрессия). | Применяется для функций, у которых есть решение в замкнутой форме (например, метод наименьших квадратов). |\n",
    "| Не требует настройки гиперпараметра, определяющего темп обучения. | Требует настройки гиперпараметра, определяющего темп обучения. |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Итоги <a class=\"anchor\" id=8></a>\n",
    "\n",
    "[к содержанию](#0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Путь решения любой задачи оптимизации:\n",
    "\n",
    "1. Формулирование задачи как задачи оптимизации. На данном шаге вам необходимо понять, к какому типу относится ваша задача, определить целевую функцию и функции ограничений (если они есть).\n",
    "\n",
    "2. Определение метода, который будет использоваться. Здесь вам надо выбрать наиболее подходящий метод оптимизации, понимая особенности вашей задачи и ограничения известных вам методов.\n",
    "\n",
    "3. Определение начальной точки. Скорее всего, вы уже запомнили, что реализация каждого алгоритма начиналась с некоторой точки отсчёта, от которой зависит результат. Часто алгоритмы «запускают» сразу из нескольких точек (как, например, в случае градиентного спуска).\n",
    "\n",
    "4. Переход в следующую точку. Здесь вы вычисляете следующую точку, используя правило алгоритма, который вы выбрали.\n",
    "\n",
    "5. Повторение предыдущего шага до сходимости алгоритма, то есть до того момента, пока не достигнете нуля или нужной точности.\n",
    "\n",
    "*** \n",
    "\n",
    "Скорее всего, при решении задач вы заметили, что самое главное — это сформулировать вашу практическую задачу в необходимых терминах, перевести её на язык математической модели, а затем подобрать нужный алгоритм. Сами методы уже реализованы в готовых библиотеках Python, и если вы понимаете их суть и ограничения, то для своих задач без проблем можете пользоваться уже готовыми решениями.\n",
    "\n",
    "ДОПОЛНИТЕЛЬНО\n",
    "\n",
    "Если вам интересно углубиться в тему оптимизации, в качестве дополнительных материалов можем рекомендовать следующее:\n",
    "\n",
    "* поработать с [визуализацией к задаче коммивояжёра](https://www.michalfudala.com/projects/simanneal/);\n",
    "\n",
    "* ознакомиться с [реализацией метода имитации отжига в Python](https://habr.com/ru/post/112189/);\n",
    "\n",
    "* обратиться к [бесплатному курсу \"Combinatorial Optimization\"](https://ocw.mit.edu/courses/mathematics/18-433-combinatorial-optimization-fall-2003/);\n",
    "\n",
    "* подробнее ознакомиться с процессом оптимизации в [официальной документации к PuLP](https://pythonhosted.org/PuLP/index.html);\n",
    "\n",
    "* изучить [обзор разных методов на основе градиентного спуска](http://ruder.io/optimizing-gradient-descent/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
