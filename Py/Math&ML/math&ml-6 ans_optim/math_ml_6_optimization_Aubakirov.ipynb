{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NyHIGVhfa_Wf"
      },
      "source": [
        "# Стохастический градиентный и координатный спуски"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gn6lluIADUKa"
      },
      "source": [
        "Для каждого задания указано количество баллов (если они оцениваются отдельно) + 1 балл за аккуратное и полное выполнение всего задания"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "txCccYvha_Wv"
      },
      "source": [
        "## Загрузка и подготовка данных"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NbyOzeZ6a_Wx"
      },
      "source": [
        "**Загрузите уже знакомый вам файл *Advertising.csv* как объект DataFrame.** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "E1L4_xeDa_Wz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data = pd.read_csv('Advertising.zip')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bf4aVFndDUKf"
      },
      "source": [
        "**Проверьте, есть ли в данных пропуски и, если они есть - удалите их**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tiVeFnR5DUKg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Пропуски в данных: False\n"
          ]
        }
      ],
      "source": [
        "print(f'Пропуски в данных: {data.isnull().any().any()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>TV</th>\n",
              "      <th>radio</th>\n",
              "      <th>newspaper</th>\n",
              "      <th>sales</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>230.1</td>\n",
              "      <td>37.8</td>\n",
              "      <td>69.2</td>\n",
              "      <td>22.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>44.5</td>\n",
              "      <td>39.3</td>\n",
              "      <td>45.1</td>\n",
              "      <td>10.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>17.2</td>\n",
              "      <td>45.9</td>\n",
              "      <td>69.3</td>\n",
              "      <td>9.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>151.5</td>\n",
              "      <td>41.3</td>\n",
              "      <td>58.5</td>\n",
              "      <td>18.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>180.8</td>\n",
              "      <td>10.8</td>\n",
              "      <td>58.4</td>\n",
              "      <td>12.9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0     TV  radio  newspaper  sales\n",
              "0           1  230.1   37.8       69.2   22.1\n",
              "1           2   44.5   39.3       45.1   10.4\n",
              "2           3   17.2   45.9       69.3    9.3\n",
              "3           4  151.5   41.3       58.5   18.5\n",
              "4           5  180.8   10.8       58.4   12.9"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rTkiqPr_DUKh"
      },
      "source": [
        "**Преобразуйте ваши признаки в массивы NumPy и разделите их на переменные X (предикторы) и y(целевая переменная)** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "R9OHIRB3a_Xa"
      },
      "outputs": [],
      "source": [
        "# Предикторы\n",
        "X = np.array(data[['TV','radio','newspaper']])\n",
        "# Целевая переменная\n",
        "y = np.array(data['sales'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KCvjSoHEDUKo"
      },
      "source": [
        "## Координатный спуск (3 балла)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yjNm8dATDUKq"
      },
      "source": [
        "**Добавим единичный столбец для того, чтобы у нас был свободный коэффициент в уравнении регрессии:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LMgq0fmKDUKr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(200, 4) (200, 1)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "X = np.hstack([np.ones(X.shape[0]).reshape(-1, 1), X])\n",
        "y = y.reshape(-1, 1)\n",
        "print(X.shape, y.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "R008OQwcDUKt"
      },
      "source": [
        "**Нормализуем данные: обычно это необходимо для корректной работы алгоритма**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2Sk7Wx-SDUKt"
      },
      "outputs": [],
      "source": [
        "X = X / np.sqrt(np.sum(np.square(X), axis=0))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "F_pHHbAdDUKu"
      },
      "source": [
        "**Реализуйте алгоритм координатного спуска:** (3 балла)\n",
        "\n",
        "Ниже приведен алгоритм координатного спуска для случая нормализованных данных:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Задано:**\n",
        "\n",
        "* $X=(x_{ij})$ - матрица наблюдений, размерностью $dim(X)=(n, m)$\n",
        "* $N=1000$ - количество итераций\n",
        "\n",
        "**Примечание:** *1000 итераций здесь указаны для этого задания, на самом деле их может быть намного больше, нет детерменированного значения.*\n",
        "\n",
        "**Алгоритм (математическая запись):**\n",
        "* Создать нулевой вектор параметров $w_0=(0, 0,..., 0)^T$\n",
        "* Для всех $t=1, 2, ..., N$ итераций:\n",
        "    * Для всех $k = 1, 2,..., m$:\n",
        "        * Фиксируем значение всех признаков, кроме $k$-ого и вычисляем прогноз модели линейной регрессии.Для этого исключаем признак $k$-ый из данных и $w_j$ из параметров при построении прогноза.\n",
        "        Математически это можно записать следующим образом:\n",
        "\n",
        "        $$h_i = \\sum_{j=1}^{k-1} x_{ij}w_{j} + \\sum_{j=k+1}^{m} x_{ij}w_j $$\n",
        "\n",
        "        **Примечание:**\n",
        "        \n",
        "        *Обратите, что в данной записи текущий признак под номером $k$ не участвует в сумме.Сравните эту запись с классической записью прогноза линейной регрессии в случае нормированных данных (когда участвуют все признаки):*\n",
        "\n",
        "        $$h_i = \\sum_{j=1}^{m} x_{ij}w_{j}$$ \n",
        "        \n",
        "        * Вычисляем новое значение параметра $k$-ого коэффициента: \n",
        "        $$w_k = \\sum_{i=1}^{n} x_{ik} (y_i - h_i) = x_k^T(y-h) $$\n",
        "\n",
        "    * Вычисляем значение функции потерь и сохраняем в историю изменения функции потерь (В оценке функции потерь участвуют все признаки):\n",
        "        $$\\hat{y_i} = \\sum_{j=1}^{m}x_{ij}$$\n",
        "        $$Loss_t = \\frac{1}{n} \\sum_{i=1}^{n}(y_i-\\hat{y_i})^2$$\n",
        "        \n",
        "        или в векторном виде:\n",
        "        \n",
        "        $$\\hat{y} = Xw$$\n",
        "        $$Loss_t = \\frac{1}{n}(y-\\hat{y})^T(y-\\hat{y})$$\n",
        "    \n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Алгоритм (псевдокод):**\n",
        "```python\n",
        "\n",
        "num_iters = #количество итераций\n",
        "m = # количество строк в матрице X\n",
        "n = # количество столбцов в матрице X\n",
        "w = #вектор размера nx1, состояющий из нулей\n",
        "\n",
        "for i in range(num_iters):\n",
        "    for k in range(n):\n",
        "        # Вычисляем прогноз без k-ого фактора\n",
        "        h = (X[:,0:k] @ w[0:k]) + (X[:,k+1:] @ w[k+1:])\n",
        "        # Обновляем новое значение k-ого коэффициента\n",
        "        w[k] =  (X[:,k].T @ (y - h))\n",
        "        # Вычисляем функцию потерь\n",
        "        cost = sum((X @ w) - y) ** 2)/(len(y))\n",
        "\n",
        "```"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "y3IdiHm9DUKv"
      },
      "source": [
        "Вам необходимо реализовать координатный спуск, и вывести веса в модели линейной регрессии."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 41.56217205, 110.13144155,  73.52860638,  -0.55006384])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_iters = 130 # опытным путём добиваемся минимальной разницы с предсказаниями модели за меньшее число итерации\n",
        "m = X.shape[0]\n",
        "n = X.shape[1]\n",
        "w = np.zeros((n,1))\n",
        "for i in range(num_iters):\n",
        "\n",
        "    for k in range(n):\n",
        "        # Вычисляем прогноз без k-ого фактора\n",
        "        h = (X[:,0:k] @ w[0:k]) + (X[:,k+1:] @ w[k+1:])\n",
        "        # Обновляем новое значение k-ого коэффициента\n",
        "        w[k] =  (X[:,k].T @ (y - h))\n",
        "        # Вычисляем функцию потерь\n",
        "        cost = sum(((X @ w) - y) ** 2)/(len(y))\n",
        "  \n",
        "w = w.T[0]\n",
        "w"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "a3jG-7UADUKx"
      },
      "source": [
        "Сравните результаты с реализацией линейной регрессии из библиотеки sklearn:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "SBl-1Yb5DUKy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 41.56217205 110.13144155  73.52860638  -0.55006384]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        " \n",
        "model = LinearRegression(fit_intercept=False)\n",
        "model.fit(X, y)\n",
        " \n",
        "print(model.coef_)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hIl0AGLyDUKy"
      },
      "source": [
        "Если вы все сделали верно, они должны практически совпасть!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VCGwFnPdDUKz"
      },
      "source": [
        "## Стохастический градиентный спуск (6 баллов)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5u7Q2YJla_Xk"
      },
      "source": [
        "**Отмасштабируйте столбцы исходной матрицы *X* (которую мы не нормализовали еще!). Для того, чтобы это сделать, надо вычесть из каждого значения среднее и разделить на стандартное отклонение** (0.5 баллов)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Среднее масштабированного массива:: 0\n",
            "Стандартные отклонения масштабированного массива: 1\n"
          ]
        }
      ],
      "source": [
        "# Вернём изначальную (ненормализованную матрицу)\n",
        "X = np.array(data[['TV','radio','newspaper']])\n",
        "X = (X - X.mean(axis=0)) / X.std(axis=0) \n",
        "     \n",
        "print('Среднее масштабированного массива:: %.0f'%(abs(X.mean())))\n",
        "print('Стандартные отклонения масштабированного массива: %.0f'%(X.std()))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8WkNYILHDUK1"
      },
      "source": [
        "**Добавим единичный столбец**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "EVl5tEGtDUK1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(200, 4)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X = np.hstack([np.ones(X.shape[0]).reshape(-1, 1), X])\n",
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.shape[1]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "m53tZA5fDUK1"
      },
      "source": [
        "**Создайте функцию mse_error для вычисления среднеквадратичной ошибки, принимающую два аргумента: реальные значения и предсказывающие, и возвращающую значение mse** (0.5 балла)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0cvtC08Aa_YK"
      },
      "outputs": [],
      "source": [
        "def mse_error(y_real, y_pred):\n",
        "    \"\"\"_Функция вычисления среднеквадратичной ошибки_\n",
        "\n",
        "    Args:\n",
        "        y_real (_float_): _реальное значение_\n",
        "        y_pred (_float_): _предсказанное значение_\n",
        "    Returns:\n",
        "        result (_float_): _значение MSE_\n",
        "    \"\"\"\n",
        "    return np.square(np.subtract(y_real, y_pred)).mean()\n",
        "\n",
        "def mae_error(y_real, y_pred):\n",
        "    \"\"\"_Функция вычисления абсолютной ошибки_\n",
        "\n",
        "    Args:\n",
        "        y_real (_float_): _реальное значение_\n",
        "        y_pred (_float_): _предсказанное значение_\n",
        "    Returns:\n",
        "        result (_float_): _значение MSE_\n",
        "    \"\"\"\n",
        "    return np.mean(np.abs(y_real - y_pred))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lpOLhdvBDUK2"
      },
      "source": [
        "**Сделайте наивный прогноз: предскажите продажи средним значением. После этого рассчитайте среднеквадратичную ошибку для этого прогноза** (0.5 балла)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "kLV_XljVa_YZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MSE: 27.085743750000002\n",
            "MAE: 4.279974999999999\n"
          ]
        }
      ],
      "source": [
        "# Скопируем массив ответов\n",
        "y_pred = y.copy()\n",
        "# заполним его средним значением\n",
        "y_pred.fill(y_pred.mean())\n",
        "# посчитаем MSE\n",
        "mse_naive = mse_error(y, y_pred)\n",
        "print(f'MSE: {mse_error(y, y_pred)}')\n",
        "print(f'MAE: {mae_error(y, y_pred)}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BbybL2ola_ZM"
      },
      "source": [
        "**Создайте функцию *lin_pred*, которая может по матрице предикторов *X* и вектору весов линейной модели *w* получить вектор прогнозов** (0.5 балла)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MSE: 2.784126314510936\n",
            "MAE: 1.2520112296870676\n"
          ]
        }
      ],
      "source": [
        "# Обновим веса w для отмасштабированных данных\n",
        "def lin_reg(X, y):\n",
        "    a = np.dot(X.T, X)\n",
        "    b = np.dot(X.T, y)\n",
        "    return np.linalg.solve(a, b)\n",
        "\n",
        "w = lin_reg(X, y).T\n",
        "\n",
        "def lin_pred(X,w):\n",
        "    \"\"\"_Функция получит предсказания по весам линейной модели_\n",
        "\n",
        "    Args:\n",
        "        X (_array_): _матрица предикторов_\n",
        "        w (_array_): _вектор весов линейной модели_\n",
        "\n",
        "    Returns:\n",
        "        _array_: _вектор прогнозов_\n",
        "    \"\"\"\n",
        "    res = np.zeros([len(X),1])\n",
        "    for i in range(len(X)):\n",
        "        res[i] = (np.dot(w,X[i]))\n",
        "    return res\n",
        "\n",
        "y_pred = lin_pred(X, w)\n",
        "print(f'MSE: {mse_error(y, y_pred)}')\n",
        "print(f'MAE: {mae_error(y, y_pred)}')\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BU4adBrya_Zm"
      },
      "source": [
        "**Создайте функцию *stoch_grad_step* для реализации шага стохастического градиентного спуска. (1.5 балла) \n",
        "Функция должна принимать на вход следующие аргументы:**\n",
        "* матрицу *X*\n",
        "* вектора *y* и *w*\n",
        "* число *train_ind* - индекс объекта обучающей выборки (строки матрицы *X*), по которому считается изменение весов\n",
        "* число *$\\eta$* (eta) - шаг градиентного спуска\n",
        "\n",
        "Результатом будет вектор обновленных весов"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dyLY-P02DUK5"
      },
      "source": [
        "Шаг для стохастического градиентного спуска выглядит следующим образом:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ORsAyIKNDUK5"
      },
      "source": [
        "$$\\Large w_j \\leftarrow w_j - \\frac{2\\eta}{\\ell} \\sum_{i=1}^\\ell{{x_{ij}((w_0 + w_1x_{i1} + w_2x_{i2} +  w_3x_{i3}) - y_i)}}$$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CQl2FrpuDUK6"
      },
      "source": [
        "Для того, чтобы написать функцию, нужно сделать следующее:\n",
        "    \n",
        "*  посчитать направление изменения: умножить объект обучающей выборки на 2 и на разницу между предсказанным значением и реальным, а потом поделить на количество элементов в выборке.\n",
        "* вернуть разницу между вектором весов и направлением изменения, умноженным на шаг градиентного спуска"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "YUhVQGsja_Zn"
      },
      "outputs": [],
      "source": [
        "def stoch_grad_step(X, y, w, train_ind, eta=0.01):\n",
        "    \"\"\"_summary_\n",
        "\n",
        "    Args:\n",
        "        X (_array_): _матрица предикторов_\n",
        "        y (_array_): _вектор ответов_\n",
        "        w (_array_): _вектор весов_\n",
        "        train_ind (_int _): _индекс объекта_\n",
        "        eta (_float_): _шаг градиентного спуска_\n",
        "    Returns:\n",
        "        _array_: _обновленный вектор весов_\n",
        "    \"\"\"\n",
        "    w_new = 2 * eta * X[train_ind] * (np.dot(w, X[train_ind]) - y[train_ind]) / y[train_ind].size\n",
        "    \n",
        "    return (w - w_new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[14.05402051,  3.94982389,  2.82300083,  0.03353466]])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stoch_grad_step(X, y, w, 0)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pXwIFd0Ma_Zx"
      },
      "source": [
        "**Создайте функцию *stochastic_gradient_descent*, для реализации стохастического градиентного спуска (2.5 балла)**\n",
        "\n",
        "**Функция принимает на вход следующие аргументы:**\n",
        "- Матрицу признаков X\n",
        "- Целевую переменнную\n",
        "- Изначальную точку (веса модели)\n",
        "- Параметр, определяющий темп обучения\n",
        "- Максимальное число итераций\n",
        "- Евклидово расстояние между векторами весов на соседних итерациях градиентного спуска,при котором алгоритм прекращает работу \n",
        "\n",
        "**На каждой итерации в вектор (список) должно записываться текущее значение среднеквадратичной ошибки. Функция должна возвращать вектор весов $w$, а также вектор (список) ошибок.**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVeoNF1JDUK7"
      },
      "source": [
        "Алгоритм сследующий:\n",
        "    \n",
        "* Инициализируйте расстояние между векторами весов на соседних итерациях большим числом (можно бесконечностью)\n",
        "* Создайте пустой список для фиксации ошибок\n",
        "* Создайте счетчик итераций\n",
        "* Реализуйте оновной цикл обучения пока расстояние между векторами весов больше того, при котором надо прекратить работу (когда расстояния станут слишком маленькими - значит, мы застряли в одном месте) и количество итераций меньше максимально разрешенного: сгенерируйте случайный индекс, запишите текущую ошибку в вектор ошибок, запишите в переменную текущий шаг стохастического спуска с использованием функции, написанной ранее. Далее рассчитайте текущее расстояние между векторами весов и прибавьте к счетчику итераций 1.\n",
        "* Верните вектор весов и вектор ошибок"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "CD_xcFNfa_Zy"
      },
      "outputs": [],
      "source": [
        "def stochastic_gradient_descent(X, y, w, eta=0.1, max_iter=10000, min_distance=0.000001):\n",
        "    \"\"\"_Функция реализующая стохастический градиентный спуск_\n",
        "\n",
        "    Args:\n",
        "        X (_array_): _матрица предикторов_\n",
        "        y (_array_): _вектор ответов_\n",
        "        w (_array_): _вектор весов_\n",
        "        eta (_float_): _шаг градиентного спуска_\n",
        "        max_iter (_type_): _максимальное количество итерации_\n",
        "        min_distance (_type_): _минимальное расстояние между векторами весов_\n",
        "    \"\"\"\n",
        "    distance = np.inf\n",
        "    \n",
        "    w_init = w\n",
        "    \n",
        "    mse_errors = []\n",
        "    \n",
        "    mae_errors = []\n",
        "    \n",
        "    w_value = []\n",
        "    \n",
        "    iter_count = 0\n",
        "    \n",
        "    np.random.seed(42)\n",
        "    \n",
        "    while distance > min_distance and iter_count < max_iter:\n",
        "        random_ind = np.random.randint(X.shape[0])\n",
        "        distance = np.linalg.norm(w-stoch_grad_step(X, y, w, random_ind, eta))\n",
        "        w_new = stoch_grad_step(X, y, w, random_ind, eta)\n",
        "        mse_errors.append(mse_error(y, lin_pred(X, w_new)))\n",
        "        mae_errors.append(mae_error(y, lin_pred(X, w_new)))\n",
        "        w_value.append(w)\n",
        "        w = w_new\n",
        "        iter_count += 1\n",
        "        \n",
        "    return w, w_value, mse_errors, mae_errors, iter_count"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0OqHO1Rta_Z7"
      },
      "source": [
        " **Запустите $10^5$ итераций стохастического градиентного спуска. Укажите вектор начальных весов, состоящий из нулей. Можете поэкспериментировать с параметром, отвечающим за темп обучения.**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "N6fHHT6vDUK8"
      },
      "source": [
        "**Постройте график зависимости ошибки от номера итерации**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "zsSfHDzLDUK9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 0 ns\n",
            "Wall time: 0 ns\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'График зависимости ошибок (MSE/MAE) от номера итерации')"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%time\n",
        "w, w_value, mse_errors, mae_errors, iter_count = stochastic_gradient_descent(X, y, np.zeros(4), 0.01, 10000, 0.0001)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure(figsize=(5, 5))\n",
        "ax = fig.add_axes([0.3,0.3,1,1])\n",
        "ax.plot(mse_errors, c='k', linewidth=4)\n",
        "ax.plot(mae_errors, c='r', linewidth=4)\n",
        "ax.set_xlim(0,150)\n",
        "ax.hlines(np.array(mse_errors).min(),0,200, color='g', linestyle='--')\n",
        "ax.text(5, 5, f'MSE: {round(np.array(mse_errors).min(),3)}', fontsize = 16, color='g')\n",
        "ax.hlines(np.array(mae_errors).min(),0,200, color='b', linestyle='--')\n",
        "ax.text(5, -4.2, f'MAE: {round(np.array(mae_errors).min(),3)}', fontsize = 16, color='b')\n",
        "ax.text(5, 15, f'MAE', fontsize = 22, color='r')\n",
        "ax.text(5.5, 190, f'MSE', fontsize = 22, color='k')\n",
        "ax.set_title('График зависимости ошибок (MSE/MAE) от номера итерации',fontsize = 18)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "O-MVOcJ6a_aY"
      },
      "source": [
        "**Выведите вектор весов, к которому сошелся метод.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPjVkXe4DUK9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Вектор весов, к которому сошелся метод: [13.90038612  3.39509747  2.75291822 -0.17464376]\n"
          ]
        }
      ],
      "source": [
        "print(f'Вектор весов, к которому сошелся метод: {w}')\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qabzMc3Qa_a5"
      },
      "source": [
        "**Выведите среднеквадратичную ошибку на последней итерации.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "На 1218 итерации модель остановилась с МАЕ: 3.113944644221084\n"
          ]
        }
      ],
      "source": [
        "print(f'На {iter_count} итерации модель остановилась с МАЕ: {mse_errors[-1]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Минимальная целевая метрика достигнута на 407 итерации, и составила 2.786943984005956 \n",
            "Веса линейной регрессии при этом составили [14.07369597  4.05173702  2.77799779 -0.09698114]\n"
          ]
        }
      ],
      "source": [
        "# Для поиска итерации на которой достигнут минимум целевой метрики используем Pandas\n",
        "model_data = pd.DataFrame({'MSE':mse_errors, 'w_value':w_values})\n",
        "w_optimal = model_data.iloc[model_data['MSE'].idxmin()][1]\n",
        "\n",
        "print(f'Минимальная целевая метрика достигнута на {model_data[\"MSE\"].idxmin()} итерации, и составила {mse_errors[model_data[\"MSE\"].idxmin()]} \\nВеса линейной регрессии при этом составили {w_optimal}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***\n",
        "если проще - градиентный спуск с учётом предыдущего шага\n",
        "***\n",
        "Формально можно записать, что для того, чтобы попасть в следующую точку $x_1$, необходимо перейти из начальной точки $x_0$ на антиградиент, домноженный на некоторый коэффициент, который называется **шагом градиентного спуска** или **темпом обучения** — о нём мы поговорим немного позже.\n",
        "\n",
        "$x_1 = x_0 - \\alpha \\nabla f (x_0)$\n",
        "\n",
        "$\\alpha$ - темп обучения\n",
        "\n",
        "$\\nabla f = \\left ( \\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y} \\right )$ - вектор градиента:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# определим функцию\n",
        "def fun(x, y, a=1, b=1):\n",
        "    return a * (x ** 2) + b * (y ** 2)\n",
        "\n",
        "# определим градиент\n",
        "def grad(x, y, a=1, b=1):\n",
        "    return np.array([2 * a * x, 2 * b * y])\n",
        "\n",
        "# простой градиентный спуск\n",
        "def grad_descend(grad, step_size=0.2, num_steps=30):\n",
        "    lst = []\n",
        "    x = np.random.uniform(0, 3, size = 2)\n",
        "    lst.append(x)\n",
        "    for i in range(num_steps):\n",
        "        x = x - step_size * grad(lst[-1][0], lst[-1][1])\n",
        "        lst.append(x)\n",
        "    return np.array(lst)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Градиентный спуск с `momentum`. Формально его можно определить следующим образом:\n",
        "\n",
        "$x_{n+1}=x_n - \\alpha\\nabla f(x_n) + \\gamma(x_n - x_{n-1})$\n",
        "\n",
        "В формуле выше $\\gamma$ — это параметр, который показывает, насколько учитывается предыдущий шаг.\n",
        "\n",
        "Для примера найдём минимум функции $2x^2 - 4xy + y^4 + 2$ с помощью градиентного спуска."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iter=1; x=(0.0800, 0.9200); f(x)=2.4348; grad f(x)=(-3.3600, 2.7948)\n",
            "iter=2; x=(0.1472, 0.8641); f(x)=2.0921; grad f(x)=(-2.8676, 1.9920)\n",
            "iter=3; x=(0.2046, 0.8243); f(x)=1.8709; grad f(x)=(-2.4788, 1.4218)\n",
            "iter=4; x=(0.2541, 0.7958); f(x)=1.7213; grad f(x)=(-2.1668, 0.9996)\n",
            "iter=5; x=(0.2975, 0.7758); f(x)=1.6161; grad f(x)=(-1.9135, 0.6781)\n",
            "iter=6; x=(0.3357, 0.7623); f(x)=1.5394; grad f(x)=(-1.7062, 0.4288)\n",
            "iter=7; x=(0.3699, 0.7537); f(x)=1.4812; grad f(x)=(-1.5354, 0.2332)\n",
            "iter=8; x=(0.4006, 0.7490); f(x)=1.4355; grad f(x)=(-1.3939, 0.0787)\n",
            "iter=9; x=(0.4284, 0.7475); f(x)=1.3983; grad f(x)=(-1.2761, -0.0434)\n",
            "iter=10; x=(0.4540, 0.7483); f(x)=1.3669; grad f(x)=(-1.1775, -0.1396)\n",
            "iter=11; x=(0.4775, 0.7511); f(x)=1.3397; grad f(x)=(-1.0944, -0.2150)\n",
            "iter=12; x=(0.4994, 0.7554); f(x)=1.3154; grad f(x)=(-1.0241, -0.2733)\n",
            "iter=13; x=(0.5199, 0.7609); f(x)=1.2935; grad f(x)=(-0.9640, -0.3175)\n",
            "iter=14; x=(0.5392, 0.7672); f(x)=1.2732; grad f(x)=(-0.9123, -0.3501)\n",
            "iter=15; x=(0.5574, 0.7742); f(x)=1.2545; grad f(x)=(-0.8673, -0.3732)\n",
            "iter=16; x=(0.5748, 0.7817); f(x)=1.2369; grad f(x)=(-0.8278, -0.3884)\n",
            "iter=17; x=(0.5913, 0.7895); f(x)=1.2205; grad f(x)=(-0.7926, -0.3971)\n",
            "iter=18; x=(0.6072, 0.7974); f(x)=1.2050; grad f(x)=(-0.7610, -0.4005)\n",
            "iter=19; x=(0.6224, 0.8054); f(x)=1.1904; grad f(x)=(-0.7321, -0.3996)\n",
            "iter=20; x=(0.6370, 0.8134); f(x)=1.1767; grad f(x)=(-0.7055, -0.3954)\n",
            "iter=21; x=(0.6511, 0.8213); f(x)=1.1638; grad f(x)=(-0.6807, -0.3884)\n",
            "iter=22; x=(0.6648, 0.8291); f(x)=1.1517; grad f(x)=(-0.6573, -0.3794)\n",
            "iter=23; x=(0.6779, 0.8367); f(x)=1.1404; grad f(x)=(-0.6351, -0.3688)\n",
            "iter=24; x=(0.6906, 0.8441); f(x)=1.1298; grad f(x)=(-0.6138, -0.3571)\n",
            "iter=25; x=(0.7029, 0.8512); f(x)=1.1199; grad f(x)=(-0.5933, -0.3446)\n",
            "iter=26; x=(0.7147, 0.8581); f(x)=1.1106; grad f(x)=(-0.5734, -0.3317)\n",
            "iter=27; x=(0.7262, 0.8647); f(x)=1.1020; grad f(x)=(-0.5540, -0.3185)\n",
            "iter=28; x=(0.7373, 0.8711); f(x)=1.0940; grad f(x)=(-0.5352, -0.3052)\n",
            "iter=29; x=(0.7480, 0.8772); f(x)=1.0865; grad f(x)=(-0.5168, -0.2921)\n",
            "iter=30; x=(0.7583, 0.8830); f(x)=1.0796; grad f(x)=(-0.4988, -0.2791)\n",
            "iter=31; x=(0.7683, 0.8886); f(x)=1.0732; grad f(x)=(-0.4812, -0.2665)\n",
            "iter=32; x=(0.7779, 0.8939); f(x)=1.0673; grad f(x)=(-0.4641, -0.2542)\n",
            "iter=33; x=(0.7872, 0.8990); f(x)=1.0618; grad f(x)=(-0.4473, -0.2423)\n",
            "iter=34; x=(0.7962, 0.9039); f(x)=1.0567; grad f(x)=(-0.4309, -0.2308)\n",
            "iter=35; x=(0.8048, 0.9085); f(x)=1.0520; grad f(x)=(-0.4149, -0.2198)\n",
            "iter=36; x=(0.8131, 0.9129); f(x)=1.0477; grad f(x)=(-0.3993, -0.2092)\n",
            "iter=37; x=(0.8211, 0.9171); f(x)=1.0437; grad f(x)=(-0.3841, -0.1991)\n",
            "iter=38; x=(0.8287, 0.9211); f(x)=1.0400; grad f(x)=(-0.3693, -0.1895)\n",
            "iter=39; x=(0.8361, 0.9248); f(x)=1.0367; grad f(x)=(-0.3549, -0.1803)\n",
            "iter=40; x=(0.8432, 0.9285); f(x)=1.0336; grad f(x)=(-0.3409, -0.1715)\n",
            "iter=41; x=(0.8500, 0.9319); f(x)=1.0307; grad f(x)=(-0.3274, -0.1632)\n",
            "iter=42; x=(0.8566, 0.9351); f(x)=1.0281; grad f(x)=(-0.3142, -0.1552)\n",
            "iter=43; x=(0.8629, 0.9382); f(x)=1.0257; grad f(x)=(-0.3015, -0.1477)\n",
            "iter=44; x=(0.8689, 0.9412); f(x)=1.0235; grad f(x)=(-0.2892, -0.1405)\n",
            "iter=45; x=(0.8747, 0.9440); f(x)=1.0215; grad f(x)=(-0.2773, -0.1337)\n",
            "iter=46; x=(0.8802, 0.9467); f(x)=1.0196; grad f(x)=(-0.2658, -0.1272)\n",
            "iter=47; x=(0.8855, 0.9492); f(x)=1.0179; grad f(x)=(-0.2547, -0.1210)\n",
            "iter=48; x=(0.8906, 0.9517); f(x)=1.0163; grad f(x)=(-0.2440, -0.1152)\n",
            "iter=49; x=(0.8955, 0.9540); f(x)=1.0149; grad f(x)=(-0.2337, -0.1096)\n",
            "iter=50; x=(0.9002, 0.9561); f(x)=1.0136; grad f(x)=(-0.2238, -0.1043)\n",
            "iter=51; x=(0.9047, 0.9582); f(x)=1.0124; grad f(x)=(-0.2142, -0.0993)\n",
            "iter=52; x=(0.9090, 0.9602); f(x)=1.0113; grad f(x)=(-0.2050, -0.0945)\n",
            "iter=53; x=(0.9131, 0.9621); f(x)=1.0103; grad f(x)=(-0.1962, -0.0899)\n",
            "iter=54; x=(0.9170, 0.9639); f(x)=1.0094; grad f(x)=(-0.1877, -0.0856)\n",
            "iter=55; x=(0.9207, 0.9656); f(x)=1.0086; grad f(x)=(-0.1795, -0.0815)\n",
            "iter=56; x=(0.9243, 0.9672); f(x)=1.0078; grad f(x)=(-0.1717, -0.0776)\n",
            "iter=57; x=(0.9278, 0.9688); f(x)=1.0071; grad f(x)=(-0.1642, -0.0739)\n",
            "iter=58; x=(0.9310, 0.9703); f(x)=1.0065; grad f(x)=(-0.1569, -0.0703)\n",
            "iter=59; x=(0.9342, 0.9717); f(x)=1.0059; grad f(x)=(-0.1500, -0.0670)\n",
            "iter=60; x=(0.9372, 0.9730); f(x)=1.0054; grad f(x)=(-0.1434, -0.0638)\n",
            "iter=61; x=(0.9401, 0.9743); f(x)=1.0049; grad f(x)=(-0.1370, -0.0607)\n",
            "iter=62; x=(0.9428, 0.9755); f(x)=1.0045; grad f(x)=(-0.1309, -0.0578)\n",
            "iter=63; x=(0.9454, 0.9767); f(x)=1.0041; grad f(x)=(-0.1251, -0.0551)\n",
            "iter=64; x=(0.9479, 0.9778); f(x)=1.0037; grad f(x)=(-0.1195, -0.0525)\n",
            "iter=65; x=(0.9503, 0.9788); f(x)=1.0034; grad f(x)=(-0.1141, -0.0500)\n",
            "iter=66; x=(0.9526, 0.9798); f(x)=1.0031; grad f(x)=(-0.1090, -0.0476)\n",
            "iter=67; x=(0.9548, 0.9808); f(x)=1.0028; grad f(x)=(-0.1041, -0.0453)\n",
            "iter=68; x=(0.9568, 0.9817); f(x)=1.0026; grad f(x)=(-0.0994, -0.0432)\n",
            "iter=69; x=(0.9588, 0.9825); f(x)=1.0023; grad f(x)=(-0.0949, -0.0411)\n",
            "iter=70; x=(0.9607, 0.9834); f(x)=1.0021; grad f(x)=(-0.0906, -0.0392)\n",
            "iter=71; x=(0.9625, 0.9842); f(x)=1.0019; grad f(x)=(-0.0865, -0.0373)\n",
            "iter=72; x=(0.9643, 0.9849); f(x)=1.0017; grad f(x)=(-0.0825, -0.0356)\n",
            "iter=73; x=(0.9659, 0.9856); f(x)=1.0016; grad f(x)=(-0.0788, -0.0339)\n",
            "iter=74; x=(0.9675, 0.9863); f(x)=1.0014; grad f(x)=(-0.0752, -0.0323)\n",
            "iter=75; x=(0.9690, 0.9869); f(x)=1.0013; grad f(x)=(-0.0717, -0.0308)\n",
            "iter=76; x=(0.9704, 0.9875); f(x)=1.0012; grad f(x)=(-0.0685, -0.0293)\n",
            "iter=77; x=(0.9718, 0.9881); f(x)=1.0011; grad f(x)=(-0.0653, -0.0279)\n",
            "iter=78; x=(0.9731, 0.9887); f(x)=1.0010; grad f(x)=(-0.0623, -0.0266)\n",
            "iter=79; x=(0.9744, 0.9892); f(x)=1.0009; grad f(x)=(-0.0595, -0.0253)\n",
            "iter=80; x=(0.9755, 0.9897); f(x)=1.0008; grad f(x)=(-0.0567, -0.0241)\n",
            "iter=81; x=(0.9767, 0.9902); f(x)=1.0007; grad f(x)=(-0.0541, -0.0230)\n",
            "iter=82; x=(0.9778, 0.9907); f(x)=1.0007; grad f(x)=(-0.0517, -0.0219)\n",
            "iter=83; x=(0.9788, 0.9911); f(x)=1.0006; grad f(x)=(-0.0493, -0.0209)\n",
            "iter=84; x=(0.9798, 0.9915); f(x)=1.0006; grad f(x)=(-0.0470, -0.0199)\n",
            "iter=85; x=(0.9807, 0.9919); f(x)=1.0005; grad f(x)=(-0.0448, -0.0190)\n",
            "iter=86; x=(0.9816, 0.9923); f(x)=1.0005; grad f(x)=(-0.0428, -0.0181)\n",
            "iter=87; x=(0.9825, 0.9927); f(x)=1.0004; grad f(x)=(-0.0408, -0.0172)\n",
            "iter=88; x=(0.9833, 0.9930); f(x)=1.0004; grad f(x)=(-0.0389, -0.0164)\n",
            "iter=89; x=(0.9841, 0.9933); f(x)=1.0003; grad f(x)=(-0.0371, -0.0156)\n",
            "iter=90; x=(0.9848, 0.9937); f(x)=1.0003; grad f(x)=(-0.0354, -0.0149)\n",
            "iter=91; x=(0.9855, 0.9940); f(x)=1.0003; grad f(x)=(-0.0337, -0.0142)\n",
            "iter=92; x=(0.9862, 0.9942); f(x)=1.0003; grad f(x)=(-0.0322, -0.0135)\n",
            "iter=93; x=(0.9868, 0.9945); f(x)=1.0002; grad f(x)=(-0.0307, -0.0129)\n",
            "iter=94; x=(0.9874, 0.9948); f(x)=1.0002; grad f(x)=(-0.0293, -0.0123)\n",
            "iter=95; x=(0.9880, 0.9950); f(x)=1.0002; grad f(x)=(-0.0279, -0.0117)\n",
            "iter=96; x=(0.9886, 0.9952); f(x)=1.0002; grad f(x)=(-0.0266, -0.0112)\n",
            "iter=97; x=(0.9891, 0.9955); f(x)=1.0002; grad f(x)=(-0.0254, -0.0106)\n",
            "iter=98; x=(0.9896, 0.9957); f(x)=1.0001; grad f(x)=(-0.0242, -0.0101)\n",
            "iter=99; x=(0.9901, 0.9959); f(x)=1.0001; grad f(x)=(-0.0231, -0.0097)\n",
            "iter=100; x=(0.9906, 0.9961); f(x)=1.0001; grad f(x)=(-0.0220, -0.0092)\n",
            "iter=101; x=(0.9910, 0.9963); f(x)=1.0001; grad f(x)=(-0.0210, -0.0088)\n",
            "iter=102; x=(0.9914, 0.9964); f(x)=1.0001; grad f(x)=(-0.0200, -0.0084)\n",
            "iter=103; x=(0.9918, 0.9966); f(x)=1.0001; grad f(x)=(-0.0191, -0.0080)\n",
            "iter=104; x=(0.9922, 0.9968); f(x)=1.0001; grad f(x)=(-0.0182, -0.0076)\n",
            "iter=105; x=(0.9926, 0.9969); f(x)=1.0001; grad f(x)=(-0.0173, -0.0072)\n",
            "iter=106; x=(0.9929, 0.9971); f(x)=1.0001; grad f(x)=(-0.0165, -0.0069)\n",
            "iter=107; x=(0.9933, 0.9972); f(x)=1.0001; grad f(x)=(-0.0158, -0.0066)\n",
            "iter=108; x=(0.9936, 0.9973); f(x)=1.0001; grad f(x)=(-0.0150, -0.0063)\n",
            "iter=109; x=(0.9939, 0.9975); f(x)=1.0001; grad f(x)=(-0.0143, -0.0060)\n",
            "iter=110; x=(0.9942, 0.9976); f(x)=1.0000; grad f(x)=(-0.0137, -0.0057)\n",
            "iter=111; x=(0.9944, 0.9977); f(x)=1.0000; grad f(x)=(-0.0130, -0.0054)\n",
            "iter=112; x=(0.9947, 0.9978); f(x)=1.0000; grad f(x)=(-0.0124, -0.0052)\n",
            "iter=113; x=(0.9949, 0.9979); f(x)=1.0000; grad f(x)=(-0.0118, -0.0049)\n",
            "iter=114; x=(0.9952, 0.9980); f(x)=1.0000; grad f(x)=(-0.0113, -0.0047)\n",
            "iter=115; x=(0.9954, 0.9981); f(x)=1.0000; grad f(x)=(-0.0108, -0.0045)\n",
            "iter=116; x=(0.9956, 0.9982); f(x)=1.0000; grad f(x)=(-0.0102, -0.0043)\n",
            "iter=117; x=(0.9958, 0.9983); f(x)=1.0000; grad f(x)=(-0.0098, -0.0041)\n",
            "iter=118; x=(0.9960, 0.9983); f(x)=1.0000; grad f(x)=(-0.0093, -0.0039)\n",
            "iter=119; x=(0.9962, 0.9984); f(x)=1.0000; grad f(x)=(-0.0089, -0.0037)\n",
            "iter=120; x=(0.9964, 0.9985); f(x)=1.0000; grad f(x)=(-0.0085, -0.0035)\n",
            "iter=121; x=(0.9966, 0.9986); f(x)=1.0000; grad f(x)=(-0.0081, -0.0034)\n",
            "iter=122; x=(0.9967, 0.9986); f(x)=1.0000; grad f(x)=(-0.0077, -0.0032)\n",
            "iter=123; x=(0.9969, 0.9987); f(x)=1.0000; grad f(x)=(-0.0073, -0.0030)\n",
            "iter=124; x=(0.9970, 0.9988); f(x)=1.0000; grad f(x)=(-0.0070, -0.0029)\n",
            "iter=125; x=(0.9972, 0.9988); f(x)=1.0000; grad f(x)=(-0.0067, -0.0028)\n",
            "iter=126; x=(0.9973, 0.9989); f(x)=1.0000; grad f(x)=(-0.0064, -0.0026)\n",
            "iter=127; x=(0.9974, 0.9989); f(x)=1.0000; grad f(x)=(-0.0061, -0.0025)\n",
            "iter=128; x=(0.9975, 0.9990); f(x)=1.0000; grad f(x)=(-0.0058, -0.0024)\n",
            "iter=129; x=(0.9977, 0.9990); f(x)=1.0000; grad f(x)=(-0.0055, -0.0023)\n",
            "iter=130; x=(0.9978, 0.9991); f(x)=1.0000; grad f(x)=(-0.0052, -0.0022)\n",
            "iter=131; x=(0.9979, 0.9991); f(x)=1.0000; grad f(x)=(-0.0050, -0.0021)\n",
            "iter=132; x=(0.9980, 0.9992); f(x)=1.0000; grad f(x)=(-0.0048, -0.0020)\n",
            "iter=133; x=(0.9981, 0.9992); f(x)=1.0000; grad f(x)=(-0.0045, -0.0019)\n",
            "iter=134; x=(0.9982, 0.9992); f(x)=1.0000; grad f(x)=(-0.0043, -0.0018)\n",
            "iter=135; x=(0.9982, 0.9993); f(x)=1.0000; grad f(x)=(-0.0041, -0.0017)\n",
            "iter=136; x=(0.9983, 0.9993); f(x)=1.0000; grad f(x)=(-0.0039, -0.0016)\n",
            "iter=137; x=(0.9984, 0.9993); f(x)=1.0000; grad f(x)=(-0.0037, -0.0016)\n",
            "iter=138; x=(0.9985, 0.9994); f(x)=1.0000; grad f(x)=(-0.0036, -0.0015)\n",
            "iter=139; x=(0.9985, 0.9994); f(x)=1.0000; grad f(x)=(-0.0034, -0.0014)\n",
            "iter=140; x=(0.9986, 0.9994); f(x)=1.0000; grad f(x)=(-0.0032, -0.0013)\n",
            "iter=141; x=(0.9987, 0.9995); f(x)=1.0000; grad f(x)=(-0.0031, -0.0013)\n",
            "iter=142; x=(0.9987, 0.9995); f(x)=1.0000; grad f(x)=(-0.0029, -0.0012)\n",
            "iter=143; x=(0.9988, 0.9995); f(x)=1.0000; grad f(x)=(-0.0028, -0.0012)\n",
            "iter=144; x=(0.9989, 0.9995); f(x)=1.0000; grad f(x)=(-0.0027, -0.0011)\n",
            "iter=145; x=(0.9989, 0.9995); f(x)=1.0000; grad f(x)=(-0.0026, -0.0011)\n",
            "iter=146; x=(0.9990, 0.9996); f(x)=1.0000; grad f(x)=(-0.0024, -0.0010)\n",
            "iter=147; x=(0.9990, 0.9996); f(x)=1.0000; grad f(x)=(-0.0023, -0.0010)\n",
            "iter=148; x=(0.9991, 0.9996); f(x)=1.0000; grad f(x)=(-0.0022, -0.0009)\n",
            "iter=149; x=(0.9991, 0.9996); f(x)=1.0000; grad f(x)=(-0.0021, -0.0009)\n",
            "iter=150; x=(0.9991, 0.9996); f(x)=1.0000; grad f(x)=(-0.0020, -0.0008)\n",
            "iter=151; x=(0.9992, 0.9997); f(x)=1.0000; grad f(x)=(-0.0019, -0.0008)\n",
            "iter=152; x=(0.9992, 0.9997); f(x)=1.0000; grad f(x)=(-0.0018, -0.0008)\n",
            "iter=153; x=(0.9993, 0.9997); f(x)=1.0000; grad f(x)=(-0.0017, -0.0007)\n",
            "iter=154; x=(0.9993, 0.9997); f(x)=1.0000; grad f(x)=(-0.0017, -0.0007)\n",
            "iter=155; x=(0.9993, 0.9997); f(x)=1.0000; grad f(x)=(-0.0016, -0.0007)\n",
            "iter=156; x=(0.9994, 0.9997); f(x)=1.0000; grad f(x)=(-0.0015, -0.0006)\n",
            "iter=157; x=(0.9994, 0.9997); f(x)=1.0000; grad f(x)=(-0.0014, -0.0006)\n",
            "iter=158; x=(0.9994, 0.9998); f(x)=1.0000; grad f(x)=(-0.0014, -0.0006)\n",
            "iter=159; x=(0.9994, 0.9998); f(x)=1.0000; grad f(x)=(-0.0013, -0.0005)\n",
            "iter=160; x=(0.9995, 0.9998); f(x)=1.0000; grad f(x)=(-0.0012, -0.0005)\n",
            "iter=161; x=(0.9995, 0.9998); f(x)=1.0000; grad f(x)=(-0.0012, -0.0005)\n",
            "iter=162; x=(0.9995, 0.9998); f(x)=1.0000; grad f(x)=(-0.0011, -0.0005)\n",
            "iter=163; x=(0.9995, 0.9998); f(x)=1.0000; grad f(x)=(-0.0011, -0.0004)\n",
            "iter=164; x=(0.9996, 0.9998); f(x)=1.0000; grad f(x)=(-0.0010, -0.0004)\n",
            "iter=165; x=(0.9996, 0.9998); f(x)=1.0000; grad f(x)=(-0.0010, -0.0004)\n",
            "iter=166; x=(0.9996, 0.9998); f(x)=1.0000; grad f(x)=(-0.0009, -0.0004)\n",
            "iter=167; x=(0.9996, 0.9998); f(x)=1.0000; grad f(x)=(-0.0009, -0.0004)\n",
            "iter=168; x=(0.9996, 0.9999); f(x)=1.0000; grad f(x)=(-0.0008, -0.0004)\n",
            "iter=169; x=(0.9997, 0.9999); f(x)=1.0000; grad f(x)=(-0.0008, -0.0003)\n",
            "iter=170; x=(0.9997, 0.9999); f(x)=1.0000; grad f(x)=(-0.0008, -0.0003)\n",
            "iter=171; x=(0.9997, 0.9999); f(x)=1.0000; grad f(x)=(-0.0007, -0.0003)\n",
            "iter=172; x=(0.9997, 0.9999); f(x)=1.0000; grad f(x)=(-0.0007, -0.0003)\n",
            "iter=173; x=(0.9997, 0.9999); f(x)=1.0000; grad f(x)=(-0.0007, -0.0003)\n",
            "iter=174; x=(0.9997, 0.9999); f(x)=1.0000; grad f(x)=(-0.0006, -0.0003)\n",
            "iter=175; x=(0.9997, 0.9999); f(x)=1.0000; grad f(x)=(-0.0006, -0.0003)\n",
            "iter=176; x=(0.9998, 0.9999); f(x)=1.0000; grad f(x)=(-0.0006, -0.0002)\n",
            "iter=177; x=(0.9998, 0.9999); f(x)=1.0000; grad f(x)=(-0.0006, -0.0002)\n",
            "iter=178; x=(0.9998, 0.9999); f(x)=1.0000; grad f(x)=(-0.0005, -0.0002)\n",
            "iter=179; x=(0.9998, 0.9999); f(x)=1.0000; grad f(x)=(-0.0005, -0.0002)\n",
            "iter=180; x=(0.9998, 0.9999); f(x)=1.0000; grad f(x)=(-0.0005, -0.0002)\n",
            "iter=181; x=(0.9998, 0.9999); f(x)=1.0000; grad f(x)=(-0.0005, -0.0002)\n",
            "iter=182; x=(0.9998, 0.9999); f(x)=1.0000; grad f(x)=(-0.0004, -0.0002)\n",
            "iter=183; x=(0.9998, 0.9999); f(x)=1.0000; grad f(x)=(-0.0004, -0.0002)\n",
            "iter=184; x=(0.9998, 0.9999); f(x)=1.0000; grad f(x)=(-0.0004, -0.0002)\n",
            "iter=185; x=(0.9998, 0.9999); f(x)=1.0000; grad f(x)=(-0.0004, -0.0002)\n",
            "iter=186; x=(0.9998, 0.9999); f(x)=1.0000; grad f(x)=(-0.0004, -0.0001)\n",
            "iter=187; x=(0.9999, 0.9999); f(x)=1.0000; grad f(x)=(-0.0003, -0.0001)\n",
            "iter=188; x=(0.9999, 0.9999); f(x)=1.0000; grad f(x)=(-0.0003, -0.0001)\n",
            "iter=189; x=(0.9999, 0.9999); f(x)=1.0000; grad f(x)=(-0.0003, -0.0001)\n",
            "iter=190; x=(0.9999, 0.9999); f(x)=1.0000; grad f(x)=(-0.0003, -0.0001)\n",
            "iter=191; x=(0.9999, 1.0000); f(x)=1.0000; grad f(x)=(-0.0003, -0.0001)\n",
            "iter=192; x=(0.9999, 1.0000); f(x)=1.0000; grad f(x)=(-0.0003, -0.0001)\n",
            "iter=193; x=(0.9999, 1.0000); f(x)=1.0000; grad f(x)=(-0.0003, -0.0001)\n",
            "iter=194; x=(0.9999, 1.0000); f(x)=1.0000; grad f(x)=(-0.0002, -0.0001)\n",
            "iter=195; x=(0.9999, 1.0000); f(x)=1.0000; grad f(x)=(-0.0002, -0.0001)\n",
            "iter=196; x=(0.9999, 1.0000); f(x)=1.0000; grad f(x)=(-0.0002, -0.0001)\n",
            "iter=197; x=(0.9999, 1.0000); f(x)=1.0000; grad f(x)=(-0.0002, -0.0001)\n",
            "iter=198; x=(0.9999, 1.0000); f(x)=1.0000; grad f(x)=(-0.0002, -0.0001)\n",
            "iter=199; x=(0.9999, 1.0000); f(x)=1.0000; grad f(x)=(-0.0002, -0.0001)\n",
            "iter=200; x=(0.9999, 1.0000); f(x)=1.0000; grad f(x)=(-0.0002, -0.0001)\n",
            "iter=201; x=(0.9999, 1.0000); f(x)=1.0000; grad f(x)=(-0.0002, -0.0001)\n",
            "iter=202; x=(0.9999, 1.0000); f(x)=1.0000; grad f(x)=(-0.0002, -0.0001)\n",
            "iter=203; x=(0.9999, 1.0000); f(x)=1.0000; grad f(x)=(-0.0002, -0.0001)\n",
            "iter=204; x=(0.9999, 1.0000); f(x)=1.0000; grad f(x)=(-0.0002, -0.0001)\n",
            "iter=205; x=(0.9999, 1.0000); f(x)=1.0000; grad f(x)=(-0.0001, -0.0001)\n",
            "iter=206; x=(0.9999, 1.0000); f(x)=1.0000; grad f(x)=(-0.0001, -0.0001)\n",
            "iter=207; x=(0.9999, 1.0000); f(x)=1.0000; grad f(x)=(-0.0001, -0.0001)\n",
            "iter=208; x=(0.9999, 1.0000); f(x)=1.0000; grad f(x)=(-0.0001, -0.0001)\n",
            "iter=209; x=(0.9999, 1.0000); f(x)=1.0000; grad f(x)=(-0.0001, -0.0000)\n",
            "iter=210; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0001, -0.0000)\n",
            "iter=211; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0001, -0.0000)\n",
            "iter=212; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0001, -0.0000)\n",
            "iter=213; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0001, -0.0000)\n",
            "iter=214; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0001, -0.0000)\n",
            "iter=215; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0001, -0.0000)\n",
            "iter=216; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0001, -0.0000)\n",
            "iter=217; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0001, -0.0000)\n",
            "iter=218; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0001, -0.0000)\n",
            "iter=219; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0001, -0.0000)\n",
            "iter=220; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0001, -0.0000)\n",
            "iter=221; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0001, -0.0000)\n",
            "iter=222; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0001, -0.0000)\n",
            "iter=223; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0001, -0.0000)\n",
            "iter=224; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0001, -0.0000)\n",
            "iter=225; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0001, -0.0000)\n",
            "iter=226; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0001, -0.0000)\n",
            "iter=227; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=228; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=229; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=230; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=231; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=232; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=233; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=234; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=235; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=236; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=237; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=238; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=239; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=240; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=241; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=242; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=243; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=244; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=245; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=246; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=247; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=248; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=249; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=250; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=251; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=252; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=253; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=254; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=255; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=256; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=257; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=258; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=259; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=260; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=261; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=262; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=263; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=264; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=265; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=266; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=267; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=268; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=269; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=270; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=271; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=272; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=273; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=274; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=275; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=276; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=277; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=278; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=279; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=280; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=281; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=282; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=283; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=284; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=285; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=286; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=287; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=288; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=289; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=290; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=291; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=292; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=293; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=294; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=295; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=296; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=297; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=298; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=299; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=300; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n",
            "iter=301; x=(1.0000, 1.0000); f(x)=1.0000; grad f(x)=(-0.0000, -0.0000)\n"
          ]
        }
      ],
      "source": [
        "def f(x, y): # сама функция далее не вызывается\n",
        "    return 2*x**2-4*x*y + y**4 + 2 # но объявим её\n",
        "def grad(x, y): # вычислятель градиента\n",
        "    dx = 4*x-4*y # частная производная по х\n",
        "    dy = 4*y**3-4*x # частная производная по у\n",
        "    return (dx, dy)\n",
        "\n",
        "x0 = (0, 1) # начальная точка\n",
        "gamma = 0.02 # темп обучения\n",
        "x_cur = x0 # текущая точка (на 1ой итерации совпадает с начальной)\n",
        "\n",
        "vals = []\n",
        "coords = []\n",
        "i = 0\n",
        "while True: # запускаем бесконечный цикл\n",
        "    x_new = (x_cur[0] - gamma * grad(*x_cur)[0],\n",
        "            x_cur[1] - gamma * grad(*x_cur)[1]) # метод градиентного спуска посчитанный покординатно\n",
        "    if i > 300: # зададим конечное количество итераций\n",
        "        break\n",
        "    x_cur = x_new # перезапишем текущую точку\n",
        "    vals.append(f(*x_cur))\n",
        "    coords.append(x_cur)\n",
        "    i += 1 # добавим счётчик итераций\n",
        "    print(f\"iter={i}; x=({x_cur[0]:.4f}, {x_cur[1]:.4f});\"\n",
        "          f\" f(x)={f(*x_cur):.4f}; grad f(x)=({grad(*x_cur)[0]:.4f}, {grad(*x_cur)[1]:.4f})\") # выведем интересующие нас значения"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Практика_Оптимизация.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.10.8 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "86c56a74836ad344b00594bf6f38fa6a676a207ceefe20d101fbc465800ccb8d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
