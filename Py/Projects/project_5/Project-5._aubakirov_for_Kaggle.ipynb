{"cells":[{"cell_type":"markdown","metadata":{"id":"F6EDfWQLkBLX","tags":[]},"source":["## 1. Постановка задачи"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"G3NPCLzmkBLa"},"source":["\n","\n","<center> <img src=https://raw.githubusercontent.com/hoittoken/Python/master/Py/Projects/project_5/logo%20v1.1.png align=\"left\"> </center>"]},{"cell_type":"markdown","metadata":{"id":"4Up0IUABkBLc","tags":[]},"source":["## 2. Знакомство с данными, базовый анализ и расширение данных"]},{"cell_type":"markdown","metadata":{"id":"7W9mZPhWkBLd"},"source":["Начнём наше исследование со знакомства с предоставленными данными. А также подгрузим дополнительные источники данных и расширим наш исходный датасет. \n"]},{"cell_type":"markdown","metadata":{"id":"UTIxJupXkBLd"},"source":["Заранее импортируем модули, которые нам понадобятся для решения задачи:"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"yeC12P0hkBLe"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import plotly.express as px\n","\n","from scipy import stats\n","from sklearn import linear_model\n","from sklearn import preprocessing\n","from sklearn import model_selection\n","from sklearn import tree\n","from sklearn import ensemble\n","from sklearn import metrics\n","from sklearn import cluster\n","from sklearn import feature_selection\n","\n","from lightgbm import LGBMRegressor"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"kk5-kjqikBLf"},"source":["## Напишем всякие полезные функции:"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"2987aTUAkBLl"},"outputs":[],"source":["def add_datetime_features(data, feature='pickup_datetime'):\n","    \n","    data['pickup_date'] = data[feature].dt.date\n","    data['pickup_hour'] = data[feature].dt.hour\n","    data['pickup_day_of_week'] = data[feature].dt.dayofweek\n","    \n","    return data"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["def add_holiday_features(df, holiday_data):\n","    holiday_data['date'] = pd.to_datetime(holiday_data['date']).dt.date\n","    df = df.merge(holiday_data, how='left', left_on='pickup_date', right_on='date')\n","    df['pickup_holiday'] = df['holiday'].fillna(0)\n","    df['pickup_holiday'] = df['pickup_holiday'].apply(lambda x: 0 if x == 0 else 1)\n","    df = df.drop(['day', 'date', 'holiday'], axis=1)\n","    return df   "]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def add_osrm_features(data, data_2, on='id', how='left', columns = ['id','total_distance', 'total_travel_time', 'number_of_steps']):\n","    merged_data = data.merge(\n","        data_2[columns],\n","        on=on,\n","        how=how\n","    )\n","    return merged_data"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"ti1rljMGkBLm"},"outputs":[],"source":["def get_haversine_distance(lat1, lng1, lat2, lng2):\n","    # переводим углы в радианы\n","    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n","    # радиус земли в километрах\n","    EARTH_RADIUS = 6371 \n","    # считаем кратчайшее расстояние h по формуле Хаверсина\n","    lat_delta = lat2 - lat1\n","    lng_delta = lng2 - lng1\n","    d = np.sin(lat_delta * 0.5) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng_delta * 0.5) ** 2\n","    h = 2 * EARTH_RADIUS * np.arcsin(np.sqrt(d))\n","    return h\n","\n","def get_angle_direction(lat1, lng1, lat2, lng2):\n","    # переводим углы в радианы\n","    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n","    # считаем угол направления движения alpha по формуле угла пеленга\n","    lng_delta_rad = lng2 - lng1\n","    y = np.sin(lng_delta_rad) * np.cos(lat2)\n","    x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(lng_delta_rad)\n","    alpha = np.degrees(np.arctan2(y, x))\n","    return alpha\n","\n","def add_geographical_features(\n","    data, \n","    lat1='pickup_latitude', \n","    lng1='pickup_longitude',\n","    lat2='dropoff_latitude', \n","    lng2='dropoff_longitude', \n","    name_1='haversine_distance', name_2='direction'):\n","    \n","    data[name_1] = get_haversine_distance(data[lat1], data[lng1], data[lat2], data[lng2])\n","    \n","    data[name_2] = get_angle_direction(data[lat1], data[lng1], data[lat2], data[lng2])\n","    \n","    return data"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def add_cluster_features(taxi_data):\n","    coords = np.hstack((taxi_data[['pickup_latitude', 'pickup_longitude']],\n","                    taxi_data[['dropoff_latitude', 'dropoff_longitude']]))\n","    # обучаем алгоритм кластеризации\n","    kmeans = cluster.KMeans(n_clusters=10, random_state=42)\n","    kmeans.fit(coords)\n","    pred = kmeans.predict(coords)\n","    pred = kmeans.labels_\n","    taxi_data['geo_cluster'] = pred\n","    return taxi_data\n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["def add_weather_features(taxi_data, weather_data):\n","    \n","    weather_data['time'] = pd.to_datetime(weather_data['time'], format='%Y-%m-%d %H:%M:%S')\n","    taxi_data['date_&_hour'] = [(x, y) for x, y in zip(taxi_data['pickup_datetime'].dt.date, taxi_data['pickup_datetime'].dt.hour)]\n","    weather_data['date_&_hour'] = [(x, y) for x, y in zip(weather_data['time'].dt.date, weather_data['time'].dt.hour)]\n","    \n","    weather_data = weather_data.loc[:, ['date_&_hour', 'temperature', 'visibility', 'wind speed', 'precip', 'events']] \n","    \n","    df = taxi_data.merge(\n","        weather_data,\n","        on = 'date_&_hour',\n","        how = 'left'\n","    )\n","    df.drop('date_&_hour', axis=1, inplace=True)\n","    return df"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"oHIYqxjKkBLn"},"outputs":[],"source":["def fill_null_weather_data(taxi_data):\n","    cols_with_null = taxi_data.isnull().mean()\n","    cols_with_null = cols_with_null[cols_with_null > 0]\n","    for col in cols_with_null.index:\n","        if col == 'events':\n","            taxi_data[col] = taxi_data[col].fillna('None')\n","        else:\n","            taxi_data[col] = taxi_data[col].fillna(taxi_data.groupby('pickup_date')[col].transform('median'))\n","    return taxi_data"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["from sklearn.preprocessing import OneHotEncoder\n","\n","scaler = preprocessing.MinMaxScaler()\n","one_hot_encoder = OneHotEncoder()\n","\n","columns_to_change = ['pickup_day_of_week', 'geo_cluster', 'events']\n","\n","best_features = ['pickup_hour', 'dropoff_latitude', 'pickup_longitude', \n"," 'haversine_distance', 'pickup_latitude', 'dropoff_longitude',\n"," 'total_distance', 'total_travel_time', 'temperature', 'number_of_steps',\n"," 'pickup_day_of_week_4', 'pickup_day_of_week_5', 'pickup_day_of_week_6',\n"," 'vendor_id', 'passenger_count', 'pickup_day_of_week_3', 'geo_cluster_9',\n"," 'pickup_day_of_week_1', 'pickup_day_of_week_2', 'geo_cluster_1',\n"," 'geo_cluster_3', 'geo_cluster_5', 'geo_cluster_7', 'store_and_fwd_flag'\n"," ]"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Подготовим данные для обучения модели"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["osrm_data_train = pd.read_csv(\"C:\\Личные документы\\Учёба SF DS\\data\\osrm_data_train.zip\")\n","train_data = pd.read_csv(\"C:\\Личные документы\\Учёба SF DS\\data\\Project5_train_data.zip\")\n","holiday_data = pd.read_csv('holiday_data.csv', sep=\";\")\n","weather_data = pd.read_csv('weather_data.zip')"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# Train set prepare\n","\n","train_data['pickup_datetime']=pd.to_datetime(train_data['pickup_datetime'],format='%Y-%m-%d %H:%M:%S')\n","train_data = add_datetime_features(train_data)\n","train_data = add_holiday_features(train_data, holiday_data)\n","train_data = add_osrm_features(train_data, osrm_data_train)\n","train_data = add_geographical_features(train_data)\n","train_data = add_cluster_features(train_data)\n","train_data = add_weather_features(train_data, weather_data)\n","train_data = fill_null_weather_data(train_data)\n","\n","train_data['vendor_id'] = train_data['vendor_id'].apply(lambda x: 0 if x == 1 else 1)\n","train_data['store_and_fwd_flag'] = train_data['store_and_fwd_flag'].apply(lambda x: 0 if x == 'N' else 1)\n","train_data_onehot = one_hot_encoder.fit_transform(train_data[columns_to_change]).toarray()\n","column_names = one_hot_encoder.get_feature_names_out(columns_to_change)\n","train_data_onehot = pd.DataFrame(train_data_onehot, columns=column_names)\n","\n","train_data = pd.concat(\n","    [train_data.reset_index(drop=True).drop(columns_to_change, axis=1), train_data_onehot], \n","    axis=1\n",")\n","train_data = train_data.dropna(axis=1)\n","train_data['trip_duration_log'] = np.log(train_data['trip_duration']+1)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["X = train_data[best_features]\n","scaler.fit(X)\n","X_test_scaled = scaler.transform(X)\n","y = train_data['trip_duration']\n","y_log = train_data['trip_duration_log']"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["X_train, X_valid, y_train_log, y_valid_log = model_selection.train_test_split(\n","    X, y_log, \n","    test_size=0.33, \n","    random_state=42\n",")"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["import xgboost as xgb\n","# Создание матриц наблюдений в формате DMatrix\n","dtrain = xgb.DMatrix(X_train, label=y_train_log, feature_names=best_features)\n","dvalid = xgb.DMatrix(X_valid, label=y_valid_log, feature_names=best_features)\n"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[0]\ttrain-rmse:5.42171\tvalid-rmse:5.42130\n","[10]\ttrain-rmse:1.93626\tvalid-rmse:1.93698\n","[20]\ttrain-rmse:0.78730\tvalid-rmse:0.78843\n","[30]\ttrain-rmse:0.48194\tvalid-rmse:0.48319\n","[40]\ttrain-rmse:0.42488\tvalid-rmse:0.42616\n","[50]\ttrain-rmse:0.41277\tvalid-rmse:0.41424\n","[60]\ttrain-rmse:0.40816\tvalid-rmse:0.41017\n","[70]\ttrain-rmse:0.40449\tvalid-rmse:0.40684\n","[80]\ttrain-rmse:0.40270\tvalid-rmse:0.40546\n","[90]\ttrain-rmse:0.40077\tvalid-rmse:0.40378\n","[100]\ttrain-rmse:0.39917\tvalid-rmse:0.40249\n","[110]\ttrain-rmse:0.39793\tvalid-rmse:0.40160\n","[120]\ttrain-rmse:0.39636\tvalid-rmse:0.40039\n","[130]\ttrain-rmse:0.39500\tvalid-rmse:0.39932\n","[140]\ttrain-rmse:0.39404\tvalid-rmse:0.39863\n","[150]\ttrain-rmse:0.39324\tvalid-rmse:0.39815\n","[160]\ttrain-rmse:0.39271\tvalid-rmse:0.39788\n","[170]\ttrain-rmse:0.39190\tvalid-rmse:0.39747\n","[180]\ttrain-rmse:0.39112\tvalid-rmse:0.39702\n","[190]\ttrain-rmse:0.39039\tvalid-rmse:0.39665\n","[200]\ttrain-rmse:0.38963\tvalid-rmse:0.39627\n","[210]\ttrain-rmse:0.38891\tvalid-rmse:0.39585\n","[220]\ttrain-rmse:0.38822\tvalid-rmse:0.39552\n","[230]\ttrain-rmse:0.38758\tvalid-rmse:0.39520\n","[240]\ttrain-rmse:0.38706\tvalid-rmse:0.39491\n","[250]\ttrain-rmse:0.38652\tvalid-rmse:0.39467\n","[260]\ttrain-rmse:0.38603\tvalid-rmse:0.39451\n","[270]\ttrain-rmse:0.38549\tvalid-rmse:0.39430\n","[280]\ttrain-rmse:0.38459\tvalid-rmse:0.39378\n","[290]\ttrain-rmse:0.38391\tvalid-rmse:0.39346\n","[300]\ttrain-rmse:0.38333\tvalid-rmse:0.39318\n","[310]\ttrain-rmse:0.38284\tvalid-rmse:0.39303\n","[320]\ttrain-rmse:0.38240\tvalid-rmse:0.39297\n","[330]\ttrain-rmse:0.38200\tvalid-rmse:0.39278\n","[340]\ttrain-rmse:0.38135\tvalid-rmse:0.39242\n","[350]\ttrain-rmse:0.38094\tvalid-rmse:0.39222\n","[360]\ttrain-rmse:0.38058\tvalid-rmse:0.39211\n","[370]\ttrain-rmse:0.38018\tvalid-rmse:0.39201\n","[380]\ttrain-rmse:0.37986\tvalid-rmse:0.39190\n","[390]\ttrain-rmse:0.37939\tvalid-rmse:0.39165\n","[399]\ttrain-rmse:0.37906\tvalid-rmse:0.39154\n"]}],"source":["# Гиперпараметры модели\n","xgb_pars = {'min_child_weight': 20, 'eta': 0.1, 'colsample_bytree': 0.9, \n","            'max_depth': 6, 'subsample': 0.9, 'lambda': 1, 'nthread': -1, \n","            'booster' : 'gbtree', 'eval_metric': 'rmse', 'objective': 'reg:squarederror'\n","           }\n","# Тренировочная и валидационная выборка\n","watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n","# Обучаем модель XGBoost\n","model = xgb.train(\n","    params=xgb_pars, #гиперпараметры модели\n","    dtrain=dtrain, #обучающая выборка\n","    num_boost_round=400, #количество моделей в ансамбле\n","    evals=watchlist, #выборки, на которых считается матрица\n","    early_stopping_rounds=20, #раняя остановка\n","    maximize=False, #смена поиска максимума на минимум\n","    verbose_eval=10 #шаг, через который происходит отображение метрик\n",")"]},{"cell_type":"code","execution_count":146,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] num_threads is set=-1, n_jobs=-1 will be ignored. Current value: num_threads=-1\n"]},{"data":{"text/plain":["LGBMRegressor(bagging_fraction=0.5, feature_fraction=0.9, learning_rate=0.08,\n","              max_bin=1000, max_depth=30, metric='rmse', num_leaves=1000,\n","              num_threads=-1, objective='regression')"]},"execution_count":146,"metadata":{},"output_type":"execute_result"}],"source":["l_model = LGBMRegressor(\n","    objective='regression',\n","    metric='rmse',\n","    learning_rate=0.1,\n","    max_depth=25,\n","    num_leaves=1000, \n","    feature_fraction=0.9,\n","    bagging_fraction=0.5,\n","    max_bin=1000 ,\n","    num_threads=-1)\n","l_model.fit(X_train, y_train_log)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["params = {\n","    'objective': 'regression',\n","    'metric' : 'rmse',\n","    'learning_rate': 0.1,\n","    'max_depth': 25,\n","    'num_leaves': 1000, \n","    'feature_fraction': 0.9,\n","    'bagging_fraction': 0.5,\n","    'max_bin': 1000 ,\n","    'num_threads' : -1}"]},{"cell_type":"code","execution_count":157,"metadata":{},"outputs":[],"source":["params = {}"]},{"cell_type":"code","execution_count":170,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\AubakirovMA\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n","  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n","c:\\Users\\AubakirovMA\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n","  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024764 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 1993\n","[LightGBM] [Info] Number of data points in the train set: 729322, number of used features: 24\n","[LightGBM] [Info] Start training from score 6.466908\n","Training until validation scores don't improve for 110 rounds\n","[100]\ttraining's l2: 0.161415\tvalid_1's l2: 0.163781\n","[200]\ttraining's l2: 0.153552\tvalid_1's l2: 0.158945\n","[300]\ttraining's l2: 0.148455\tvalid_1's l2: 0.156262\n","[400]\ttraining's l2: 0.144477\tvalid_1's l2: 0.154669\n","[500]\ttraining's l2: 0.141361\tvalid_1's l2: 0.153496\n","[600]\ttraining's l2: 0.138437\tvalid_1's l2: 0.152485\n","[700]\ttraining's l2: 0.136214\tvalid_1's l2: 0.151941\n","[800]\ttraining's l2: 0.134079\tvalid_1's l2: 0.151317\n","[900]\ttraining's l2: 0.132185\tvalid_1's l2: 0.150867\n","[1000]\ttraining's l2: 0.130419\tvalid_1's l2: 0.15055\n","[1100]\ttraining's l2: 0.128787\tvalid_1's l2: 0.150307\n","[1200]\ttraining's l2: 0.127247\tvalid_1's l2: 0.149959\n","[1300]\ttraining's l2: 0.125647\tvalid_1's l2: 0.149764\n","[1400]\ttraining's l2: 0.124319\tvalid_1's l2: 0.149612\n","[1500]\ttraining's l2: 0.123081\tvalid_1's l2: 0.149394\n","Did not meet early stopping. Best iteration is:\n","[1500]\ttraining's l2: 0.123081\tvalid_1's l2: 0.149394\n"]}],"source":["import lightgbm as lgb\n","\n","train_data = lgb.Dataset(X_train, y_train_log)\n","eval_data = lgb.Dataset(X_valid, y_valid_log)\n","\n","l_model = lgb.train(params, train_data, num_boost_round=1500, valid_sets=(train_data, eval_data),\n","                early_stopping_rounds=110, verbose_eval=100)"]},{"cell_type":"code","execution_count":165,"metadata":{},"outputs":[],"source":["y_train_lgb = l_model.predict(X_train)\n","y_valid_lgb = l_model.predict(X_valid)"]},{"cell_type":"code","execution_count":140,"metadata":{},"outputs":[],"source":["y_train_xgb = model.inplace_predict(X_train)\n","y_valid_xgb = model.inplace_predict(X_valid)\n","\n","y_train_lgb = l_model.predict(X_train)\n","y_valid_lgb = l_model.predict(X_valid)\n"]},{"cell_type":"code","execution_count":167,"metadata":{},"outputs":[],"source":["\n","y_train_pred = (y_train_xgb + y_train_lgb) / 2\n","y_valid_pred = (y_valid_xgb + y_valid_lgb) / 2"]},{"cell_type":"code","execution_count":166,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["RMSLE on train by XGB: 0.3780398208476835\n","RMSLE on valid by XGB: 0.3937419406785572\n","\n","RMSLE on train by LGB: 0.35082873860493413\n","RMSLE on valid by LGB: 0.38651539370263915\n","\n","MeAE on valid: 1.75\n"]}],"source":["from sklearn.metrics import median_absolute_error\n","\n","def rmsle(y_real, y_pred):\n","    mse = np.sum(np.square(np.subtract(y_real, y_pred))) / len(y_real)\n","    return np.sqrt(mse)  \n","\n","y_valid_real = np.exp(y_valid_log)-1\n","y_valid_pred_real = np.exp(y_valid_lgb) - 1\n","\n","MeAE = median_absolute_error(y_valid_real, y_valid_pred_real) / 60\n","\n","print(f\"RMSLE on train by XGB: {rmsle(y_train_log, y_train_xgb)}\")\n","print(f\"RMSLE on valid by XGB: {rmsle(y_valid_log, y_valid_xgb)}\")\n","print()\n","print(f\"RMSLE on train by LGB: {rmsle(y_train_log, y_train_lgb)}\")\n","print(f\"RMSLE on valid by LGB: {rmsle(y_valid_log, y_valid_lgb)}\")\n","print()\n","print(f\"MeAE on valid: {round(MeAE, 2)}\")"]},{"cell_type":"code","execution_count":169,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["RMSLE on train: 0.3617577161751215\n","RMSLE on valid: 0.3872130780646488\n","MeAE on valid: 1.78\n"]}],"source":["from sklearn.metrics import median_absolute_error\n","\n","y_valid_real = np.exp(y_valid_log)-1\n","y_valid_pred_real = np.exp(y_valid_pred) - 1\n","\n","def rmsle(y_real, y_pred):\n","    mse = np.sum(np.square(np.subtract(y_real, y_pred))) / len(y_real)\n","    return np.sqrt(mse)    \n","\n","MeAE = median_absolute_error(y_valid_real, y_valid_pred_real) / 60\n","\n","print(f\"RMSLE on train: {rmsle(y_train_log, y_train_pred)}\")\n","print(f\"RMSLE on valid: {rmsle(y_valid_log, y_valid_pred)}\")\n","print(f\"MeAE on valid: {round(MeAE, 2)}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Тестовые данные для kaggle"]},{"cell_type":"code","execution_count":104,"metadata":{},"outputs":[],"source":["osrm_data_test = pd.read_csv(\"C:\\Личные документы\\Учёба SF DS\\data\\Project5_osrm_data_test.zip\")\n","test_data = pd.read_csv(\"C:\\Личные документы\\Учёба SF DS\\data\\Project5_test_data.zip\")\n","holiday_data = pd.read_csv('holiday_data.csv', sep=\";\")\n","weather_data = pd.read_csv('weather_data.zip')\n","test_id = test_data['id']"]},{"cell_type":"code","execution_count":105,"metadata":{"id":"nqWQbqhGkBLy"},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of data: (625134, 24)\n"]}],"source":["test_data['pickup_datetime']=pd.to_datetime(test_data['pickup_datetime'],format='%Y-%m-%d %H:%M:%S')\n","test_data = add_datetime_features(test_data)\n","test_data = add_holiday_features(test_data, holiday_data)\n","test_data = add_osrm_features(test_data, osrm_data_test)\n","test_data = add_geographical_features(test_data)\n","test_data = add_cluster_features(test_data)\n","test_data = add_weather_features(test_data, weather_data)\n","test_data = fill_null_weather_data(test_data)\n","\n","test_data['vendor_id'] = test_data['vendor_id'].apply(lambda x: 0 if x == 1 else 1)\n","test_data['store_and_fwd_flag'] = test_data['store_and_fwd_flag'].apply(lambda x: 0 if x == 'N' else 1)\n","test_data_onehot = one_hot_encoder.fit_transform(test_data[columns_to_change]).toarray()\n","column_names = one_hot_encoder.get_feature_names_out(columns_to_change)\n","test_data_onehot = pd.DataFrame(test_data_onehot, columns=column_names)\n","\n","test_data = pd.concat(\n","    [test_data.reset_index(drop=True).drop(columns_to_change, axis=1), test_data_onehot], \n","    axis=1\n",")\n","X_test = test_data[best_features]\n","scaler.fit(X_test)\n","X_test_scaled = scaler.transform(X_test)\n","print('Shape of data: {}'.format(X_test.shape))"]},{"cell_type":"code","execution_count":107,"metadata":{},"outputs":[],"source":["y_test_xgb = model.inplace_predict(X_test)\n","\n","y_test_lgb = booster.predict(X_test)\n","\n","y_test_pred = (y_test_xgb + y_test_lgb) / 2"]},{"cell_type":"code","execution_count":109,"metadata":{},"outputs":[],"source":["y_test_predict = np.exp(y_test_pred) - 1"]},{"cell_type":"code","execution_count":110,"metadata":{"id":"JeMAzrHHkBLy"},"outputs":[],"source":["submission = pd.DataFrame({'id': test_id, 'trip_duration': y_test_predict})\n","submission.to_csv('submission_gb.csv', index=False)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Private Score: 0.40158   \n","# Public Score: 0.40414"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":0}
